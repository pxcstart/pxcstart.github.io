<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>RLHF(三)：基于TRL的GrpoTrainer详解</title>
    <link href="/2025/04/19/RLHF(%E4%B8%89)/"/>
    <url>/2025/04/19/RLHF(%E4%B8%89)/</url>
    
    <content type="html"><![CDATA[<p>写在前面：目前主流的LLM post-training框架主要有<a href="https://github.com/huggingface/trl">trl</a>, <a href="https://github.com/OpenRLHF/OpenRLHF">OpenRLHF</a>, <a href="https://github.com/volcengine/verl">verl</a>。后两者集成度较高，适合对LLM零代码训练，而trl灵活性较强，这里主要对GRPO Trainer的训练流程进行梳理</p><span id="more"></span><h3 id="GRPOTrainer类"><a href="#GRPOTrainer类" class="headerlink" title="GRPOTrainer类"></a>GRPOTrainer类</h3><p>它继承了transformers.Trainer，并重写或拓展了若干方法，包括：<br><strong><strong>init</strong></strong><br>作用：初始化模型、参考模型（ref_model）、奖励模型（reward_funcs）等，并作一些超参数设置（如 num_generations, beta 等）。</p><ul><li>model: 加载策略模型, 可以是字符串（模型ID或路径）或预训练模型对象。仅支持因果语言模型</li><li>reward_funcs: 加载奖励函数，可以是预训练模型(仅支持<code>SequenceClassification</code>模型);用户自定义Python函数；或者是一个列表，意味着多种奖励函数一起用</li><li>args: GRPOConfig对象，包含训练的所有参数</li><li>train_dataset: 训练数据集，必须包含名为’prompt’的列，可以是Dataset或IterableDataset</li><li>eval_dataset: 评估数据集</li><li>processing_class: 数据处理器类，用于对训练和评估数据进行预处理 typing.Optional[transformers.tokenization_utils_base.PreTrainedTokenizerBase] &#x3D; None</li><li>reward_processing_class: 奖励函数对应的分词器，支持单个分词器或多个分词器的列表</li><li>callback: 自定义训练回调列表，可扩展或覆盖默认的训练过程 typing.Optional[list[transformers.trainer_callback.TrainerCallback]] &#x3D; None</li><li>optimizer</li><li>peft_config</li></ul><p>补充说明：</p><ol><li>processing_class 填充侧必须设置为 “left”。如果为 None，则使用 from_pretrained 从模型名称加载处理类。</li><li>reward_processing_class: 可选，默认为None。在自定义时，必须与 reward_funcs 中奖励函数的顺序和长度匹配。</li></ol><p><strong>_prepare_inputs</strong><br>作用：在训练循环中，每一个 batch 先对 prompt 进行采样生成多条回答，调用奖励模型打分，计算组内相对优势。<br>简要流程：</p><ol><li>对batch中每个prompt调用模型一次性生成 <code>num_generations</code>条回答。若生成中提前出现EOS，对EOS之后的token使用completion_mask进行掩码</li><li>使用ref_model对完整序列(prompt+completion) 计算token级对数概率，用于后面进行KL</li><li>调用reward model对每条回答打分，形成[B*G]的reward</li><li>计算相对优势：[B*G]-reshape-&gt;[B,G], 对同一个 prompt 的 G 条回答做“均值、标准差”，再 broadcast 回去，以得到每条回答的相对 advantage<br>全部代码：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br><span class="line">107</span><br><span class="line">108</span><br><span class="line">109</span><br><span class="line">110</span><br><span class="line">111</span><br><span class="line">112</span><br><span class="line">113</span><br><span class="line">114</span><br><span class="line">115</span><br><span class="line">116</span><br><span class="line">117</span><br><span class="line">118</span><br><span class="line">119</span><br><span class="line">120</span><br><span class="line">121</span><br><span class="line">122</span><br><span class="line">123</span><br><span class="line">124</span><br><span class="line">125</span><br><span class="line">126</span><br><span class="line">127</span><br><span class="line">128</span><br><span class="line">129</span><br><span class="line">130</span><br><span class="line">131</span><br><span class="line">132</span><br><span class="line">133</span><br><span class="line">134</span><br><span class="line">135</span><br><span class="line">136</span><br><span class="line">137</span><br><span class="line">138</span><br><span class="line">139</span><br><span class="line">140</span><br><span class="line">141</span><br><span class="line">142</span><br><span class="line">143</span><br><span class="line">144</span><br><span class="line">145</span><br><span class="line">146</span><br><span class="line">147</span><br><span class="line">148</span><br><span class="line">149</span><br><span class="line">150</span><br><span class="line">151</span><br><span class="line">152</span><br><span class="line">153</span><br><span class="line">154</span><br><span class="line">155</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">_prepare_inputs</span>(<span class="hljs-params">self, inputs: <span class="hljs-built_in">dict</span>[<span class="hljs-built_in">str</span>, <span class="hljs-type">Union</span>[torch.Tensor, <span class="hljs-type">Any</span>]]</span>) -&gt; <span class="hljs-built_in">dict</span>[<span class="hljs-built_in">str</span>, <span class="hljs-type">Union</span>[torch.Tensor, <span class="hljs-type">Any</span>]]:<br>    device = <span class="hljs-variable language_">self</span>.accelerator.device<br>    prompts = [x[<span class="hljs-string">&quot;prompt&quot;</span>] <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> inputs]<br>    prompts_text = [maybe_apply_chat_template(example, <span class="hljs-variable language_">self</span>.processing_class)[<span class="hljs-string">&quot;prompt&quot;</span>] <span class="hljs-keyword">for</span> example <span class="hljs-keyword">in</span> inputs]<br>    prompt_inputs = <span class="hljs-variable language_">self</span>.processing_class(<br>        prompts_text, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>, padding=<span class="hljs-literal">True</span>, padding_side=<span class="hljs-string">&quot;left&quot;</span>, add_special_tokens=<span class="hljs-literal">False</span><br>    )<br>    prompt_inputs = <span class="hljs-built_in">super</span>()._prepare_inputs(prompt_inputs)<br>    prompt_ids, prompt_mask = prompt_inputs[<span class="hljs-string">&quot;input_ids&quot;</span>], prompt_inputs[<span class="hljs-string">&quot;attention_mask&quot;</span>]<br><br>    <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.max_prompt_length <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>        prompt_ids = prompt_ids[:, -<span class="hljs-variable language_">self</span>.max_prompt_length :]<br>        prompt_mask = prompt_mask[:, -<span class="hljs-variable language_">self</span>.max_prompt_length :]<br><br>    <span class="hljs-comment"># Generate completions using either vLLM or regular generation</span><br>    <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.args.use_vllm:<br>        <span class="hljs-comment"># First, have main process load weights if needed</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.state.global_step != <span class="hljs-variable language_">self</span>._last_loaded_step:<br>            <span class="hljs-keyword">with</span> unwrap_model_for_generation(<br>                <span class="hljs-variable language_">self</span>.model, <span class="hljs-variable language_">self</span>.accelerator, gather_deepspeed3_params=<span class="hljs-variable language_">self</span>.args.ds3_gather_for_generation<br>            ) <span class="hljs-keyword">as</span> unwrapped_model:<br>                <span class="hljs-keyword">if</span> is_compiled_module(unwrapped_model):<br>                    state_dict = unwrapped_model._orig_mod.state_dict()<br>                <span class="hljs-keyword">else</span>:<br>                    state_dict = unwrapped_model.state_dict()<br>            <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.accelerator.is_main_process:<br>                llm_model = <span class="hljs-variable language_">self</span>.llm.llm_engine.model_executor.driver_worker.model_runner.model<br>                llm_model.load_weights(state_dict.items())<br>            <span class="hljs-variable language_">self</span>._last_loaded_step = <span class="hljs-variable language_">self</span>.state.global_step<br><br>        <span class="hljs-comment"># Generate completions using vLLM: gather all prompts and use them in a single call in the main process</span><br>        all_prompts_text = gather_object(prompts_text)<br>        <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.accelerator.is_main_process:<br>            outputs = <span class="hljs-variable language_">self</span>.llm.generate(all_prompts_text, sampling_params=<span class="hljs-variable language_">self</span>.sampling_params, use_tqdm=<span class="hljs-literal">False</span>)<br>            completion_ids = [out.token_ids <span class="hljs-keyword">for</span> completions <span class="hljs-keyword">in</span> outputs <span class="hljs-keyword">for</span> out <span class="hljs-keyword">in</span> completions.outputs]<br>        <span class="hljs-keyword">else</span>:<br>            completion_ids = [<span class="hljs-literal">None</span>] * <span class="hljs-built_in">len</span>(all_prompts_text) * <span class="hljs-variable language_">self</span>.num_generations<br><br>        <span class="hljs-comment"># Broadcast the completions from the main process to all processes, ensuring each process receives its</span><br>        <span class="hljs-comment"># corresponding slice.</span><br>        completion_ids = broadcast_object_list(completion_ids, from_process=<span class="hljs-number">0</span>)<br>        process_slice = <span class="hljs-built_in">slice</span>(<br>            <span class="hljs-variable language_">self</span>.accelerator.process_index * <span class="hljs-built_in">len</span>(prompts) * <span class="hljs-variable language_">self</span>.num_generations,<br>            (<span class="hljs-variable language_">self</span>.accelerator.process_index + <span class="hljs-number">1</span>) * <span class="hljs-built_in">len</span>(prompts) * <span class="hljs-variable language_">self</span>.num_generations,<br>        )<br>        completion_ids = completion_ids[process_slice]<br><br>        <span class="hljs-comment"># Pad the completions, and concatenate them with the prompts</span><br>        completion_ids = [torch.tensor(ids, device=device) <span class="hljs-keyword">for</span> ids <span class="hljs-keyword">in</span> completion_ids]<br>        completion_ids = pad(completion_ids, padding_value=<span class="hljs-variable language_">self</span>.processing_class.pad_token_id)<br>        prompt_ids = torch.repeat_interleave(prompt_ids, <span class="hljs-variable language_">self</span>.num_generations, dim=<span class="hljs-number">0</span>)<br>        prompt_mask = torch.repeat_interleave(prompt_mask, <span class="hljs-variable language_">self</span>.num_generations, dim=<span class="hljs-number">0</span>)<br>        prompt_completion_ids = torch.cat([prompt_ids, completion_ids], dim=<span class="hljs-number">1</span>)<br>    <span class="hljs-keyword">else</span>:<br>        <span class="hljs-comment"># Regular generation path</span><br>        <span class="hljs-keyword">with</span> unwrap_model_for_generation(<span class="hljs-variable language_">self</span>.model, <span class="hljs-variable language_">self</span>.accelerator) <span class="hljs-keyword">as</span> unwrapped_model:<br>            prompt_completion_ids = unwrapped_model.generate(<br>                prompt_ids, attention_mask=prompt_mask, generation_config=<span class="hljs-variable language_">self</span>.generation_config<br>            )<br><br>        <span class="hljs-comment"># Compute prompt length and extract completion ids</span><br>        prompt_length = prompt_ids.size(<span class="hljs-number">1</span>)<br>        prompt_ids = prompt_completion_ids[:, :prompt_length]<br>        completion_ids = prompt_completion_ids[:, prompt_length:]<br>        prompt_mask = prompt_mask.repeat_interleave(<span class="hljs-variable language_">self</span>.num_generations, dim=<span class="hljs-number">0</span>)<br><br>    <span class="hljs-comment"># Mask everything after the first EOS token</span><br>    is_eos = completion_ids == <span class="hljs-variable language_">self</span>.processing_class.eos_token_id<br>    eos_idx = torch.full((is_eos.size(<span class="hljs-number">0</span>),), is_eos.size(<span class="hljs-number">1</span>), dtype=torch.long, device=device)<br>    eos_idx[is_eos.<span class="hljs-built_in">any</span>(dim=<span class="hljs-number">1</span>)] = is_eos.<span class="hljs-built_in">int</span>().argmax(dim=<span class="hljs-number">1</span>)[is_eos.<span class="hljs-built_in">any</span>(dim=<span class="hljs-number">1</span>)]<br>    sequence_indices = torch.arange(is_eos.size(<span class="hljs-number">1</span>), device=device).expand(is_eos.size(<span class="hljs-number">0</span>), -<span class="hljs-number">1</span>)<br>    completion_mask = (sequence_indices &lt;= eos_idx.unsqueeze(<span class="hljs-number">1</span>)).<span class="hljs-built_in">int</span>()<br><br>    <span class="hljs-comment"># Concatenate prompt_mask with completion_mask for logit computation</span><br>    attention_mask = torch.cat([prompt_mask, completion_mask], dim=<span class="hljs-number">1</span>)  <span class="hljs-comment"># (B*G, P+C)</span><br><br>    logits_to_keep = completion_ids.size(<span class="hljs-number">1</span>)  <span class="hljs-comment"># we only need to compute the logits for the completion tokens</span><br><br>    <span class="hljs-keyword">with</span> torch.inference_mode():<br>        <span class="hljs-keyword">if</span> <span class="hljs-variable language_">self</span>.ref_model <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            ref_per_token_logps = <span class="hljs-variable language_">self</span>._get_per_token_logps(<br>                <span class="hljs-variable language_">self</span>.ref_model, prompt_completion_ids, attention_mask, logits_to_keep<br>            )<br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-keyword">with</span> <span class="hljs-variable language_">self</span>.accelerator.unwrap_model(<span class="hljs-variable language_">self</span>.model).disable_adapter():<br>                ref_per_token_logps = <span class="hljs-variable language_">self</span>._get_per_token_logps(<br>                    <span class="hljs-variable language_">self</span>.model, prompt_completion_ids, attention_mask, logits_to_keep<br>                )<br><br>    <span class="hljs-comment"># Decode the generated completions</span><br>    completions = <span class="hljs-variable language_">self</span>.processing_class.batch_decode(completion_ids, skip_special_tokens=<span class="hljs-literal">True</span>)<br>    <span class="hljs-keyword">if</span> is_conversational(inputs[<span class="hljs-number">0</span>]):<br>        completions = [[&#123;<span class="hljs-string">&quot;role&quot;</span>: <span class="hljs-string">&quot;assistant&quot;</span>, <span class="hljs-string">&quot;content&quot;</span>: completion&#125;] <span class="hljs-keyword">for</span> completion <span class="hljs-keyword">in</span> completions]<br><br>    <span class="hljs-comment"># Compute the rewards</span><br>    prompts = [prompt <span class="hljs-keyword">for</span> prompt <span class="hljs-keyword">in</span> prompts <span class="hljs-keyword">for</span> _ <span class="hljs-keyword">in</span> <span class="hljs-built_in">range</span>(<span class="hljs-variable language_">self</span>.num_generations)]  <span class="hljs-comment"># repeat prompts</span><br><br>    rewards_per_func = torch.zeros(<span class="hljs-built_in">len</span>(prompts), <span class="hljs-built_in">len</span>(<span class="hljs-variable language_">self</span>.reward_funcs), device=device)<br>    <span class="hljs-keyword">for</span> i, (reward_func, reward_processing_class) <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(<br>        <span class="hljs-built_in">zip</span>(<span class="hljs-variable language_">self</span>.reward_funcs, <span class="hljs-variable language_">self</span>.reward_processing_classes)<br>    ):<br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(reward_func, nn.Module):  <span class="hljs-comment"># Module instead of PretrainedModel for compat with compiled models</span><br>            <span class="hljs-keyword">if</span> is_conversational(inputs[<span class="hljs-number">0</span>]):<br>                messages = [&#123;<span class="hljs-string">&quot;messages&quot;</span>: p + c&#125; <span class="hljs-keyword">for</span> p, c <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(prompts, completions)]<br>                texts = [apply_chat_template(x, reward_processing_class)[<span class="hljs-string">&quot;text&quot;</span>] <span class="hljs-keyword">for</span> x <span class="hljs-keyword">in</span> messages]<br>            <span class="hljs-keyword">else</span>:<br>                texts = [p + c <span class="hljs-keyword">for</span> p, c <span class="hljs-keyword">in</span> <span class="hljs-built_in">zip</span>(prompts, completions)]<br>            reward_inputs = reward_processing_class(<br>                texts, return_tensors=<span class="hljs-string">&quot;pt&quot;</span>, padding=<span class="hljs-literal">True</span>, padding_side=<span class="hljs-string">&quot;right&quot;</span>, add_special_tokens=<span class="hljs-literal">False</span><br>            )<br>            reward_inputs = <span class="hljs-built_in">super</span>()._prepare_inputs(reward_inputs)<br>            <span class="hljs-keyword">with</span> torch.inference_mode():<br>                rewards_per_func[:, i] = reward_func(**reward_inputs).logits[:, <span class="hljs-number">0</span>]  <span class="hljs-comment"># Shape (B*G,)</span><br>        <span class="hljs-keyword">else</span>:<br>            <span class="hljs-comment"># Repeat all input columns (but &quot;prompt&quot; and &quot;completion&quot;) to match the number of generations</span><br>            reward_kwargs = &#123;key: [] <span class="hljs-keyword">for</span> key <span class="hljs-keyword">in</span> inputs[<span class="hljs-number">0</span>].keys() <span class="hljs-keyword">if</span> key <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> [<span class="hljs-string">&quot;prompt&quot;</span>, <span class="hljs-string">&quot;completion&quot;</span>]&#125;<br>            <span class="hljs-keyword">for</span> key <span class="hljs-keyword">in</span> reward_kwargs:<br>                <span class="hljs-keyword">for</span> example <span class="hljs-keyword">in</span> inputs:<br>                    <span class="hljs-comment"># Repeat each value in the column for `num_generations` times</span><br>                    reward_kwargs[key].extend([example[key]] * <span class="hljs-variable language_">self</span>.num_generations)<br>            output_reward_func = reward_func(prompts=prompts, completions=completions, **reward_kwargs)<br>            rewards_per_func[:, i] = torch.tensor(output_reward_func, dtype=torch.float32, device=device)<br><br>    <span class="hljs-comment"># Sum the rewards from all reward functions</span><br>    rewards = rewards_per_func.<span class="hljs-built_in">sum</span>(dim=<span class="hljs-number">1</span>)<br><br>    <span class="hljs-comment"># Compute grouped-wise rewards</span><br>    mean_grouped_rewards = rewards.view(-<span class="hljs-number">1</span>, <span class="hljs-variable language_">self</span>.num_generations).mean(dim=<span class="hljs-number">1</span>)<br>    std_grouped_rewards = rewards.view(-<span class="hljs-number">1</span>, <span class="hljs-variable language_">self</span>.num_generations).std(dim=<span class="hljs-number">1</span>)<br><br>    <span class="hljs-comment"># Normalize the rewards to compute the advantages</span><br>    mean_grouped_rewards = mean_grouped_rewards.repeat_interleave(<span class="hljs-variable language_">self</span>.num_generations, dim=<span class="hljs-number">0</span>)<br>    std_grouped_rewards = std_grouped_rewards.repeat_interleave(<span class="hljs-variable language_">self</span>.num_generations, dim=<span class="hljs-number">0</span>)<br>    advantages = (rewards - mean_grouped_rewards) / (std_grouped_rewards + <span class="hljs-number">1e-4</span>)<br><br>    <span class="hljs-comment"># Log the metrics</span><br>    reward_per_func = <span class="hljs-variable language_">self</span>.accelerator.gather_for_metrics(rewards_per_func).mean(<span class="hljs-number">0</span>)<br>    <span class="hljs-keyword">for</span> i, reward_func <span class="hljs-keyword">in</span> <span class="hljs-built_in">enumerate</span>(<span class="hljs-variable language_">self</span>.reward_funcs):<br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(reward_func, nn.Module):  <span class="hljs-comment"># Module instead of PretrainedModel for compat with compiled models</span><br>            reward_func_name = reward_func.config._name_or_path.split(<span class="hljs-string">&quot;/&quot;</span>)[-<span class="hljs-number">1</span>]<br>        <span class="hljs-keyword">else</span>:<br>            reward_func_name = reward_func.__name__<br>        <span class="hljs-variable language_">self</span>._metrics[<span class="hljs-string">f&quot;rewards/<span class="hljs-subst">&#123;reward_func_name&#125;</span>&quot;</span>].append(reward_per_func[i].item())<br><br>    <span class="hljs-variable language_">self</span>._metrics[<span class="hljs-string">&quot;reward&quot;</span>].append(<span class="hljs-variable language_">self</span>.accelerator.gather_for_metrics(rewards).mean().item())<br>    <span class="hljs-variable language_">self</span>._metrics[<span class="hljs-string">&quot;reward_std&quot;</span>].append(<span class="hljs-variable language_">self</span>.accelerator.gather_for_metrics(std_grouped_rewards).mean().item())<br><br>    <span class="hljs-keyword">return</span> &#123;<br>        <span class="hljs-string">&quot;prompt_ids&quot;</span>: prompt_ids,<br>        <span class="hljs-string">&quot;prompt_mask&quot;</span>: prompt_mask,<br>        <span class="hljs-string">&quot;completion_ids&quot;</span>: completion_ids,<br>        <span class="hljs-string">&quot;completion_mask&quot;</span>: completion_mask,<br>        <span class="hljs-string">&quot;ref_per_token_logps&quot;</span>: ref_per_token_logps,<br>        <span class="hljs-string">&quot;advantages&quot;</span>: advantages,<br>    &#125;<br></code></pre></td></tr></table></figure>具体细节：<a href="https://blog.csdn.net/shizheng_Li/article/details/145794949">https://blog.csdn.net/shizheng_Li/article/details/145794949</a></li></ol><p><strong>compute_loss</strong><br>作用：根据 GRPO 公式，结合 KL 惩罚项和相对优势，计算最终损失并进行反向传播。<br>简要流程：</p><ol><li>在当前策略下计算完整序列的token级对数概率</li><li>根据actor model和ref model的token_log_prob,计算KL散度</li><li>利用KL散度和<code>_prepare_inputs</code>中得到的相对优势计算GRPO Loss,然后反向传播进行梯度更新<br>全部代码：<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">compute_loss</span>(<span class="hljs-params">self, model, inputs, return_outputs=<span class="hljs-literal">False</span>, num_items_in_batch=<span class="hljs-literal">None</span></span>):<br>    <span class="hljs-keyword">if</span> return_outputs:<br>        <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">&quot;The GRPOTrainer does not support returning outputs&quot;</span>)<br>    <span class="hljs-comment"># Compute the per-token log probabilities for the model</span><br><br>    prompt_ids, prompt_mask = inputs[<span class="hljs-string">&quot;prompt_ids&quot;</span>], inputs[<span class="hljs-string">&quot;prompt_mask&quot;</span>]<br>    completion_ids, completion_mask = inputs[<span class="hljs-string">&quot;completion_ids&quot;</span>], inputs[<span class="hljs-string">&quot;completion_mask&quot;</span>]<br>    input_ids = torch.cat([prompt_ids, completion_ids], dim=<span class="hljs-number">1</span>)<br>    attention_mask = torch.cat([prompt_mask, completion_mask], dim=<span class="hljs-number">1</span>)<br>    logits_to_keep = completion_ids.size(<span class="hljs-number">1</span>)  <span class="hljs-comment"># we only need to compute the logits for the completion tokens</span><br><br>    per_token_logps = <span class="hljs-variable language_">self</span>._get_per_token_logps(model, input_ids, attention_mask, logits_to_keep)<br><br>    <span class="hljs-comment"># Compute the KL divergence between the model and the reference model</span><br>    ref_per_token_logps = inputs[<span class="hljs-string">&quot;ref_per_token_logps&quot;</span>]<br>    per_token_kl = torch.exp(ref_per_token_logps - per_token_logps) - (ref_per_token_logps - per_token_logps) - <span class="hljs-number">1</span><br><br>    <span class="hljs-comment"># x - x.detach() allows for preserving gradients from x</span><br>    advantages = inputs[<span class="hljs-string">&quot;advantages&quot;</span>]<br>    per_token_loss = torch.exp(per_token_logps - per_token_logps.detach()) * advantages.unsqueeze(<span class="hljs-number">1</span>)<br>    per_token_loss = -(per_token_loss - <span class="hljs-variable language_">self</span>.beta * per_token_kl)<br>    loss = ((per_token_loss * completion_mask).<span class="hljs-built_in">sum</span>(dim=<span class="hljs-number">1</span>) / completion_mask.<span class="hljs-built_in">sum</span>(dim=<span class="hljs-number">1</span>)).mean()<br><br>    <span class="hljs-comment"># Log the metrics</span><br>    completion_length = <span class="hljs-variable language_">self</span>.accelerator.gather_for_metrics(completion_mask.<span class="hljs-built_in">sum</span>(<span class="hljs-number">1</span>)).<span class="hljs-built_in">float</span>().mean().item()<br>    <span class="hljs-variable language_">self</span>._metrics[<span class="hljs-string">&quot;completion_length&quot;</span>].append(completion_length)<br><br>    mean_kl = ((per_token_kl * completion_mask).<span class="hljs-built_in">sum</span>(dim=<span class="hljs-number">1</span>) / completion_mask.<span class="hljs-built_in">sum</span>(dim=<span class="hljs-number">1</span>)).mean()<br>    <span class="hljs-variable language_">self</span>._metrics[<span class="hljs-string">&quot;kl&quot;</span>].append(<span class="hljs-variable language_">self</span>.accelerator.gather_for_metrics(mean_kl).mean().item())<br><br>    <span class="hljs-keyword">return</span> loss<br></code></pre></td></tr></table></figure>具体细节：<a href="https://blog.csdn.net/shizheng_Li/article/details/145793070">https://blog.csdn.net/shizheng_Li/article/details/145793070</a></li></ol><p><strong>prediction_step</strong><br>作用：训练 &#x2F; 验证阶段如何调用 _prepare_inputs 并获取 loss。<br>在评估或预测时，也需要执行 _prepare_inputs 来生成多条回答并算 loss，只不过不会再反向传播。这个函数就是在 eval 阶段或预测时，对应地拿到 loss，用于打印日志或 early stopping 等操作。</p><p><strong>log 与 create_model_card</strong><br>作用：日志与模型卡，可上传到 Hugging Face Hub 做模型管理。<br>create_model_card输入参数：</p><ul><li>model_name：模型的名称</li><li>dataset_name：用于训练的数据集的名称</li><li>tags：要与模型卡关联的标签</li></ul><h3 id="自定义奖励函数"><a href="#自定义奖励函数" class="headerlink" title="自定义奖励函数"></a>自定义奖励函数</h3><p>GRPOTrainer 支持使用自定义奖励函数来代替密集奖励模型。为了确保兼容性，您的奖励函数必须满足以下要求：</p><ol><li>输入参数<br>函数必须接受以下内容作为关键字参数：<ul><li>prompts（包含提示），</li><li>completions（包含生成的补全），</li><li>数据集可能包含的所有列名（但prompt除外）。例如，如果数据集包含名为ground_truth的列，则将使用ground_truth作为关键字参数调用该函数。满足此要求的最简单方法是在函数签名中使用**kwargs。</li></ul></li></ol><p>根据数据集格式，输入会有所不同：<br>    - 对于标准格式，prompts和completions将是字符串列表。<br>    - 对于对话格式，prompts和completions将是消息字典列表。</p><ol start="2"><li>返回值<br>函数必须返回一个浮点数列表。每个浮点数代表对应于单个补全的奖励。</li></ol><p>示例一: 奖励较长的补全</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">reward_func</span>(<span class="hljs-params">completions, **kwargs</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;奖励函数，对较长的补全给予更高的分数。&quot;&quot;&quot;</span><br>    <span class="hljs-keyword">return</span> [<span class="hljs-built_in">float</span>(<span class="hljs-built_in">len</span>(completion)) <span class="hljs-keyword">for</span> completion <span class="hljs-keyword">in</span> completions]<br></code></pre></td></tr></table></figure><p>示例二：奖励具有特定格式的补全</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> re<br><br><span class="hljs-keyword">def</span> <span class="hljs-title function_">format_reward_func</span>(<span class="hljs-params">completions, **kwargs</span>):<br>    <span class="hljs-string">&quot;&quot;&quot;奖励函数，检查补全是否具有特定格式。&quot;&quot;&quot;</span><br>    pattern = <span class="hljs-string">r&quot;^&amp;lt;think&amp;gt;.*?&amp;lt;/think&amp;gt;&amp;lt;answer&amp;gt;.*?&amp;lt;/answer&amp;gt;$&quot;</span><br>    completion_contents = [completion[<span class="hljs-number">0</span>][<span class="hljs-string">&quot;content&quot;</span>] <span class="hljs-keyword">for</span> completion <span class="hljs-keyword">in</span> completions]<br>    matches = [re.<span class="hljs-keyword">match</span>(pattern, content) <span class="hljs-keyword">for</span> content <span class="hljs-keyword">in</span> completion_contents]<br>    <span class="hljs-keyword">return</span> [<span class="hljs-number">1.0</span> <span class="hljs-keyword">if</span> <span class="hljs-keyword">match</span> <span class="hljs-keyword">else</span> <span class="hljs-number">0.0</span> <span class="hljs-keyword">for</span> <span class="hljs-keyword">match</span> <span class="hljs-keyword">in</span> matches]<br></code></pre></td></tr></table></figure><p>写在最后： 详见<a href="https://huggingface.co/docs/trl/main/en/grpo_trainer">https://huggingface.co/docs/trl/main/en/grpo_trainer</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>LLM</tag>
      
      <tag>RLHF</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>RLHF(二)：偏好数据采集</title>
    <link href="/2025/04/18/RLHF(%E4%BA%8C)/"/>
    <url>/2025/04/18/RLHF(%E4%BA%8C)/</url>
    
    <content type="html"><![CDATA[<h3 id="ORM-和-PRM"><a href="#ORM-和-PRM" class="headerlink" title="ORM 和 PRM"></a>ORM 和 PRM</h3><p><strong>ORM</strong>：结果奖励模型，是不管推理有多少步，对完整的生成结果进行一次打分，是一个反馈更稀疏的奖励模型<br><strong>PRM</strong>：过程奖励模型，是在生成过程中，分步骤，对每一步进行打分，是更细粒度的奖励模型</p><p>使用PRM可以在post-training和inference两个阶段提升模型的推理性能：</p><ul><li>Post-Training阶段：在偏好对齐阶段，通过在RL过程中增加PRM，对采样的结果按步骤输出奖励值，为模型提供更精细的监督信号，来指导策略模型优化，提升模型按步推理的能力</li><li>Inference阶段：对于一个训练好的PRM，可以在inference阶段筛选优质生成结果。</li></ul><h3 id="MCTS-蒙特卡洛树搜索"><a href="#MCTS-蒙特卡洛树搜索" class="headerlink" title="MCTS 蒙特卡洛树搜索"></a>MCTS 蒙特卡洛树搜索</h3><p>MCTS是强化学习领域提出的方法，通过采样方式预估当前动作或状态的价值。具体操作步骤：使用已有的策略与环境做仿真交互，进行多次rollout采样，最终构成了一个从当前节点出发的一颗Tree（每个rollout表示<strong>从当前节点到最终结束状态的多次与环境仿真交互的过程</strong>）。这颗Tree的所有叶子节点都是结束状态，结束状态是可以通过定义规则进行量化收益的。当Tree的叶子结点有了奖励值，就可通过反向传播，计算每个中间节点的奖励值，最终计算出整个Tree所有节点的奖励值。MCTS一次rollout包括：sample，expand，simulate，backprop四个步骤。</p><ul><li><strong>Sample(采样)：</strong>选择一个未被探索的节点，在Reasoning Model中的节点表示一个带有特定标签的推理步骤（如：planning 节点，reflection节点等）。初始情况，Tree只有一个表示原始查询的节点</li><li><strong>Expand(扩展)：</strong>从未被选择的节点出发，展开所有可能的子节点，如下图中从$S_0$到$S_{1,1}, S_{1,2}, S_{1,3}, S_{1,4}$的过程。对于文本生成模型，不可能穷举所有子节点(next token)，因此需要设置一个最大生活次数，即最大子节点个数。</li><li><strong>Simulate(模拟)：</strong>从展开的子节点中，随机选择一个节点，再展开它的子节点，重复进行expand过程。直到达到叶子结点。该迭代过程需要控制搜索树的最大深度N</li><li><strong>Backprop(回传)：</strong>通过多次模拟我们得到了一个从根节点到叶子结点的Tree，如下图所示。通过计算<code>(从当前节点出发到正确答案的路径数)/(从当前节点出发的总路径数)</code>的比值作为节点的奖励值。</li></ul><p>使用MCTS也可以在post-training和inference两个节点提升模型的推理性能：</p><ul><li>Post-Training阶段: 对于每个problem 通过上述方法构造一个搜索Tree，然后进行Tree的游走遍历采样，再用采样的样本SFT或RL训练模型。</li><li>Inference阶段：在推理阶段，也是对一个problem探索多节点构造一棵搜索树，对于到达正确答案的路径，根据节点路径的置信度打分，贪心选取最优路径作为最终的推理结果。</li></ul><p><img src="/../article_img/RLHF/img2-1.png" alt="MCTS生成搜索树的过程"></p><h3 id="分而治之：OmegaPRM"><a href="#分而治之：OmegaPRM" class="headerlink" title="分而治之：OmegaPRM"></a>分而治之：OmegaPRM</h3><p>由于MCTS和PRM方法的规模化成本过高，为了更高效的收集高质量的过程监督数据，OmegaPRM算法通过二分搜索快速识别思维链CoT中的第一个错误，并平衡正反例，从而确保效率和质量。</p><p>OmegaPRM的搜索树如下图所示，树中的每个节点都表示部分思维链解决方案的状态，其中包含信息，如推出的准确性和其他统计数据。每条边都表示从上一个状态开始的一个动作，即推理步骤。黄色边是正确的步骤，蓝色边是错误的步骤。<br><img src="/../article_img/RLHF/img2-2.png" alt="OmegaPRM生成搜索树的过程"></p><p>搜索算法的具体实现如下：建立了一个<code>完成者策略</code>，该策略可以采用问题 𝑞 和一个由前 𝑡 步骤 $𝑥_{1:𝑡}$ 组成的前缀解决方案，并输出后续步骤的完成情况（在强化学习中通常称为<code>rollout</code>），直到得出最终答案。对于解决方案的任何步骤，<code>完成者策略</code>可用于从该步骤中随机抽取 𝑘 个 <code>rollout</code>。将这些 rollout 的最终答案与正确答案进行比较，提供与 𝑘 个 rollout 相对应的 𝑘 个答案正确性标签。随后，计算第 𝑡 步中正确 rollout 占总 rollout 的比率，公式1 展示了估计前缀步骤 $𝑥_{1:𝑡}$ “正确性水平”的方法。只要在逻辑推理场景中任何一个推出是正确的，𝑥1:𝑡 就应该被视为正确的。为了优化注释效率，OmegaPRM采用二分搜索的蒙特卡洛估计，如下图所示。<br>$c_t &#x3D; MontoCarlo(q,x_{1:t}) &#x3D; \frac{Num(\text{correct rollouts from t-th step})}{Num(\text{total rollouts from t-th step})}$<br><img src="/../article_img/RLHF/img2-3.png" alt="OmegaPRM二分搜索算法"></p>]]></content>
    
    
    
    <tags>
      
      <tag>LLM</tag>
      
      <tag>RLHF</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>RLHF(一)：LLM post-training</title>
    <link href="/2025/04/17/RLHF(%E4%B8%80)/"/>
    <url>/2025/04/17/RLHF(%E4%B8%80)/</url>
    
    <content type="html"><![CDATA[<h3 id="1-PPO算法"><a href="#1-PPO算法" class="headerlink" title="1. PPO算法"></a>1. PPO算法</h3><p>$L_{PPO} &#x3D; \sum_{(s_t,a_t)}\frac{\pi_\theta(a_t|s_t)}{\pi_{ref}(a_t|s_t)}A(s_t,a_t) - \beta KL(\pi_\theta, \pi_{ref})$</p><p>PPO的训练步骤如下：<br>（1）收集人类反馈，人工标注数据 （2）训练奖励模型 （3）采用PPO强化学习，优化策略<br><img src="/../article_img/RLHF/img1-1.png" alt="PPO训练流程"></p><p>在LLM上使用PPO算法进行post-training时，主要涉及4个model:</p><ul><li><strong>Actor Model</strong>： 是我们要优化学习的策略模型，同时用于做数据采样，用SFT Model热启</li><li><strong>Reference Model</strong>： 是为了控制Actor模型学习的分布与原始模型的分布相差不会太远的参考模型，通过loss中增加KL项，来达到这个效果。训练过程中该模型不更新</li><li><strong>Critic Model</strong>：是对每个状态做打分的价值模型，衡量当前token到生成结束的整体价值打分，一般可用Reward Model热启</li><li><strong>Reward Model</strong>：对整个生成的结果打分，是事先训练好的Reward Model。训练过程中该模型不更新</li></ul><h3 id="2-DPO算法"><a href="#2-DPO算法" class="headerlink" title="2. DPO算法"></a>2. DPO算法</h3><p>$L_{DPO}(\pi_\theta; \pi_{ref}) &#x3D; -E_{(x,y_w,y_l)\sim D}[log \sigma(\beta log\frac{\pi_\theta(y_w|x)}{\pi_{ref}(y_w|x)} - \beta log\frac{\pi_\theta(y_l|x)}{\pi_{ref}(y_l|x)})]$<br>DPO相比于PPO算法，由于它直接定义偏好损失作为策略来优化LLM，省去了奖励模型的定义，因此在post-training的流程上更为简练(PPO: SFT-&gt;RM-&gt;PPO)</p><p>如何理解DPO loss:<br>开始训练时，Reference model和Policy model都是同一个模型，在训练过程中Reference model不更新权重。我们将loss公式中的log部分展开，然后设$\beta$为1，并且暂时不看前面的log_sigmoid。从而将优化函数简化为：$[logp(y_w) - logp_{ref}(y_w)]-[logp(y_l)-logp_{ref}(y_l)]$</p><p>训练的目标为将优化函数最大化。即我们希望公式的左半部分和右半部分的margin越大越好，左半部分的含义是good response相较于没训练之前的累计概率差值，右半部分代表bad response相较于没训练之前的累计概率差值</p><h3 id="3-KTO算法"><a href="#3-KTO算法" class="headerlink" title="3. KTO算法"></a>3. KTO算法</h3><p>DPO算法中依赖的训练数据为：问题——期望回答——拒绝回答。高质量的偏好数据收集较为困难。KTO则回避了这个问题，可以直接利用二元信号标记的数据来训练算法，对于负样本更加敏感</p><p><strong>前景价值函数</strong>：决策者根据实际收益或损失所产生的主观感受的价值。（决策时，相对于收益，决策者对损失更加敏感）</p><div align="center">$$v(z,z_{ref};\lambda,a) =\begin{cases} (z - z_{ref})^\alpha & \text{if } z > z_{ref} \\-\lambda(z_{ref} - z)^\alpha & \text{if } z < z_{ref}\end{cases}$$</div><p>KTO算法的loss如下：<br>$L_{KTO}(\pi_\theta; \pi_{ref}) &#x3D; E_{(x,y)\sim D}[\lambda_y - v(x,y)]$</p><div align="center">$$v(x,y)=\begin{cases} \lambda_D \sigma(\beta(r_\theta(x,y)-z_0)) & if y \sim y_{desirable}|x \\-\lambda_U \sigma(\beta(z_0 - r_\theta(x,y))) & if y \sim y_{undesirable}|x\end{cases}$$</div>其中，$z_0 = KL(\pi_\theta(y'|x)||\pi_{ref}(y'|x))$， $r_\theta(x,y)=log \frac{\pi_\theta(y|x)}{\pi_{ref}(y|x)}$<h3 id="4-GRPO算法"><a href="#4-GRPO算法" class="headerlink" title="4.GRPO算法"></a>4.GRPO算法</h3><p>在PPO算法中，每个时间步都有一个价值网络去估计优势函数，这使得训练资源消耗巨大。为了减少在post-training的过程中对价值网络的依赖，GRPO采用“分组输出相互比较”的方式来估计基线，从而省去Critic Model。<br><strong>关键点：分组采样与相对奖励</strong><br>对于一个问题q，Actor Model会产生多份输出$o_1,o_2,…,o_G$，然后将这组输出一起送入奖励模型，得到奖励集合$r_1,r_2,…,r_G$。然后对奖励集合进行归一化$\tilde{r_i}&#x3D;(r_i-mean(r))&#x2F;std(r)$，从而得到分组内的相对水平，即<strong>相对奖励</strong>。GRPO会将相对奖励赋给该输出对应的所有token的优势函数。即$\hat{A}_{i,t} &#x3D; \tilde{r_i}$，也就是说，<strong>输出$o_i$的所有token共享同一个分数</strong>，于是我们得到了一个无价值网络的优势函数。</p><p>综上所述，因为不再需要在每个token上拟合一个价值函数，GRPO可以大幅节省内存。</p><p>下面给出GRPO的loss function:<br>$L_{GRPO}&#x3D; E[\frac{1}{G}\sum_{i&#x3D;1}^G \frac{1}{||o_i||}\sum_{t&#x3D;1}^{||o_i||} min(\frac{\pi_\theta(o_{i,t}|q,o_{i,&lt;t})}{\pi_{ref}(o_{i,t}|q,o_{i,&lt;t})},clip(\frac{\pi_\theta(o_{i,t}|q,o_{i,&lt;t})}{\pi_{ref}(o_{i,t}|q,o_{i,&lt;t})},1-\epsilon,1+\epsilon)) \cdot \hat{A_{i,t}} -\beta KL(\pi_\theta, \pi_{ref})]$<br><img src="/../article_img/RLHF/img1-2.png" alt="GRPO vs PPO"></p><p>Ques: 如何使用GRPO进行过程监督</p>]]></content>
    
    
    
    <tags>
      
      <tag>LLM</tag>
      
      <tag>RLHF</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>2025.03论文阅读</title>
    <link href="/2025/04/14/20250414/"/>
    <url>/2025/04/14/20250414/</url>
    
    <content type="html"><![CDATA[<ol><li>ReARTeR: Retrieval-Augmented Reasoning with Trustworthy Process Rewarding<br>基于过程奖励模型PRM的检索增强系统存在以下挑战：<br>（1）缺乏解释：现有的PRM通常会生成无法解释的标量分数 （2）PRM训练数据的分布偏差：通过蒙特卡洛树方法收集过程监督数据集往往会导致分布偏差，其中大多数问题会获得不成比例的高分。因此，PRM难以识别错误的步骤 （3）PRM的早期步骤偏差：由于早期步骤的随机性和不确定性较大，与更接近推理终点的步骤相比，PRM在早期的推理步骤中预测奖励的准确率较低 （4）缺乏推理优化：这些方法依赖现成的LLM作为生成器，没有在post-training阶段加入推理优化</li></ol><p>论文提出了一种<strong>可信过程奖励机制</strong>，来增强基于PRM的RAG推理。可信过程奖励通过两种模型进行实现：（1）过程奖励模型PRM，他提供的标量分数虽然准确，但缺乏可解释性 （2）过程解释模型PEM，它为PRM的分数生成自然语言解释，有助于优化分数较低的早期步骤<br>作者在post-training阶段引入了步骤级（step-level）离线强化微调，来增强RAG系统的推理能力。在每次迭代中，使用基于可信过程奖励指导的蒙特卡洛树来生成高质量的step-level偏好数据，这些数据用于优化模型，从而显著提升系统的推理性能。</p><p>ReARTeR的策略模型$\pi_\theta$包括一个生成器G(可进行post-training进行推理增强)，以及一个检索器E。此外，还结合了过程奖励模型R和过程解释模型C。</p>]]></content>
    
    
    
    <tags>
      
      <tag>Recsys</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Syncthing安装</title>
    <link href="/2025/03/25/syncthing%E5%AE%89%E8%A3%85/"/>
    <url>/2025/03/25/syncthing%E5%AE%89%E8%A3%85/</url>
    
    <content type="html"><![CDATA[<h3 id="下载安装"><a href="#下载安装" class="headerlink" title="下载安装"></a>下载安装</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-comment"># 1、下载最新部署包</span><br>curl -s https://api.github.com/repos/syncthing/syncthing/releases/latest | grep browser_download_url | grep linux-amd64 | <span class="hljs-built_in">cut</span> -d <span class="hljs-string">&#x27;&quot;&#x27;</span> -f 4 | wget -qi -<br><span class="hljs-comment"># 2、解压并安装</span><br>tar -xvf syncthing-linux-amd64-v1.28.1.tar.gz<br><span class="hljs-built_in">mv</span> syncthing-linux-amd64-v1.28.1/syncthing /usr/bin/<br><span class="hljs-comment"># 3、启动并且测试</span><br>syncthing<br></code></pre></td></tr></table></figure><h3 id="开机自启"><a href="#开机自启" class="headerlink" title="开机自启"></a>开机自启</h3><p>如果要立刻启动<strong>syncthing</strong>，直接使用命令 <code>syncthing</code> 即可，但这样运行十分不优雅，因此可以使用systemd配置开机自启。<br>创建一个新的 systemd 服务文件：</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">sudo</span> nano /etc/systemd/system/syncthing.service<br></code></pre></td></tr></table></figure><figure class="highlight ini"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><code class="hljs Ini"><span class="hljs-section">[Unit]</span><br><span class="hljs-attr">Description</span>=Syncthing - Open Source Continuous File Synchronization<br><span class="hljs-attr">After</span>=work.target<br><span class="hljs-section">[Service]</span><br><span class="hljs-attr">User</span>=&lt;your_user_name&gt;<br><span class="hljs-attr">ExecStart</span>=/usr/bin/syncthing -<span class="hljs-literal">no</span>-browser -<span class="hljs-literal">no</span>-restart -logflags=<span class="hljs-number">0</span><br><span class="hljs-attr">Restart</span>=<span class="hljs-literal">on</span>-failure<br><span class="hljs-attr">SuessExitStatus</span>=<span class="hljs-number">3</span> <span class="hljs-number">4</span><br><span class="hljs-attr">RestartForceExitStatus</span>=<span class="hljs-number">3</span> <span class="hljs-number">4</span><br><span class="hljs-section">[Install]</span><br><span class="hljs-attr">WantedBy</span>=multi-user.target<br></code></pre></td></tr></table></figure><p>在User处填上用户名，然后重启systemd</p><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><code class="hljs bash"><span class="hljs-built_in">sudo</span> systemctl daemon-reload<br>systemctl --user restart syncthing<br>systemctl --user status syncthing<br></code></pre></td></tr></table></figure><p>查看当前syncthing进程判断是否正常运行： <code>ps aux | grep syncthing</code></p><h3 id="本地网络访问"><a href="#本地网络访问" class="headerlink" title="本地网络访问"></a>本地网络访问</h3><p>默认情况下，Syncthing 的 Web GUI 只监听 localhost:8384，所以如果在本地网络访问远程服务器的syncthing服务，需要修改监听地址</p><ol><li>编辑配置文件<figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><code class="hljs bash">nano ~/.config/syncthing/config.xml<br></code></pre></td></tr></table></figure>如果不知道syncthing的位置的话，可以在命令行输入<code>syncthing -paths</code>找到 <code>config.xml</code> 这项</li><li>找到<code>&lt;gui&gt;</code>节点，修改其中的 address 字段：<figure class="highlight xml"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><code class="hljs xml"><span class="hljs-tag">&lt;<span class="hljs-name">gui</span> <span class="hljs-attr">enabled</span>=<span class="hljs-string">&quot;true&quot;</span> <span class="hljs-attr">tls</span>=<span class="hljs-string">&quot;false&quot;</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">address</span>&gt;</span>0.0.0.0:8384<span class="hljs-tag">&lt;/<span class="hljs-name">address</span>&gt;</span>   <span class="hljs-comment">&lt;!-- ← 这里改成 0.0.0.0 表示监听所有地址 --&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">apikey</span>&gt;</span>你的apikey<span class="hljs-tag">&lt;/<span class="hljs-name">apikey</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">user</span>&gt;</span>用户名<span class="hljs-tag">&lt;/<span class="hljs-name">user</span>&gt;</span><br>    <span class="hljs-tag">&lt;<span class="hljs-name">password</span>&gt;</span>密码（Base64编码）<span class="hljs-tag">&lt;/<span class="hljs-name">password</span>&gt;</span><br><span class="hljs-tag">&lt;/<span class="hljs-name">gui</span>&gt;</span><br></code></pre></td></tr></table></figure></li></ol>]]></content>
    
    
    
    <tags>
      
      <tag>Tool</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>强化学习系列（五）：Policy Gradient</title>
    <link href="/2025/03/15/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%EF%BC%88%E4%BA%94%EF%BC%89/"/>
    <url>/2025/03/15/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%EF%BC%88%E4%BA%94%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<p>写在前面：前面所提到的Q-value Based方法无法解决连续动作空间场景下的优化问题，因为Q-learning的策略是从多个离散动作中贪婪地选择最大Q值，在连续空间中，无法枚举所有动作。为此，本节讲述一种<strong>直接面向策略</strong>的优化方法：Policy Gradient</p><span id="more"></span><h3 id="1-Policy-Gradient"><a href="#1-Policy-Gradient" class="headerlink" title="1. Policy Gradient"></a>1. Policy Gradient</h3><p>训练目标：$\pi_\theta(s,a)&#x3D;P[a|s,\theta]$<br>Ques: 什么样的策略算是好的策略,如何进行梯度更新<br>Ans:<br><code>think</code> $\theta$会影响策略$\pi$,$\pi$会影响episode采样路径$\tau&#x3D;{s_1,a_1,r_1,s_2,a_2,r_2,…,s_T,a_T,r_T}$，根据$\tau$, 我们可以算出$R(\tau)&#x3D;\sum_{n&#x3D;1}^N r_n$，我们希望采样路径的$R(\tau)$尽可能的大<br><code>method</code><br>用采样路径的平均收益来评价一个$\theta$的好坏<br>$\bar R_\theta &#x3D; \sum_{\tau} R(\tau) \cdot P(\tau|\theta) \approx \frac{1}{N} \sum_{n&#x3D;1}^{N} R(\tau^n)$<br>推导$\bar R_\theta$的梯度:<br>$$\nabla \bar R_\theta&#x3D;\sum_\tau R(\tau) \nabla P(\tau|\theta)&#x3D;\sum_\tau R(\tau)P(\tau|\theta) \frac{\nabla P(\tau|\theta)}{P(\tau|\theta)}&#x3D;<br>\sum_\tau R(\tau)P(\tau|\theta)\nabla logP(\tau|\theta) \approx \frac{1}{N}\sum_{n&#x3D;1}^N R(\tau^n) \nabla log P(\tau^n|\theta)$$<br>其中，$P(\tau|\theta)&#x3D;p(s_1)p(a_1|s_1,\theta)p(r_1,s_2|s_1,a_1)p(a_2|s_2,\theta)…$<br>由此，我们有：$log P(\tau|\theta)&#x3D;logp(s_1)+\sum_{t&#x3D;1}^Tlogp(a_t|s_t,\theta)+logp(r_t,s_{t+1}|s_t,a_t)$<br>$\nabla logP(\tau|\theta)&#x3D;\sum_{t&#x3D;1}^T\nabla logp(a_t|s_t, \theta)$<br>由此，我们可以将policy gradient视为一个分类任务对$\nabla \bar R_\theta$进行更新<br><img src="/../article_img/Reinforcement/img5-2.png" alt="Gradient Descent"><br>类比分类任务：<br><img src="/../article_img/Reinforcement/img5-1.png" alt="Classification Task"></p><h3 id="2-Actor-Critic"><a href="#2-Actor-Critic" class="headerlink" title="2. Actor-Critic"></a>2. Actor-Critic</h3><p>在Policy Gradient方法中，我们在更新$\nabla \bar R_\theta$时，借助累计收益$R(\tau^n)$作为优势估计，然而在一些情况下，虽然$R(\tau^n)$是positive的，但是效益还比不过随机效益的话，其实也是不能接受的。因此，需要add a baseline去进一步评估positive or negtive，从而更好地指导$\nabla logP(\tau|\theta)$的更新</p><p>核心思想：<br>（1）Actor:学习策略$\pi_\theta(a|s)$，负责决策<br>（2）Critic:学习一个价值函数 $V_w(s)$，估计状态的“好坏”，指导 Actor 改进策略</p><p>梯度更新：<br>$$\nabla \bar R_\theta \approx \frac{1}{N}\sum_{n&#x3D;1}^{N}\sum_{t&#x3D;1}^{T_n}(R_\tau^n-baseline) \nabla log P_\theta(a_t^n|s_t^n)$$<br>其中$R_\tau^n$可以表示为$E[G_t^n]&#x3D;Q^{\pi_\theta}(s_t^n,a_t^n)$，baseline表示为$V^{\pi_\theta}(s_t^n)$</p><p>Ques: 需要estimate Q和V两个网络吗<br>Ans: 可以只estimate value network<br>利用$Q^{\pi_\theta}(s_t^n,a_t^n)&#x3D;E[r_t^n + V^{\pi_\theta}(s_{t+1}^n)] \approx r_t^n + V^{\pi_\theta}(s_{t+1}^n)$<br>我们可以得到<strong>Advantage Function</strong>：$Q^{\pi_\theta}(s_t^n,a_t^n)-V^{\pi_\theta}(s_t^n)&#x3D;r_t^n - (V^{\pi_\theta}(s_t^n)-V^{\pi_\theta}(s_{t+1}^n))$</p><h3 id="summary"><a href="#summary" class="headerlink" title="summary"></a>summary</h3><p><img src="/../article_img/Reinforcement/img5-5.png" alt="summary"></p>]]></content>
    
    
    
    <tags>
      
      <tag>AI</tag>
      
      <tag>Algorithm</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>强化学习系列（四）：DQN算法</title>
    <link href="/2025/03/14/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%EF%BC%88%E5%9B%9B%EF%BC%89/"/>
    <url>/2025/03/14/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%EF%BC%88%E5%9B%9B%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<p>写在前面：前面讲解了在On-Policy和Off-Policy环境下如何进行策略提升的几种常用方法，但是在实际应用场景中，state的数量可能是非常庞大的，为了存储每个state-action pair所需要的lookup-table所需要的空间会很庞大，无法通过遍历的方式去evaluate每个state-action pair。这就需要我们考虑如何对Q，V进行建模，通过函数逼近的方法估计连续空间下的state value</p><span id="more"></span><h3 id="1-Value-Approximation"><a href="#1-Value-Approximation" class="headerlink" title="1. Value Approximation"></a>1. Value Approximation</h3><p>Target: 寻找到合适的参数w使得mse loss尽可能的小  $J(w)&#x3D;E_\pi[(v_\pi(S)-\hat{v}(S,w))^2]$<br>通过梯度下降来更新w: $\Delta w&#x3D;-\frac{1}{2}\alpha \nabla_w J(w)&#x3D;\alpha(v_\pi(S)-\hat{v}(S,w))\nabla_w\hat{v}(S_t,w)$</p><p>使用特征向量表示state $x(S)&#x3D;(x_1(S)…x_n(S))$<br>$\hat{v}(S_t,w)$可以通过线性加权得到，即$\hat{v}(S,w)&#x3D;x(S)^Tw&#x3D;\sum_{j&#x3D;1}^n x_j(S)w_j$ </p><p>于是我们便有：$\Delta w&#x3D;\alpha(v_\pi(S)-\hat{v}(S_t,w))x(S)$</p><p>现在思考一个问题，我们如何得到$v_\pi(S)$的真实表示？依然是采用MC&#x2F;TD等方法进行泛化估计<br>MC: $\Delta w&#x3D;\alpha(G_t-\hat{v}(S,w))\nabla_w\hat{v}(S_t,w)$<br>TD(0): $\Delta w&#x3D;\alpha(R_{t+1}+\gamma\hat{v}(S_{t+1},w)-\hat{v}(S_t,w))\nabla_w\hat{v}(S_t,w)$</p><h3 id="2-Replay-Buffer"><a href="#2-Replay-Buffer" class="headerlink" title="2. Replay Buffer"></a>2. Replay Buffer</h3><p>replay buffer是一个 <strong>存储智能体交互经验</strong> 的数据结构（通常是一个队列或数组），用来保存智能体与环境交互的轨迹样本：$(S_t, A_t, S_{t+1}, A_{t+1}, <code>done</code>)$<br>✅ 打破时间相关性 (Break Correlation)<br>强化学习中的数据是按时间顺序生成的，彼此高度相关。Replay Buffer让我们从过去的经验中随机采样，打乱数据顺序，降低相关性，提升训练稳定性。</p><p>✅ 提高数据利用率 (Improve Data Efficiency)<br>每次交互生成的数据如果只用一次就丢掉，效率太低了。Replay Buffer让数据能被多次复用，大大提高训练效率。</p><p>✅ 平滑训练 (Smooth Training)<br>缓冲区里的经验包括成功和失败的多种场景，防止模型被最近的几次失败经验“误导”得太严重。</p><p>✅ 支持 Off-Policy 学习<br>DQN 是 Off-Policy 算法，目标策略是贪婪策略，行为策略是<strong>ϵ-greedy</strong>。Replay Buffer让我们从过去的经验中训练，不必跟当前行为策略严格匹配。</p><h3 id="3-DQN算法"><a href="#3-DQN算法" class="headerlink" title="3. DQN算法"></a>3. DQN算法</h3><p>DQN算法是结合了Q-Learning和深度神经网络的强化学习算法。它用深度神经网络替代传统的Q表格，近似的学习Q值函数：$Q(s,a;\theta) \approx Q*(s,a)$<br>下面对该算法进行介绍：<br>（1）经验回放（Experience Replay）<br>在传统Q-Learning中，每次训练都用最新的数据更新Q值，可能导致网络过拟合最近的经验，收敛不稳定。<br>DQN引入经验回放机制，将每次经历 (s, a, r, s’) 保存到回放缓冲池（Replay Buffer），每次训练时随机采样一批数据进行训练。<br>（2）目标网络（Target Network）<br>DQN使用<strong>在线网络</strong>和<strong>目标网络</strong>两个网络，前者用于实时更新，输出$Q(s,a;\theta)$;后者每隔一段时间赋值在线网络参数$\theta$到目标网络$\theta’$，输出$Q(s’,a’;\theta’)$<br>目标网络的Q值用作训练的目标： $Loss&#x3D;(r + \gamma max_{a’}Q(s’,a’;\theta’)-Q(s,a;\theta))^2$<br><img src="/../article_img/Reinforcement/img4-1.png" alt="DQN framework"><br>（3）使用Epsilon Greedy策略在探索与利用之间做权衡</p><p>下面给出DQN算法的伪代码：<br><img src="/../article_img/Reinforcement/img4-2.png" alt="DQN Algorithm"></p><p>Ques: DQN网络如何实现端到端的学习<br>Ans: 中间层是常规的卷积网络用来学习state表征，输出层通过映射到一个output空间，定义每个维度代表一个value</p><h3 id="4-DQN算法的延伸"><a href="#4-DQN算法的延伸" class="headerlink" title="4. DQN算法的延伸"></a>4. DQN算法的延伸</h3><h4 id="Double-DQN"><a href="#Double-DQN" class="headerlink" title="Double DQN"></a>Double DQN</h4><p>DQN算法存在的问题：使用同一个网络既选择动作又评估价值，可能会导致over estimate的问题<br>改进方案：$Q(s_t,a_t) \leftarrow r_t + Q’(s_{t+1}, argmax_a Q(s_{t+1}，a))$ 内层的Q用来选择action，外层的Q用来评估价值</p><h4 id="Dueling-DQN"><a href="#Dueling-DQN" class="headerlink" title="Dueling DQN"></a>Dueling DQN</h4><p>DQN算法存在的问题：在很多状态下，动作的价值差别不大。比如：游戏中站在原地等待 vs 随便走一步 这种情况下，DQN仍然必须给每个动作单独估计 Q 值，浪费了大量学习能力，导致训练效率下降。<br>Dueling DQN的核心思想：将Q值分成两部分，<strong>状态值函数 V(s)：</strong>描述当前状态本身的“好坏” <strong>优势函数 A(s, a)：</strong>描述某个具体动作比其他动作好多少<br>于是，我们便有：$Q(s,a)&#x3D;V(s)+A(s,a)$<br>优点：状态价值主导训练，不用每次都算所有动作；更高效地学习，尤其在动作多、动作差别小的场景；收敛更快，性能更好<br><img src="/../article_img/Reinforcement/img4-3.png" alt="Dueling DQN"></p><h4 id="Batch-Constraint-DQN-BCQ"><a href="#Batch-Constraint-DQN-BCQ" class="headerlink" title="Batch-Constraint DQN(BCQ)"></a>Batch-Constraint DQN(BCQ)</h4><p>DQN算法存在的外推误差（extrapolation error）问题：Q网络在没见过的state-action pair上会over estimate Q值<br>问题的根源在于：<strong>有限的数据集：</strong>训练数据无法覆盖所有可能的状态-动作对。<strong>目标网络延迟更新：</strong>目标 Q 值用的是旧网络（延迟更新），可能不准确。<strong>最大化操作带来的偏差：</strong>maxQ 倾向于选更高估的值。<br>核心思想：尽可能让agent action和batch中sample action产生的结果相近，即<strong>约束策略不要偏离已收集的数据分布，同时最大化回报</strong></p><p>下面给出BCQ算法的伪代码：<br><img src="/../article_img/Reinforcement/img4-4.png" alt="BCQ Algorithm"><br>可以看到，在行为选择时，除了考虑高Q值以外，我们还希望它能够接近数据分布的action，伪代码的第5行中我们通过设定阈值的方式筛选掉了一些发生概率较低的action</p><h4 id="Distributional-Q-function"><a href="#Distributional-Q-function" class="headerlink" title="Distributional Q-function"></a>Distributional Q-function</h4><p>DQN算法存在的问题：在DQN中，我们学的是Q值的期望，$Q(s,a)&#x3D;E[R(s,a)]$ 但问题是，期望值无法描述奖励的不确定性和波动。也就是说，DQN 只学到了平均回报，没学到“回报的分布情况”。<br>👉 举个例子：假设有两个动作$a_1$和$a_2$，它们的期望回报都是 10。$a_1$的回报是 [10, 10, 10]，非常稳定。$a_2$的回报是 [0, 10, 20]，波动很大。传统 DQN 认为它俩一样好，但显然$a_1$更可靠<br>核心观点：学习回报的“完整分布” $Q(s,a)&#x3D;E[Z(s,a)]$ 这里的Z(s,a)为一个概率分布<br>step：定义回报范围$[v_min,v_max]$,划分为n个小区间；学习每个bin的概率$p_i$,形成一个回报的离散分布；训练目标变成让网络预测整个分布，而不是单一Q值</p>]]></content>
    
    
    
    <tags>
      
      <tag>AI</tag>
      
      <tag>Algorithm</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>强化学习系列（三）：Model-Free Control</title>
    <link href="/2025/03/12/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%EF%BC%88%E4%B8%89%EF%BC%89/"/>
    <url>/2025/03/12/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%EF%BC%88%E4%B8%89%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<p>写在前面：系列二中提及的MC&#x2F;TD方法都是在已知策略$\pi$的前提下，估计每个状态的期望回报。前者是等到整个回合结束利用完整回报$G_t$来更新价值函数,后者利用一步预测和当前奖励动态更新价值函数。可以看到的是，这些方法知识学习了价值函数，并没有改变策略。在这一节，我们主要介绍一些常用的策略优化方法。</p><span id="more"></span><h3 id="1-Epsilon-Greedy"><a href="#1-Epsilon-Greedy" class="headerlink" title="1. Epsilon Greedy"></a>1. Epsilon Greedy</h3><p>在Model-based Control中，我们基于MDP Transition采用贪心策略进行policy improvement：$\pi’(s)&#x3D;argmax_{a \in A}R_s^a + P_{ss’}^aV(s’)$<br>在Model-free背景下，由于缺失MDP Transition，往往采用对行为价值函数Q(s,a)进行建模: $\pi’(s)&#x3D;argmax_{a \in A}Q(s,a)$<br>这里介绍一个常用的贪心策略Epsilon Greedy：</p><div align="center">$$\pi(a|s) =\begin{cases} \frac{\epsilon}{m} + 1 - \epsilon & \text{if } a = \arg\max\limits_{a \in A} Q(s,a) \\\frac{\epsilon}{m} & \text{otherwise}\end{cases}$$</div>why to search bad state: 避免局部最优解，通过增加对不良状态的探索，智能体能够全面地了解环境，发现可能被忽略的潜在机会，进而提升整体策略的质量。<p>&lt;定义&gt;Greedy in the limit with infinite exploration(GLIE)<br>随着时间的推进，策略最终收敛到纯贪婪策略；在训练过程中，每个状态和动作都要被探索无限次，确保获得充分的信息来评估所有可能的动作。<br>即GLIE要求策略$\pi_t$满足以下条件：<br>(1) $lim_{k-&gt;\infty}N_k(s,a)&#x3D;\infty$<br>(2) $lim_{k-&gt;\infty}\pi_k(a|s)&#x3D;1(a&#x3D;argmax_{a’ \in A}Q_k(s,a’))$</p><p>GLIE的意义：（1）保证收敛 （2）避免局部最优</p><p>Epsilon Greedy满足GLIE：在探索的过程中逐渐减少探索率$\epsilon$，例如$\epsilon_t&#x3D;1&#x2F;t$</p><p>下面介绍下$\epsilon$-Greedy与Monte-Carlo结合的Control方法：<br>(1) 使用当前策略$\pi$与环境进行第k次完整交互(kth episode)得到采样数据： ${S_1,A_1,R_2,…,,S_T} \sim \pi$<br>(2) 使用MC方法对进行Q值更新：$N(S_t,A_t) \leftarrow N(S_t,A_t) + 1$, $Q(S_t, A_t) \leftarrow Q(S_t,A_t) + \frac{1}{N(S_t,A_t)}(G_t - Q(S_t, A_t))$<br>(3) 基于\epsilon-Greedy进行策略提升：$\epsilon \leftarrow 1&#x2F;k$, $\pi \leftarrow \epsilon-greedy(Q)$</p><h3 id="2-SARSA"><a href="#2-SARSA" class="headerlink" title="2. SARSA"></a>2. SARSA</h3><p>将TD方法和GLIE结合的Control方法我们称其为SARSA。<br><img src="/../article_img/Reinforcement/img3-1.png" alt="SARSA"><br>下面是SARSA进行Q值更新和策略提升的伪代码:<br><img src="/../article_img/Reinforcement/img3-2.png" alt="SARSA Algorithm"></p><p>n-step SARSA:<br>$q_t^{(n)}&#x3D;R_{t+1} + \gamma R_{t+2} + … + \gamma^{n-1}R_{t+n} + \gamma^n Q(S_{t+n})$<br>$Q(S_t,A_t) \leftarrow Q(S_t,A_t) + \alpha (q_t^{(n)}-Q(S_t,A_t))$</p><h3 id="3-Off-Policy-Learning"><a href="#3-Off-Policy-Learning" class="headerlink" title="3. Off-Policy Learning"></a>3. Off-Policy Learning</h3><p>on policy learning:策略学习只能使用 <strong>当前策略</strong> 生成的数据<br>off policy learning: 策略学习可以使用 <strong>其他策略</strong> 生成的数据，包括历史数据和经验回放<br>​特点：训练策略和行为策略可以不同；可以使用 <strong>过去的数据（经验回放）</strong> 进行训练，提高数据利用率<br>对于off-policy learning，由于行为策略和训练策略并不相同，行为策略$\mu$产生的数据可能不符合目标策略$\pi$的分布，直接使用它来更新目标策略会产生偏差。为了修正分布偏差，可以采用Importance Sampling方法。<br>important sampling:  <strong>用于估计一个分布的期望值，而采样数据却来自另一个分布</strong> 的技术。它通过调整采样分布的影响，来修正采样偏差，使得估计值更准确。<br>$E_{X\sim P}[f(X)]&#x3D;\sum P(X)f(X)&#x3D;\sum Q(X)\frac{P(X)}{Q(X)}f(X)&#x3D;E_{X\sim Q}[\frac{P(X)}{Q(X)}f(X)]$</p><p>下面介绍Importance Sampling for Off-Policy Monte-Carlo:<br>Target: 通过model-free control学习一个目标策略$\pi$,但是采样数据来自于行为策略$\mu$<br>在这里，我们称$\mu$为行为策略，它负责生成训练数据；$\pi$为目标策略，即我们希望学习的策略<br>Step：假设一个完整的episode采样序列：$\tau &#x3D; {S_1, A_1, R_2,…,S_T}$<br>在目标策略$\pi$中的回报期望为 $G_t^{\pi&#x2F;\mu} &#x3D; \frac{\pi(A_t|S_t)}{\mu(A_t|S_t)}\frac{\pi(A_{t+1}|S_{t+1})}{\mu(A_{t+1}|S_{t+1})}…\frac{\pi(A_T|S_T)}{\mu(A_T|S_T)}G_t$<br>更新Q值函数：$Q(S_t,A_t) \leftarrow Q(S_t,A_t) + \alpha(G_t^{\pi&#x2F;\mu}-Q(S_t,A_t))$</p><p>Ques: 为什么$\pi(A_t|S_t)$已知却不能直接用$\pi来更新Q值$<br>Ans: 我们手头的数据是行为策略 μ 生成的，而不是目标策略 π 生成的。如果我们直接用目标策略 π 来更新Q值，实际上我们是在<strong>假设数据是按目标策略采样的，但数据实际是按行为策略采样的</strong>。这会导致估计产生偏差。</p><h3 id="4-Q-Learning"><a href="#4-Q-Learning" class="headerlink" title="4. Q-Learning"></a>4. Q-Learning</h3><p>Q-learning是一种Off-Policy Temporal Difference Learning的控制方法，它可以借助行为策略产生的数据而无需Importance Sampling<br>公式：$Q(S,a) \leftarrow Q(S,a) + \alpha [R+\gamma max_{a’}Q(s’,a’)-Q(s,a)]$<br><img src="/../article_img/Reinforcement/img3-4.png" alt="Q-Learning"></p><p>Ques: 为什么Q-learning不需要重要性采样<br>Ans:<br>（1）Q-learning 的目标策略始终是贪婪策略（$max_{a’}Q(s’,a’)$），但行为策略可以是探索性的（比如 ϵ-greedy）。因为它只用目标策略来选择下一个状态的最优动作，不需要纠正行为策略带来的偏差。<br>（2）它用的是单步TD更新，不是完整回报。TD更新只依赖当前经验(s,a,r,s’)，所以行为策略只需要提供足够多样的数据，不要求它和目标策略一致</p><p>Ans:无法与真实环境交互，如何训练目标策略<br>Ques: Simulator和Batch RL<br>Simulator: 强化学习中的一个环境，允许Agent在其中交互、收集数据并进行训练。<strong>适用于Online RL（在线强化学习）</strong>：算法可以随时与环境交互并收集数据（如 DQN、PPO）。<br>Batch RL：它仅使用一个固定的数据集来训练，而不会与环境交互。<strong>需要 Off-Policy 方法</strong>：由于数据可能来自多个不同的策略（而非当前策略），必须使用 <strong>Off-Policy RL</strong>（如 DQN、BCQ）</p><p>区分Q-learning和SARSA的Q值更新：<br>SARSA的Q值更新公式为：$Q(S_t,A_t) \leftarrow Q(S_t,A_t) + \alpha[R_{t+1}+\gamma Q(S_{t+1},A_{t+1})-Q(S_t,A_t)]$<br>Q-learning用的是下一状态的最优动作$max_{a’}Q(s’,a’)$来更新，属于Off-Policy。SARSA用的是实际选到的下一动作$A_{t+1}$来更新，属于On-Policy。</p><p>下面给出Q-Learning的伪代码：<br><img src="/../article_img/Reinforcement/img3-5.png" alt="Q-Learning Algorithm"></p><h3 id="Summary"><a href="#Summary" class="headerlink" title="Summary"></a>Summary</h3><p><img src="/../article_img/Reinforcement/img3-6.png" alt="Summary"></p>]]></content>
    
    
    
    <tags>
      
      <tag>AI</tag>
      
      <tag>Algorithm</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Lora Adapter调试跟踪</title>
    <link href="/2025/03/11/Lora%20Adapter%E8%B0%83%E8%AF%95%E8%B7%9F%E8%B8%AA/"/>
    <url>/2025/03/11/Lora%20Adapter%E8%B0%83%E8%AF%95%E8%B7%9F%E8%B8%AA/</url>
    
    <content type="html"><![CDATA[<p>打算花点时间看看在peft库中lora是怎么注入base model的，这里简单总结下：</p><p>首先写个测试程序：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">import</span> torch<br><span class="hljs-keyword">from</span> peft <span class="hljs-keyword">import</span> LoraModel, LoraConfig<br><span class="hljs-keyword">from</span> transformers <span class="hljs-keyword">import</span> AutoModelForCausalLM, AutoTokenizer<br><br><br>model = AutoModelForCausalLM.from_pretrained(<span class="hljs-string">&#x27;meta-llama/Llama-2-7b-hf&#x27;</span>, torch_dtype=torch.float16, device_map=<span class="hljs-string">&quot;cuda&quot;</span>)<br>tokenizer = AutoTokenizer.from_pretrained(<span class="hljs-string">&#x27;meta-llama/Llama-2-7b-hf&#x27;</span>)<br>tokenizer.pad_token = tokenizer.eos_token<br>lora_config = LoraConfig(<br>    r=<span class="hljs-number">32</span>,<br>    lora_alpha=<span class="hljs-number">16</span>,<br>    target_modules=[<span class="hljs-string">&quot;gate_proj&quot;</span>,<span class="hljs-string">&quot;up_proj&quot;</span>,<span class="hljs-string">&quot;q_proj&quot;</span>,<span class="hljs-string">&quot;down_proj&quot;</span>,<span class="hljs-string">&quot;o_proj&quot;</span>,<span class="hljs-string">&quot;k_proj&quot;</span>,<span class="hljs-string">&quot;v_proj&quot;</span>],<br>    lora_dropout=<span class="hljs-number">0.05</span>,<br>    bias=<span class="hljs-string">&quot;none&quot;</span>,<br>    task_type=<span class="hljs-string">&quot;CAUSAL_LM&quot;</span>,<br>)<br><br>model = LoraModel(model, lora_config, <span class="hljs-string">&quot;default&quot;</span>) <span class="hljs-comment"># 在这里打个断点</span><br>model.print_trainable_parameters()<br><span class="hljs-keyword">for</span> name, param <span class="hljs-keyword">in</span> model.named_parameters():<br>    <span class="hljs-built_in">print</span>(name, param)<br></code></pre></td></tr></table></figure><p>跟进去调试，LoraModel是基于BaseTuner类实现的子类，BaseTuner的init函数中有一个inject_adapter方法，该方法实现了如何将lora中的target module与base model中的module进行替换</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><code class="hljs python">elf.active_adapter: <span class="hljs-built_in">str</span> | <span class="hljs-built_in">list</span>[<span class="hljs-built_in">str</span>] = adapter_name<br><span class="hljs-variable language_">self</span>._pre_injection_hook(<span class="hljs-variable language_">self</span>.model, <span class="hljs-variable language_">self</span>.peft_config[adapter_name], adapter_name)<br><span class="hljs-keyword">if</span> peft_config != PeftType.XLORA <span class="hljs-keyword">or</span> peft_config[adapter_name] != PeftType.XLORA:<br>    <span class="hljs-variable language_">self</span>.inject_adapter(<span class="hljs-variable language_">self</span>.model, adapter_name, low_cpu_mem_usage=low_cpu_mem_usage)<br></code></pre></td></tr></table></figure><p>跟进inject_adapter,它首先会收集base model的所有named modules组成一个key list，然后进入循环，如果key在peft_config中定义的target_modules中，则调用_create_and_replace方法（@abstractmethod），该方法在子类中进行实现。下面重点介绍下该方法：</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">_create_and_replace</span>(<span class="hljs-params"></span><br><span class="hljs-params">        self,</span><br><span class="hljs-params">        lora_config,</span><br><span class="hljs-params">        adapter_name,</span><br><span class="hljs-params">        target,</span><br><span class="hljs-params">        target_name,</span><br><span class="hljs-params">        parent,</span><br><span class="hljs-params">        current_key,</span><br><span class="hljs-params">    </span>):<br>        <span class="hljs-keyword">if</span> current_key <span class="hljs-keyword">is</span> <span class="hljs-literal">None</span>:<br>            <span class="hljs-keyword">raise</span> ValueError(<span class="hljs-string">&quot;Current Key shouldn&#x27;t be `None`&quot;</span>)<br><br>        <span class="hljs-comment"># Regexp matching - Find key which matches current target_name in patterns provided</span><br>        pattern_keys = <span class="hljs-built_in">list</span>(chain(lora_config.rank_pattern.keys(), lora_config.alpha_pattern.keys()))<br>        target_name_key = <span class="hljs-built_in">next</span>(<span class="hljs-built_in">filter</span>(<span class="hljs-keyword">lambda</span> key: re.<span class="hljs-keyword">match</span>(<span class="hljs-string">rf&quot;.*\.<span class="hljs-subst">&#123;key&#125;</span>$&quot;</span>, current_key), pattern_keys), current_key)<br>        r = lora_config.rank_pattern.get(target_name_key, lora_config.r)<br>        alpha = lora_config.alpha_pattern.get(target_name_key, lora_config.lora_alpha)<br><br>        kwargs = &#123;<br>            <span class="hljs-string">&quot;r&quot;</span>: r,<br>            <span class="hljs-string">&quot;lora_alpha&quot;</span>: alpha,<br>            <span class="hljs-string">&quot;lora_dropout&quot;</span>: lora_config.lora_dropout,<br>            <span class="hljs-string">&quot;fan_in_fan_out&quot;</span>: lora_config.fan_in_fan_out,<br>            <span class="hljs-string">&quot;init_lora_weights&quot;</span>: lora_config.init_lora_weights,<br>            <span class="hljs-string">&quot;use_rslora&quot;</span>: lora_config.use_rslora,<br>            <span class="hljs-string">&quot;use_dora&quot;</span>: lora_config.use_dora,<br>            <span class="hljs-string">&quot;ephemeral_gpu_offload&quot;</span>: lora_config.runtime_config.ephemeral_gpu_offload,<br>            <span class="hljs-string">&quot;loaded_in_8bit&quot;</span>: <span class="hljs-built_in">getattr</span>(<span class="hljs-variable language_">self</span>.model, <span class="hljs-string">&quot;is_loaded_in_8bit&quot;</span>, <span class="hljs-literal">False</span>),<br>            <span class="hljs-string">&quot;loaded_in_4bit&quot;</span>: <span class="hljs-built_in">getattr</span>(<span class="hljs-variable language_">self</span>.model, <span class="hljs-string">&quot;is_loaded_in_4bit&quot;</span>, <span class="hljs-literal">False</span>),<br>        &#125;<br><br>        quant_methods = [<span class="hljs-string">&quot;gptq&quot;</span>, <span class="hljs-string">&quot;aqlm&quot;</span>, <span class="hljs-string">&quot;awq&quot;</span>]<br>        <span class="hljs-keyword">for</span> quant_method <span class="hljs-keyword">in</span> quant_methods:<br>            quantization_config = get_quantization_config(<span class="hljs-variable language_">self</span>.model, method=quant_method)<br>            <span class="hljs-keyword">if</span> quantization_config <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>                kwargs[<span class="hljs-string">f&quot;<span class="hljs-subst">&#123;quant_method&#125;</span>_quantization_config&quot;</span>] = quantization_config<br><br>        <span class="hljs-comment"># note: AdaLoraLayer is a subclass of LoraLayer, we need to exclude it</span><br>        <span class="hljs-keyword">from</span> peft.tuners.adalora <span class="hljs-keyword">import</span> AdaLoraLayer<br><br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">isinstance</span>(target, LoraLayer) <span class="hljs-keyword">and</span> <span class="hljs-keyword">not</span> <span class="hljs-built_in">isinstance</span>(target, AdaLoraLayer):<br>            target.update_layer(<br>                adapter_name,<br>                r,<br>                lora_alpha=alpha,<br>                lora_dropout=lora_config.lora_dropout,<br>                init_lora_weights=lora_config.init_lora_weights,<br>                use_rslora=lora_config.use_rslora,<br>                use_dora=lora_config.use_dora,<br>            )<br>        <span class="hljs-keyword">else</span>:<br>            new_module = <span class="hljs-variable language_">self</span>._create_new_module(lora_config, adapter_name, target, **kwargs)<br>            <span class="hljs-keyword">if</span> adapter_name <span class="hljs-keyword">not</span> <span class="hljs-keyword">in</span> <span class="hljs-variable language_">self</span>.active_adapters:<br>                <span class="hljs-comment"># adding an additional adapter: it is not automatically trainable</span><br>                new_module.requires_grad_(<span class="hljs-literal">False</span>)<br>            <span class="hljs-variable language_">self</span>._replace_module(parent, target_name, new_module, target)<br></code></pre></td></tr></table></figure><p>kwargs为构造Lora层所必需的参数，此外这里还检查了base model是否已经被8 bit or 4 bit量化，以防止不兼容的操作。然后判断target是都已经是一个LoraLayer：<strong>如果 <code>target</code> 已经是一个 LoRA 层</strong>（<code>LoraLayer</code>），则 <strong>更新</strong> 其参数（如 <code>r</code>、<code>lora_alpha</code> 等）；<strong>如果 <code>target</code> 是 <code>AdaLoraLayer</code>，则跳过</strong>，因为 AdaLoRA 有自己的适配逻辑；否则，创建新的 LoRA 层并替换目标层。self._replace_module用于 将 <code>parent</code> 中的 <code>target_name</code> 层替换为 <code>new_module</code>。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><code class="hljs python"><span class="hljs-keyword">def</span> <span class="hljs-title function_">_replace_module</span>(<span class="hljs-params">self, parent, child_name, new_module, child</span>):<br>        <span class="hljs-built_in">setattr</span>(parent, child_name, new_module)<br>        <span class="hljs-comment"># It&#x27;s not necessary to set requires_grad here, as that is handled by</span><br>        <span class="hljs-comment"># _mark_only_adapters_as_trainable</span><br><br>        <span class="hljs-comment"># child layer wraps the original module, unpack it</span><br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">hasattr</span>(child, <span class="hljs-string">&quot;base_layer&quot;</span>):<br>            child = child.base_layer<br><br>        <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-built_in">hasattr</span>(new_module, <span class="hljs-string">&quot;base_layer&quot;</span>):<br>            <span class="hljs-keyword">if</span> <span class="hljs-built_in">hasattr</span>(new_module, <span class="hljs-string">&quot;W_q&quot;</span>):  <span class="hljs-comment"># HQQ</span><br>                new_module.W_q = child.W_q<br>            <span class="hljs-keyword">else</span>:<br>                new_module.weight = child.weight<br>            <span class="hljs-keyword">if</span> <span class="hljs-built_in">hasattr</span>(child, <span class="hljs-string">&quot;bias&quot;</span>):<br>                new_module.bias = child.bias<br><br>        <span class="hljs-keyword">if</span> <span class="hljs-built_in">getattr</span>(child, <span class="hljs-string">&quot;state&quot;</span>, <span class="hljs-literal">None</span>) <span class="hljs-keyword">is</span> <span class="hljs-keyword">not</span> <span class="hljs-literal">None</span>:<br>            <span class="hljs-keyword">if</span> <span class="hljs-built_in">hasattr</span>(new_module, <span class="hljs-string">&quot;base_layer&quot;</span>):<br>                new_module.base_layer.state = child.state<br>            <span class="hljs-keyword">else</span>:<br>                new_module.state = child.state<br>            new_module.to(child.weight.device)<br><br>        meta = torch.device(<span class="hljs-string">&quot;meta&quot;</span>)<br>        <span class="hljs-comment"># dispatch to correct device</span><br>        <span class="hljs-keyword">for</span> name, module <span class="hljs-keyword">in</span> new_module.named_modules():<br>            <span class="hljs-keyword">if</span> (<span class="hljs-variable language_">self</span>.prefix <span class="hljs-keyword">in</span> name) <span class="hljs-keyword">or</span> (<span class="hljs-string">&quot;ranknum&quot;</span> <span class="hljs-keyword">in</span> name):<br>                weight = (<br>                    child.qweight<br>                    <span class="hljs-keyword">if</span> <span class="hljs-built_in">hasattr</span>(child, <span class="hljs-string">&quot;qweight&quot;</span>)<br>                    <span class="hljs-keyword">else</span> child.W_q<br>                    <span class="hljs-keyword">if</span> <span class="hljs-built_in">hasattr</span>(child, <span class="hljs-string">&quot;W_q&quot;</span>)<br>                    <span class="hljs-keyword">else</span> child.weight<br>                    <span class="hljs-keyword">if</span> <span class="hljs-built_in">hasattr</span>(child, <span class="hljs-string">&quot;weight&quot;</span>)<br>                    <span class="hljs-keyword">else</span> <span class="hljs-built_in">next</span>(child.parameters())<br>                )<br>                <span class="hljs-keyword">if</span> <span class="hljs-keyword">not</span> <span class="hljs-built_in">any</span>(p.device == meta <span class="hljs-keyword">for</span> p <span class="hljs-keyword">in</span> module.parameters()):<br>                    module.to(weight.device)<br></code></pre></td></tr></table></figure>]]></content>
    
    
    
    <tags>
      
      <tag>Code</tag>
      
      <tag>debug</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>强化学习系列（二）：Model-free Prediction</title>
    <link href="/2025/03/08/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%EF%BC%88%E4%BA%8C%EF%BC%89/"/>
    <url>/2025/03/08/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%EF%BC%88%E4%BA%8C%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<p>写在前面: Prediction任务是用来在给定策略$\pi$的前提下，基于价值函数和奖励函数来评估该策略的好坏。Control任务用来对策略进行提升和改进。根据是否已知状态转移矩阵(MDP transition)分为Model-Based Prediction和Model-free Prediction</p><span id="more"></span><h3 id="Model-Based-Prediction-Control"><a href="#Model-Based-Prediction-Control" class="headerlink" title="Model-Based Prediction&amp;Control"></a>Model-Based Prediction&amp;Control</h3><h4 id="1-Policy-Iteration"><a href="#1-Policy-Iteration" class="headerlink" title="1. Policy Iteration"></a>1. Policy Iteration</h4><p>策略迭代（Policy Iteration）是一种用于求解马尔可夫决策过程（MDP）的动态规划算法。它主要用于寻找最优策略，即在给定MDP的情况下，使得累计奖励最大的策略。<br>step: （1）<strong>策略评估（Policy Evaluation）</strong> Prediction Phase<br>在当前策略$\pi$下，计算所有状态s的状态函数$V^{\pi}(s)$, $V^{\pi}(s)&#x3D;\sum_{a}\pi(a|s)[R_s^a +\gamma \sum_{s’}P_{ss’}^a V^\pi(s’)]$<br>(2) <strong>策略改进（Policy Improvement）</strong> Control Phase<br>利用贪心策略来改进当前策略：$\pi’&#x3D;greedy(\pi)$<br>具体来说，对于每个状态s，找到使得价值函数最大的动作a，从而得到最优策略$\pi*$<br>$\pi*(s)&#x3D;argmax_a\sum_{s’}P_{ss’}^a[R_s^a + \gamma V^\pi(s’)]$</p><h4 id="2-Value-Iteration"><a href="#2-Value-Iteration" class="headerlink" title="2. Value Iteration"></a>2. Value Iteration</h4><p>价值迭代（Value Iteration）在更新状态时并不涉及策略$\pi$，它将根据最大的动作奖励与转移后的状态价值之和所对应的那个动作作为新的策略。即$V_{k+1}(s)&#x3D;max_{a \in A}(R_s^a + \gamma \sum_{s’ \in S}P_{ss’}^a V_k(s’))$<br>下面给出Policy Iteration和Value Iteration的对比：</p><table><thead><tr><th><strong>特点</strong></th><th><strong>Value Iteration (值迭代)</strong></th><th><strong>Policy Iteration (策略迭代)</strong></th></tr></thead><tbody><tr><td><strong>思路</strong></td><td>直接迭代更新值函数直到收敛，再提取最优策略</td><td>交替执行策略评估和策略改进，不断迭代优化策略</td></tr><tr><td><strong>主要步骤</strong></td><td>1. 更新值函数 → 2. 提取策略</td><td>1. 策略评估 → 2. 策略改进 → 3. 重复直到策略收敛</td></tr><tr><td><strong>收敛条件</strong></td><td>值函数变化足够小 (</td><td>V_{k+1} - V_k</td></tr><tr><td><strong>策略提取时机</strong></td><td>最后一步提取最优策略</td><td>每轮策略改进都会更新策略</td></tr><tr><td><strong>计算量</strong></td><td>更新每个状态时都考虑所有动作（计算量大但收敛快）</td><td>每次完整评估策略再改进策略，可能需要多次评估，但改进次数较少</td></tr><tr><td><strong>策略更新</strong></td><td>从值函数提取策略（离线策略）</td><td>每次改进立即生成新策略（在线策略）</td></tr></tbody></table><h3 id="Model-Free-Prediction"><a href="#Model-Free-Prediction" class="headerlink" title="Model-Free Prediction"></a>Model-Free Prediction</h3><h4 id="1-Monte-Carlo-Reinforcement-Learning"><a href="#1-Monte-Carlo-Reinforcement-Learning" class="headerlink" title="1. Monte-Carlo Reinforcement Learning"></a>1. Monte-Carlo Reinforcement Learning</h4><p>特点：no bootstrapping(bootstrapping是指用现有的估计值来更新当前的估计值)，即不依赖未来状态的估计值，每次只用真实的经验回报来更新策略。<br>bootstrapping（Q-learning、SARSA）: 更新值依赖于未来状态的估计值<br>$Q(s,a) \leftarrow Q(s,a) + \alpha(r + \gamma Q(s’,a’) -Q(s,a))$   这里的Q(s’,a’)就是对未来的估计值<br>no bootstrapping(Monte Carlo): 完全基于真实的完整回报𝐺更新，不依赖未来状态的估计<br>$Q(s,a) \leftarrow Q(s,a) + \alpha(G-Q(s,a))$ 其中$G&#x3D;\sum_{t’&#x3D;t}^T \gamma^{t’-t}r_{t’}$</p><table><thead><tr><th><strong>特性</strong></th><th><strong>No Bootstrapping</strong></th><th><strong>Bootstrapping</strong></th></tr></thead><tbody><tr><td><strong>代表算法</strong></td><td>Monte Carlo, REINFORCE</td><td>Q-learning, SARSA, TD(0)</td></tr><tr><td><strong>更新依赖</strong></td><td>完整回报</td><td>未来状态的估计值</td></tr><tr><td><strong>误差来源</strong></td><td>高方差</td><td>有偏差（因依赖估计值）</td></tr><tr><td><strong>收敛速度</strong></td><td>慢（方差大、数据需求多）</td><td>快（但可能收敛到次优解）</td></tr><tr><td><strong>实时性</strong></td><td>必须等完整轨迹结束才更新</td><td>可以在线更新，每一步都能调整</td></tr></tbody></table><p>step: 为了估计状态s，每一次采样路径经过状态s时，$N(s) \leftarrow N(s)+1$,同时增加累积回报 $S(s) \leftarrow S(s) + G_t$。最终，采用平均估计的方式得到状态价值V(s)&#x3D;S(s)&#x2F;N(s)。接下来更新策略$V(s) \leftarrow V_{\pi}(s)$，直到$N(s) \leftarrow \infty$</p><p>根据$\mu_k &#x3D; \frac{1}{k}\sum_{j&#x3D;1}^k x_j&#x3D;\mu_{k-1}+\frac{1}{k}(x_k-\mu_{k-1})$<br>$V(S_t) \leftarrow V(S_t) + \frac{1}{N(S_t)}(G_t -V(S_t))$<br>通常情况下，我们将这里的$\frac{1}{N(S_t)}$替换为折损因子\alpha，即$V(S_t) \leftarrow V(S_t) + \alpha(G_t -V(S_t))$</p><h4 id="2-Temporal-Difference-Learning"><a href="#2-Temporal-Difference-Learning" class="headerlink" title="2. Temporal Difference Learning"></a>2. Temporal Difference Learning</h4><p>利用Bellman方程的方式来泛化估计$G_t$，即$V(S_t) \leftarrow V(S_t) + \alpha(R_{t+1} + \gamma V(S_{t+1})-V(S_t))$<br>其中$R_{t+1} + \gamma V(S_{t+1})$为TD Target, $\delta_t&#x3D;R_{t+1} + \gamma V(S_{t+1}) - V(S_t)$为TD Error</p><table><thead><tr><th><strong>特性</strong></th><th><strong>TD Learning</strong></th><th><strong>Monte Carlo (MC)</strong></th></tr></thead><tbody><tr><td><strong>更新时机</strong></td><td>每一步更新（在线学习）</td><td>完整轨迹结束后再更新</td></tr><tr><td><strong>回报估计</strong></td><td>依赖当前经验 + 未来估计值（bootstrapping）</td><td>依赖完整回报（no bootstrapping）</td></tr><tr><td><strong>偏差 vs. 方差</strong></td><td>有偏但方差小（因为估计依赖未来状态的值）</td><td>无偏但方差大（因为回报波动大）</td></tr><tr><td><strong>收敛速度</strong></td><td>快，能更快调整值函数</td><td>慢，因为要等待完整回报</td></tr><tr><td><strong>适用场景</strong></td><td>适用于连续或在线任务</td><td>适用于回合式任务（如游戏）</td></tr></tbody></table><h4 id="3-N-Step-Prediction-TD-Learning的变体"><a href="#3-N-Step-Prediction-TD-Learning的变体" class="headerlink" title="3. N-Step Prediction(TD Learning的变体)"></a>3. N-Step Prediction(TD Learning的变体)</h4><p>将TD target设定为未来n步的奖励和转移状态期望，相当于TD和MC的折中,迭代n步再更新<br>Consider the following n-step returns for n&#x3D;1,2…,\infty<br>n&#x3D;1 (TD) $G_t^{(1)}&#x3D;R_{t+1}+\gamma V(S_{t+1})$<br>n&#x3D;2 (2-Step) $G_t^{(2)}&#x3D;R_{t+1}+\gamma R_{t+2} + \gamma^2 V(S_{t+2})$<br>… … …<br>n&#x3D;\infty (MC) $G_t^\infty&#x3D;R_{t+1}+\gamma R_{t+2} + … + \gamma^{T-1}R_T$<br>因此，对于n-step temporal-difference learning: $V(S_t) \leftarrow V(S_t) + \alpha(G_t^{(n)}-V(S_t))$</p><h4 id="4-Lamda-Return-N-Step-Prediction的变体"><a href="#4-Lamda-Return-N-Step-Prediction的变体" class="headerlink" title="4. Lamda Return(N-Step Prediction的变体)"></a>4. Lamda Return(N-Step Prediction的变体)</h4><p>考虑了短步回报和长步回报的不同收益： $G_t^\lambda&#x3D; (1-\lambda)\sum_{n&#x3D;1}^\infty \lambda^{n-1}G_t^{(n)}$<br>$V(S_t) \leftarrow V(S_t) + \alpha (G_t^\lambda- V(S_t))$<br>λ 越小，更依赖当前的 TD 估计，快速调整但可能抖动大; λ 越大，更依赖长时间的回报，稳定但更新慢</p><h4 id="总结"><a href="#总结" class="headerlink" title="总结"></a>总结</h4><p>Monte Carlo Backup<br><img src="/../article_img/Reinforcement/img2-1.png" alt="Monte Carlo Backup"><br>Temporal-Difference Backup<br><img src="/../article_img/Reinforcement/img2-2.png" alt="Temporal-Difference Backup"><br>Dynamic Programming Backup(Policy&#x2F;Value Iteration)<br><img src="/../article_img/Reinforcement/img2-3.png" alt="Dynamic Programming Backup"></p>]]></content>
    
    
    
    <tags>
      
      <tag>AI</tag>
      
      <tag>Algorithm</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>强化学习系列（一）:基础概念</title>
    <link href="/2025/03/05/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%EF%BC%88%E4%B8%80%EF%BC%89/"/>
    <url>/2025/03/05/%E5%BC%BA%E5%8C%96%E5%AD%A6%E4%B9%A0%E7%B3%BB%E5%88%97%EF%BC%88%E4%B8%80%EF%BC%89/</url>
    
    <content type="html"><![CDATA[<h2 id="三要素："><a href="#三要素：" class="headerlink" title="三要素："></a>三要素：</h2><p><strong>rewards, actions, states</strong><br><img src="/article_img/Reinforcement/img1-1.png" alt="RL framework"></p><h2 id="马尔可夫奖励过程："><a href="#马尔可夫奖励过程：" class="headerlink" title="马尔可夫奖励过程："></a>马尔可夫奖励过程：</h2><p>马尔可夫奖励过程可以表示为 $&lt;S,P,R,\gamma&gt;$，其中：</p><ul><li>$S$ 为状态集合；</li><li>$P$ 为状态转移矩阵，$P_{ss’}&#x3D;P[S_{t+1}&#x3D;s’|S_t&#x3D;s]$；</li><li>$R$ 为奖励函数，$R_s&#x3D;E[R_{t+1}|S_t&#x3D;s]$；</li><li>$\gamma$ 为折扣因子，$\gamma \in [0,1]$。</li></ul><h3 id="总折扣奖励："><a href="#总折扣奖励：" class="headerlink" title="总折扣奖励："></a>总折扣奖励：</h3><p>回报（Return）$G_t$ 表示从时刻 $t$ 开始的总折扣奖励：<br>$$<br>G_t&#x3D;R_{t+1}+\gamma R_{t+2}+…&#x3D;\sum_{k&#x3D;0}^\infty \gamma^k R_{t+k+1}<br>$$</p><ul><li><strong>状态价值函数</strong>：$v(s)&#x3D;E[G_t|S_t&#x3D;s]$  </li><li><strong>行为价值函数</strong>：$q(s,a)&#x3D;E[G_t|S_t&#x3D;s,A_t&#x3D;a]$</li></ul><h2 id="Bellman-方程："><a href="#Bellman-方程：" class="headerlink" title="Bellman 方程："></a>Bellman 方程：</h2><p>Bellman 方程表示状态价值函数的递归形式：<br>$$<br>v(s)&#x3D;E[G_t|S_t&#x3D;s]&#x3D;E[R_{t+1}+\gamma R_{t+2}+…|S_t&#x3D;s]&#x3D;E[R_{t+1}+\gamma G_{t+1}|S_t&#x3D;s]&#x3D;E[R_{t+1}+\gamma v(S_{t+1})|S_t&#x3D;s]<br>$$</p><p>根据下方的<strong>状态价值迭代示意图</strong>，我们可以得到：<br>$$<br>v(s)&#x3D;R_s + \gamma \sum_{s’ \in S}P_{ss’}v(s’)<br>$$<br><img src="/article_img/Reinforcement/img1-2.png" alt="状态价值迭代示意图"></p><p>根据<strong>行为价值迭代示意图</strong>，可以得到：<br>$$<br>q(s,a)&#x3D;R_s^a + \gamma \sum_{s’ \in S}P_{ss’}^a v(s’)<br>$$<br><img src="/article_img/Reinforcement/img1-3.png" alt="行为价值迭代示意图"></p><h2 id="马尔可夫决策过程："><a href="#马尔可夫决策过程：" class="headerlink" title="马尔可夫决策过程："></a>马尔可夫决策过程：</h2><p>马尔可夫决策过程（MDP）表示为 $&lt;S,A,P,R,\gamma&gt;$，其中：</p><ul><li>$S$ 是状态集合；</li><li>$A$ 为动作集合；</li><li>$P$ 为状态转移矩阵，$P_{ss’}^a&#x3D;P[S_{t+1}&#x3D;s’|S_t&#x3D;s, A_t&#x3D;a]$；</li><li>$R$ 为奖励函数，$R_s^a&#x3D;E[R_{t+1}|S_t&#x3D;s, A_t&#x3D;a]$。</li></ul><p>状态的转移基于决策策略（policy）$\pi$ 所产生的动作，$\pi$ 用来基于当前状态给出下一步行动的规划：<br>$$<br>\pi(a|s)&#x3D;P[A_t&#x3D;a|S_t&#x3D;s]<br>$$</p><h3 id="价值函数："><a href="#价值函数：" class="headerlink" title="价值函数："></a>价值函数：</h3><p>由此，我们可以定义策略 $\pi$ 下的状态价值函数和行为价值函数：<br>$$<br>v_{\pi}(s)&#x3D;E_{\pi}[G_t|S_t&#x3D;s]<br>$$<br>$$<br>q_{\pi}(s,a)&#x3D;E_{\pi}[G_t|S_t&#x3D;s, A_t&#x3D;a]<br>$$</p><p>使用即时奖励的形式，可以转换为：<br>$$<br>v_{\pi}(s)&#x3D;E_{\pi}[R_{t+1}+\gamma v_{\pi}(S_{t+1})|S_t&#x3D;s]<br>$$<br>$$<br>q_{\pi}(s,a)&#x3D;E_{\pi}[R_{t+1}+\gamma q_{\pi}(S_{t+1},A_{t+1})|S_t&#x3D;s, A_t&#x3D;a]<br>$$</p><h3 id="最优价值函数："><a href="#最优价值函数：" class="headerlink" title="最优价值函数："></a>最优价值函数：</h3><p>$$<br>v_*(s)&#x3D;\max_{\pi}v_{\pi}(s)<br>$$<br>$$<br>q_*(s,a)&#x3D;\max_{\pi}q_{\pi}(s,a)<br>$$</p><p><strong>定理</strong>：如果对于任意的状态 $s$，都有 $v_{\pi}(s) \geq v_{\pi’}(s)$，则策略 $\pi$ 优于策略 $\pi’$。</p><h2 id="Q-A："><a href="#Q-A：" class="headerlink" title="Q&amp;A："></a>Q&amp;A：</h2><p><strong>Ques：</strong> 奖励是由状态变化产生的还是由行动产生的？<br><strong>Ans：</strong> 奖励（Reward）通常是由 <strong>行动（Action）</strong> 产生的，而不是由状态变化直接产生的。在强化学习（RL）中，奖励的定义是智能体（Agent）在环境（Environment）中执行某个 <strong>动作</strong> 后得到的反馈，它表示智能体在采取该动作后获得的即时回报。状态变化是行为导致的结果。</p>]]></content>
    
    
    
    <tags>
      
      <tag>AI</tag>
      
      <tag>Algorithm</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Solidity智能合约</title>
    <link href="/2025/03/01/FundMe/"/>
    <url>/2025/03/01/FundMe/</url>
    
    <content type="html"><![CDATA[<h3 id="安装Remix-ide"><a href="#安装Remix-ide" class="headerlink" title="安装Remix-ide"></a>安装Remix-ide</h3><p>先排个雷，目前官方提供的remix-ide desktop版能安装，但是跑不起来，我在github release列表下了latest version,启动后就是漫长的白屏，网上给的解释是需要修改代理ping通github，但是我这边是能ping通的，多次尝试无果后我选择使用Remix-Ethereum IDE + 本地 remixd。remixd是Remix IDE提供的一个辅助工具，主要用于 Solidity 智能合约开发时，让本地文件系统和 Remix Web IDE 之间建立连接。<br>安装方式：npm install -g @remix-project&#x2F;remixd<br>启动命令：remixd -s .&#x2F; –remix-ide <a href="https://remix.ethereum.org/">https://remix.ethereum.org</a></p><h3 id="vscode-solc"><a href="#vscode-solc" class="headerlink" title="vscode + solc"></a>vscode + solc</h3><p>需要在vscode中安装solidity插件，另外也可安装remix-light插件用于轻量化编译、部署。使用solidity插件对sol编译的方式是右键选择Solidity：Compile Contract。如果编译器版本不匹配的话，solidity插件支持三种不同的更换版本方式：<br>（1）远程下载: 使用远程版本进行编译，需要更改solidity setting中的solidity.compileUsingRemoteVersion，设置为相应的版本号<br>（2）使用本地文件：更改solidity setting中的solidity.compileUsingLocalVersion，设置为solc文件本地路径<br>（3）Npm&#x2F;节点安装：可以在solidity项目文件夹中本地安装solc（npm install solc）<br>优先顺序：首先使用npm&#x2F;节点安装，然后使用本地文件，最后使用远程</p><p>Ques: 简述Solidity的编译和部署都干了什么事儿<br>Ans：Solidity的编译是将人类可读的智能合约代码转化成EVM（以太坊虚拟机）能理解的字节码；Solidity部署是将合约代码正式发布到区块链上，从而拥有独立的地址（合约地址），用来管理资产，支持收款与支付</p><h3 id="区块链交易明细"><a href="#区块链交易明细" class="headerlink" title="区块链交易明细"></a>区块链交易明细</h3><ul><li>交易哈希（Transaction Hash）：这个是在这个区块链上这笔交易的唯一ID，这个交易哈希标识了发送了0.25个以太币到我们的地址的操作。</li><li>状态（Status）：我们可以看到交易状态是成功的，它没有因某种情况而失败。</li><li>区块（Block）：我们可以看到这个交易所在的区块高度。</li><li>时间戳（Timestamp）：这里是时间戳，代表这个交易是何时发生的。</li><li>发送者（From）：我们可以看到这个交易是由谁发送的，当然我们可以在新的标签页打开它，你就能看到发送交易的账户信息。</li><li>接受者（To）：接受了交易的用户，这里就是我们自己。</li><li>价值（Value）：交易的资产价值是0.25个以太币。</li><li>交易手续费（Transaction Fee）：交易的手续费，付给矿工处理这笔交易的费用。</li><li>Gas价格（Gas Price）：Gas的价格，Gas价格是交易中每个执行单元的花费（用ether和gwei做单位），Gas价格越高，被写到区块中的机会越大。</li></ul><h3 id="区分ETH代币和Link代币"><a href="#区分ETH代币和Link代币" class="headerlink" title="区分ETH代币和Link代币"></a>区分ETH代币和Link代币</h3><p>Link代币的功能：</p><ul><li>支付数据服务费：用 LINK 代币支付获取链下数据的费用，比如价格、天气等</li><li>节点质押担保：节点需要质押 LINK 作为担保，作恶会被罚掉</li><li>节点奖励激励：节点提供高质量数据，获得 LINK 奖励</li><li>跨链数据传输费：用于支付跨链数据通信的成本，比如在不同链之间同步信息</li><li>DeFi 抵押品：在一些 DeFi 协议中可以作为抵押品，借贷稳定币等<br>下面是Eth的代币和Link代币的对比总结：<table><thead><tr><th><strong>特点</strong></th><th><strong>ETH（以太坊）</strong></th><th><strong>LINK（Chainlink）</strong></th></tr></thead><tbody><tr><td><strong>项目类型</strong></td><td>智能合约平台</td><td>去中心化预言机网络</td></tr><tr><td><strong>代币功能</strong></td><td>支付交易费、质押、DApp</td><td>支付节点费用、质押、奖励</td></tr><tr><td><strong>技术层级</strong></td><td>Layer 1 主链</td><td>ERC-20 代币（基于以太坊）</td></tr><tr><td><strong>市场定位</strong></td><td>“区块链燃料”、平台代币</td><td>“数据桥梁”、预言机领域龙头</td></tr><tr><td><strong>代表场景</strong></td><td>DeFi、NFT、DAO、Staking</td><td>数据喂价、跨链数据传输</td></tr></tbody></table></li></ul><h3 id="编写FundMe代码在链上筹款"><a href="#编写FundMe代码在链上筹款" class="headerlink" title="编写FundMe代码在链上筹款"></a>编写FundMe代码在链上筹款</h3><figure class="highlight plaintext"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><code class="hljs solidity">// Get funds from users<br>// Withdraw funds<br>// Set a minimum funding value in ETH<br><br>// SPDX-License-Identifier: MIT<br>pragma solidity ^0.8.22;<br><br>contract FundMe &#123;<br><br>    uint256 public minimumETH = 0.01 * 1e18; // 1 * 10 ** 18<br><br>    address[] public funders;<br>    mapping(address =&gt; uint256) public addressToAmountFunded;<br>    <br>    function fund() public payable &#123;<br>        // Want to be able to set a minimum fund amount in currency<br>        // 1. How do we send ETH to this contract?<br>        require(msg.value &gt;= minimumETH, &quot;Didn&#x27;t send enough!&quot;);<br>        funders.push(msg.sender);<br>        addressToAmountFunded[msg.sender] = msg.value;<br>    &#125;<br><br>    function withdraw() public &#123;<br>        /* starting index, ending index, step amount */<br>        for (uint256 funderIndex = 0; funderIndex &lt; funders.length; funderIndex++) &#123;<br>            // code<br>            address funder = funders[funderIndex];<br>            addressToAmountFunded[funder] = 0;<br>        &#125;<br>        // resret the array<br>        funders = new address[](0);<br>        (bool callSuccess, ) =  payable(msg.sender).call&#123;value: address(this).balance&#125;(&quot;&quot;);<br>        require(callSuccess, &quot;Call failed&quot;);<br>    &#125;<br>&#125;<br></code></pre></td></tr></table></figure>]]></content>
    
    
    
    <tags>
      
      <tag>Web3</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>区块链入门指南</title>
    <link href="/2025/03/01/Web3/"/>
    <url>/2025/03/01/Web3/</url>
    
    <content type="html"><![CDATA[<h1 id="区块链与加密货币入门指南"><a href="#区块链与加密货币入门指南" class="headerlink" title="区块链与加密货币入门指南"></a>区块链与加密货币入门指南</h1><h2 id="基础篇：区块链与加密货币基础"><a href="#基础篇：区块链与加密货币基础" class="headerlink" title="基础篇：区块链与加密货币基础"></a>基础篇：区块链与加密货币基础</h2><h3 id="学习目标"><a href="#学习目标" class="headerlink" title="学习目标"></a>学习目标</h3><ul><li>理解区块链基本原理</li><li>掌握加密货币基础知识</li><li>了解DeFi生态系统</li><li>尝试链上交互</li></ul><h3 id="核心学习资源"><a href="#核心学习资源" class="headerlink" title="核心学习资源"></a>核心学习资源</h3><h3 id="1-入门"><a href="#1-入门" class="headerlink" title="1. 入门"></a>1. 入门</h3><ul><li><strong>视频教程</strong>: <a href="https://www.youtube.com/watch?v=bBC-nXj3Ng4">3Blue1Brown的”比特币和区块链是如何工作的”</a> - 通过可视化方式解释区块链的基本原理</li><li><strong>在线课程</strong>: <a href="https://www.coursera.org/learn/blockchain-basics">Coursera - 区块链基础知识</a> &amp; <a href="https://space.bilibili.com/586660955/lists/894352">PKU学生区块链中心WEB3新人公开课</a></li><li><strong>书籍</strong>: 《精通比特币》(Mastering Bitcoin) - Andreas Antonopoulos著</li><li><strong>指南</strong>: <a href="https://yeasy.gitbook.io/blockchain_guide/">区块链技术指南</a> - 完整的中文区块链入门资料</li><li><a href="https://www.youtube.com/watch?v=HNDkNXD0m20">DeFi 工具扫盲</a></li></ul><h3 id="2-白皮书"><a href="#2-白皮书" class="headerlink" title="2. 白皮书"></a>2. 白皮书</h3><ul><li><strong>官方文档</strong>: <a href="https://bitcoin.org/bitcoin.pdf">比特币白皮书</a> - 了解比特币的设计理念</li><li><strong>视频：</strong><a href="https://www.youtube.com/c/WhiteboardCrypto">Whiteboard Crypto</a> - 简化的加密货币教学</li></ul><h3 id="4-实践项目"><a href="#4-实践项目" class="headerlink" title="4. 实践项目"></a>4. 实践项目</h3><ol><li><strong>创建和管理加密钱包</strong><ul><li>OKX 钱包  &#x2F; Metamask &#x2F; …</li><li>学习私钥管理和安全</li><li>在测试网络获取测试代币</li></ul></li><li><strong>体验基本的区块链交互</strong><ul><li>在测试网络上发送交易</li><li>使用区块浏览器如<a href="https://etherscan.io/">Etherscan</a>查看交易</li><li>参与简单的代币交换(使用Uniswap测试网)</li></ul></li></ol><h2 id="进阶篇：智能合约与DeFi"><a href="#进阶篇：智能合约与DeFi" class="headerlink" title="进阶篇：智能合约与DeFi"></a>进阶篇：智能合约与DeFi</h2><h3 id="核心学习资源-1"><a href="#核心学习资源-1" class="headerlink" title="核心学习资源"></a>核心学习资源</h3><h3 id="1-智能合约开发"><a href="#1-智能合约开发" class="headerlink" title="1. 智能合约开发"></a>1. 智能合约开发</h3><h4 id="ETH"><a href="#ETH" class="headerlink" title="ETH"></a>ETH</h4><ul><li><a href="https://docs.soliditylang.org/">Solidity文档</a> - 智能合约的编程语言</li><li><strong>实践指南</strong>: <a href="https://cryptozombies.io/">CryptoZombies</a> - Solidity 互动式智能合约开发课程</li></ul><h4 id="Solana"><a href="#Solana" class="headerlink" title="Solana"></a>Solana</h4><ul><li>Solana 官方文档</li></ul><h3 id="2-DeFi协议解析"><a href="#2-DeFi协议解析" class="headerlink" title="2. DeFi协议解析"></a>2. DeFi协议解析</h3><ul><li><strong>Uniswap</strong>: <a href="https://docs.uniswap.org/">Uniswap文档</a> - 学习AMM原理</li><li><strong>Aave</strong>: <a href="https://docs.aave.com/developers/">Aave开发者文档</a> - 了解借贷协议</li><li><a href="https://www.youtube.com/c/Finematics">Finematics</a> - 专注DeFi教学的YouTube频道</li><li><a href="https://www.youtube.com/watch?v=HNDkNXD0m20">3D的频道中的各个协议</a></li></ul><h3 id="3-区块链数据分析"><a href="#3-区块链数据分析" class="headerlink" title="3. 区块链数据分析"></a>3. 区块链数据分析</h3><ul><li><strong>学习资源</strong>: <a href="https://github.com/SixdegreeLab/MasteringChainAnalytics">精通链上数据分析</a> - 从入门到高级的链上数据分析教程</li><li><strong>分析平台</strong>: <a href="https://dune.com/">Dune Analytics</a> - 区块链数据可视化平台</li></ul><h2 id="从零到无穷大"><a href="#从零到无穷大" class="headerlink" title="从零到无穷大"></a>从零到无穷大</h2><h3 id="核心学习资源-2"><a href="#核心学习资源-2" class="headerlink" title="核心学习资源"></a>核心学习资源</h3><h3 id="1-MEV"><a href="#1-MEV" class="headerlink" title="1. MEV"></a>1. MEV</h3><ul><li><a href="https://github.com/33357/smartcontract-apps">智能合约应用指南</a> 包含了常见的套利思路</li><li><a href="https://docs.flashbots.net/">Flashbots</a> - ETH 民主化 MEV</li></ul><h3 id="2-开源实现参考"><a href="#2-开源实现参考" class="headerlink" title="2. 开源实现参考"></a>2. 开源实现参考</h3><ul><li><a href="https://github.com/duckdegen/apebot">ApeBot</a> 已失效的 new listing snipper</li><li><a href="https://hummingbot.io/">Hummingbot</a></li></ul><h3 id="3-实践项目"><a href="#3-实践项目" class="headerlink" title="3. 实践项目"></a>3. 实践项目</h3><ol><li><strong>信息搜集</strong><ul><li>新闻爬取，新闻交易</li><li>社区情绪分析</li><li>市场热点收集与分析</li></ul></li><li><strong>链上数据</strong><ul><li>使用Dune Analytics创建DeFi数据看板</li><li>跟踪协议TVL、交易量等关键指标</li><li>实现简单的链上行为分析。老鼠仓（内幕交易），巨鲸检测。bot盈利、策略分析</li><li>套利机会检测、风险监控</li></ul></li><li><strong>链上套利</strong><ul><li><a href="https://github.com/33357/smartcontract-apps">智能合约应用指南</a> 涵盖了常见的套利思路，比较简略</li><li><a href="https://github.com/antaintan/uniswap-arbitrage-analysis">Uniswap套利分析</a> 经典实战案例分析</li></ul></li><li><strong>DeFi协议分析</strong><ul><li>分析Uniswap的智能合约代码</li><li>理解借贷协议的风险模型</li><li>研究流动性挖矿机制</li></ul></li><li><strong>MEV研究</strong><ul><li>检测和监控 MEV 行为</li></ul></li><li><strong>工具与资源</strong>:<ul><li><a href="https://www.tradingview.com/">TradingView</a></li><li>dune</li><li><a href="https://defillama.com/">DeFi Llama</a> - DeFi协议跟踪与分析</li><li><a href="https://messari.io/">Messari</a> - 同上</li><li>geckoterminal - DEX 数据查看器</li><li>solscan, etherscan …</li></ul></li></ol><h2 id="持续学习资源"><a href="#持续学习资源" class="headerlink" title="持续学习资源"></a>持续学习资源</h2><h3 id="新闻"><a href="#新闻" class="headerlink" title="新闻"></a>新闻</h3><ul><li><a href="https://www.theblock.co/">The Block</a> - 区块链新闻</li><li>推特<ul><li>AggrNews</li><li>BWENews (方程式新闻)</li><li>…</li></ul></li></ul>]]></content>
    
    
    
    <tags>
      
      <tag>Web3</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>命名实体识别&amp;关系抽取 小结</title>
    <link href="/2025/02/21/%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB&amp;%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96%20%E5%B0%8F%E7%BB%93/"/>
    <url>/2025/02/21/%E5%91%BD%E5%90%8D%E5%AE%9E%E4%BD%93%E8%AF%86%E5%88%AB&amp;%E5%85%B3%E7%B3%BB%E6%8A%BD%E5%8F%96%20%E5%B0%8F%E7%BB%93/</url>
    
    <content type="html"><![CDATA[<h4 id="BIO标记法"><a href="#BIO标记法" class="headerlink" title="BIO标记法"></a>BIO标记法</h4><p><strong>B-（Begin）</strong>：表示一个实体的<strong>开始</strong>（首个 token）。</p><p><strong>I-（Inside）</strong>：表示实体的<strong>内部</strong>（除首个 token 以外的部分）。</p><p><strong>O-（Outside）</strong>：表示<strong>非实体</strong>的 token。</p><p>例如：**”Apple Inc. is based in California.”</p><p>BIO 标注：</p><table><thead><tr><th>Token</th><th>BIO 标签</th></tr></thead><tbody><tr><td>Apple</td><td><strong>B-ORG</strong></td></tr><tr><td>Inc.</td><td><strong>I-ORG</strong></td></tr><tr><td>is</td><td><strong>O</strong></td></tr><tr><td>based</td><td><strong>O</strong></td></tr><tr><td>in</td><td><strong>O</strong></td></tr><tr><td>California</td><td><strong>B-LOC</strong></td></tr><tr><td>.</td><td><strong>O</strong></td></tr></tbody></table><p><strong>常见的实体类别</strong></p><table><thead><tr><th>实体类别</th><th>含义</th></tr></thead><tbody><tr><td><strong>PER</strong> (Person)</td><td>人名（例如：Elon Musk, Bill Gates）</td></tr><tr><td><strong>ORG</strong> (Organization)</td><td>组织名（例如：Apple, Google, NASA）</td></tr><tr><td><strong>LOC</strong> (Location)</td><td>地名（例如：California, Beijing, Paris）</td></tr><tr><td><strong>MISC</strong> (Miscellaneous)</td><td>其他（例如：品牌名、事件名等）</td></tr></tbody></table><p>在 <strong>NER 任务</strong> 中，BIO 标签常作为<strong>序列标注模型</strong>（如 BiLSTM-CRF、Transformer）训练的目标</p><p><strong>训练模型</strong></p><ul><li><strong>输入</strong>：Word Embeddings（如 BERT, GloVe）</li><li><strong>模型</strong>：BiLSTM + CRF &#x2F; Transformer</li><li><strong>输出</strong>：BIO 标注序列</li></ul><p><strong>预测新文本</strong></p><p>输入新文本后，模型预测对应的 BIO 标签，从而<strong>提取实体</strong>。</p><h4 id="RE-Relation-Extraction-训练集"><a href="#RE-Relation-Extraction-训练集" class="headerlink" title="RE(Relation Extraction)训练集"></a>RE(Relation Extraction)训练集</h4><p><strong>spo:</strong> subject-predicate-object 头实体-关系-尾实体</p><p><strong>训练集中的Predicate列表</strong></p><p>​{<br>      “O”: 0,<br>      “I”: 1,<br>      “注册资本”: 2,<br>      “作者”: 3,<br>      “所属专辑”: 4,<br>      “歌手”: 5,<br>      …<br>      “上映时间_@value”: 8,<br>      “上映时间_@area”: 9,<br>      …<br>​}</p><p>对于同一个S-P，句子中是可能存在多个不同的合法O的，那我们就需要使用两个或多个不同的S-P来对应这些不同的O。一种最常见的方法就是对P再进行细分。如上面示例中的“上映时间_@value”和”上映时间_@area“</p><p><strong>训练集中的spo列表</strong></p><p>​{<br>      “predicate”: [“empty”, “empty”, “注册资本”, “作者”, “所属专辑”, …],<br>      “subject_type”: [“empty”, “empty”, “企业”, “图书作品”, “歌曲”, …],<br>      “object_type”: [“empty”, “empty”, “Number”, “人物”, “音乐专辑”, …]<br>​}</p><p>前两个empty是为了O和I标签留的,因为之前定义的predicate列表中的前2个标签分别为O、I，这两个标签不会起到连接首尾实体的作用，因此需要置为empty。</p><h4 id="NER模型的训练原理"><a href="#NER模型的训练原理" class="headerlink" title="NER模型的训练原理"></a>NER模型的训练原理</h4><p>将实体抽取问题转化为Token Classfication问题。</p><p>Ques：如何实现关系抽取？ </p><p>Ans：在字符类别中添加入「关系标签」，即该字符是否能和这句话当中的其他字符产生关联关系。</p><p><img src="/article_img/NER/NER.png" alt="NER model"></p><p>对每个token，label一共有（2N + 2）维，其中 N 为 Predicate 的类别个数</p><p>损失函数一般使用BCE Loss</p>]]></content>
    
    
    
    <tags>
      
      <tag>NER</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>GraphRAG综述</title>
    <link href="/2025/02/20/GraphRAG%E7%BB%BC%E8%BF%B0/"/>
    <url>/2025/02/20/GraphRAG%E7%BB%BC%E8%BF%B0/</url>
    
    <content type="html"><![CDATA[<h4 id="Sec-1-传统RAG系统面临的挑战"><a href="#Sec-1-传统RAG系统面临的挑战" class="headerlink" title="Sec.1 传统RAG系统面临的挑战"></a>Sec.1 传统RAG系统面临的挑战</h4><p><strong>复杂的查询理解：</strong>传统的 RAG 方法依赖于简单的关键字匹配和向量相似性技术，不足以捕捉准确和全面所需的深层语义细微差别和多步骤推理过程。例如，当询问概念 A 和概念 D 之间的联系时，这些系统通常只检索直接相关的信息，而错过了像 B 和 C 这样可以弥合关系的关键中间概念。这种狭窄的检索范围限制了 RAG 的能力，使其无法进行广泛的上下文理解和复杂的推理</p><p><strong>集成来自分布式源的领域知识：</strong>检索到的知识通常是扁平的、广泛的和错综复杂的，而领域概念通常分散在多个文档中，不同概念之间没有明确的层次结构关系。尽管 RAG 系统试图通过将文档划分为较小的块以实现有效和高效的索引来管理这种复杂性，但这种方法无意中牺牲了关键的上下文信息，严重损害了检索准确性和上下文理解。这种限制阻碍了在相关知识点之间建立强大联系的能力，导致理解碎片化，并降低利用特定领域专业知识的效率。</p><p><strong>LLM的固有约束：</strong>受到其固定上下文窗口的限制，LLM无法完全捕获复杂文档中的长距离依赖关系。在专业领域中，在广泛的知识背景下保持连贯性的挑战变得越来越棘手，因为关键信息可能会在上下文窗口截断期间丢失。</p><p><strong>系统效率和可拓展性：</strong>RAG 系统在计算上可能既昂贵又耗时 ，尤其是在处理大规模知识源时，因为模型需要搜索大量非结构化文本以查找相关信息。此外，实时检索和跨文档推理可能会带来相当大的延迟，从而对用户体验产生负面影响。此外，实时检索和跨文档推理可能会带来相当大的延迟，从而对用户体验产生负面影响，从而限制了它在广泛和动态的专业环境中的实际部署</p><h4 id="Sec-2-现有的GraphRAG分类"><a href="#Sec-2-现有的GraphRAG分类" class="headerlink" title="Sec.2 现有的GraphRAG分类"></a>Sec.2 现有的GraphRAG分类</h4><p><strong>基于知识的GraphRAG:</strong> 它使用图作为知识载体，专注于将非结构化文本文档转换为显式和结构化的 KG，其中node代表领域概念，edge捕获它们之间的语义关系，从而更好地表示分层关系和复杂的知识依赖关系</p><p><strong>基于索引的GraphRAG:</strong> 使用图作为索引工具从语料库中检索相关的原始文本，它保留了原始文本形式，同时主要将图形结构用作索引机制来有效地组织和检索相关文本块。通过将图形结构合并到文本索引中，基于索引的 GraphRAG 方法在文本块之间建立语义连接，以实现高效的查找作和检索</p><p><strong>基于知识的 GraphRAG 旨在通过基于图的推理能力创建结构化的知识表示，以便更好地理解复杂的关系；而基于索引的 GraphRAG 则侧重于通过基于图的索引策略优化相关文本信息的检索和可访问性</strong></p><h4 id="Sec-3-Overview-the-Framework-of-RAG"><a href="#Sec-3-Overview-the-Framework-of-RAG" class="headerlink" title="Sec.3 Overview the Framework of RAG"></a>Sec.3 Overview the Framework of RAG</h4><p>RAG框架主要由三个部分组成，Knowledge Organization,Knowledge Retrieval,Knowledge Integration。下图展示了传统RAG、基于知识的GraphRAG和基于索引的GraphRAG在这三部分的各自实现方式。</p><p><img src="/../article_img/20250220/img1.png" alt="传统 RAG 和两个典型 GraphRAG 工作流程的全面概述。传统 RAG 将语料库组织成块，按相似性对它们进行排名，并检索最相关的文本以生成响应。基于知识的 GraphRAG 使用实体识别和关系提取从语料库中提取详细的知识图谱，提供精细的、特定于领域的信息。基于索引的 GraphRAG 将语料库总结为高级主题节点，这些节点链接以形成索引图，而事实链接将主题映射到文本。这种两层结构将高效的主题检索与详细的文本知识相结合，与基于知识的 GraphRAG 相比，提供了可扩展性和性能"></p><p><strong>Knowledge Organization:  该部分关注外部知识库的构建</strong></p><p><strong>传统RAG</strong>：传统RAG主要采用将大规模文本语料库拆分为可管理块的策略，然后使用嵌入模型将这些块转换为嵌入，其中嵌入用作向量数据库中原始文本块的键。此设置通过在语义空间中基于距离的搜索实现高效的查找作和检索相关内容。常见的优化方法：<strong>粒度优化</strong>和<strong>索引优化</strong>。</p><p><strong>GraphRAG:</strong> 基于图的方法构建外部信息，通过显式知识表示（作为知识载体的图）或索引机制（用于知识索引的图）。这些方法可实现高效的上下文感知信息检索。</p><p><strong>Knowledge Retrieval: 该部分关注如何更精准高效的进行检索</strong></p><p><strong>传统RAG</strong>: 常涉及的检索方法：KNN、TF-IDF、BM25。为了提高检索的准确性和效率，一方面可以在检索前通过优化表示或重排序等技术来提高检索模型的准确性，例如<a href="https://arxiv.org/pdf/2009.08553">GAR</a>、<a href="https://arxiv.org/pdf/2305.17080">EAR</a>等；另一方面，对检索模型进行训练，例如<a href="https://arxiv.org/pdf/2301.12652">Replug</a>，<a href="https://www.jmlr.org/papers/volume24/23-0037/23-0037.pdf">Atlas</a>，<a href="https://arxiv.org/pdf/2305.06983">Flare</a>等。</p><p><strong>GraphRAG:</strong> GraphRAG 模型使用基于图的规划器（可学习的规划器或基于图算法的规划器）根据输入查询检索相关信息。这些检索技术不仅考虑了查询和每个文本块之间的语义相似性，还考虑了查询类型和检索到的子图之间的逻辑连贯性</p><p><strong>Knowledge Integration:  该部分关注如何将检索结果和用户查询高质量整合</strong></p><p><strong>传统RAG</strong>:提高检索到的内容的质量：强化学习方法<a href="https://www.sciencedirect.com/science/article/pii/S294971912400013X">LeanContext</a> LLM自评估方法<a href="https://arxiv.org/pdf/2310.11511">Self-RAG</a>  <a href="https://arxiv.org/pdf/2307.03027">评估检索内容重要性</a>。此外，考虑到对大量检索到的段落进行编码是资源密集型的，这会导致大量的计算和内存开销，相关优化方法：<a href="https://arxiv.org/pdf/2404.12457">RAGCache</a> <a href="https://arxiv.org/pdf/2310.15556">TCRA-LLM</a></p><p><strong>GraphRAG:</strong> 一旦检索到相关知识，GraphRAG 模型就会将其与用户查询整合起来作为LLM input。集成过程的目标是将检索到的知识无缝合并到生成的文本中，从而提高其质量和信息量。一个关键的设计考虑因素是，如何在最终的基于文本的提示中保留检索到的子图信息的丰富性，而不会引入冗余或错误地强调文本描述中不太关键的方面</p><h4 id="Sec-4-Knowledge-Organization-in-GraphRAG"><a href="#Sec-4-Knowledge-Organization-in-GraphRAG" class="headerlink" title="Sec.4 Knowledge Organization in GraphRAG"></a>Sec.4 Knowledge Organization in GraphRAG</h4><p>首先构建一个图结构来组织知识，然后检索和集成与查询相关的信息。下面对Sec.2 提到的范式展开具体介绍。</p><p><strong>Graphs for Knowledge Indexing</strong></p><p>​基于索引的 GraphRAG 方法利用图结构来索引和检索相关的原始文本块，然后将其馈送到 LLM 中以进行知识注入和上下文理解。这些索引图应用语义相似性或特定于域的关系等原则来有效地桥接单独文本段落之间的连接。与仅将图用作知识载体相比，这种技术通过直接总结与查询相关的原始文本块中的信息来提供更丰富的答案。</p><p>​面临的挑战：（1） <strong>简洁性和相关性</strong>：确保构建的图仅捕获相关关系，而不会因不必要的连接而过载，这是一项重大挑战，从而有助于有效调用相关文本块而不会产生冗余 （2）<strong>一致性和冲突解决：</strong>不同的数据块可能会引入冲突的信息。解决这些冲突并确保图保持一致、可靠和结构良好至关重要。</p><p><strong>Graphs for Knowledge Carriers</strong></p><p>​优势：1）高效检索与查询相关的知识 2）长跨度的连贯多步推理  </p><p>​局限性：（1） <strong>缺乏高质量的 KG：</strong>对于直接使用KG作为外部知识库，这一研究方向受到高质量KG可用性的限制。构建KG是资源密集型的，但大多数公开可用的KG仍然远非全面（2） <strong>效率和有效性之间的权衡：</strong>当从文本语料库构建 KG 时，提取知识的粒度在平衡效率和有效性方面起着至关重要的作用。保留细粒度信息会导致更大、更详细的 KG，这可能会阻碍计算效率。相反，紧凑的 KG 可能会牺牲重要的细节，从而导致潜在的信息丢失。</p><h4 id="Sec-5-Knowledge-Retrieval-Process"><a href="#Sec-5-Knowledge-Retrieval-Process" class="headerlink" title="Sec.5 Knowledge Retrieval Process"></a>Sec.5 Knowledge Retrieval Process</h4><p>​基于图的知识检索一般分为Preprocess&#x2F;Matching&#x2F;Pruning三个步骤，如下图所示：</p><p><img src="/../article_img/20250220/img2.png" alt="Retrieval Process"></p><p><strong>Query&#x2F;Graph Preprocess</strong> 预处理阶段同时对查询数据库和图形数据库运行，以便为高效检索做好准备。对于查询预处理，系统通过矢量化或关键术语提取将输入问题转换为结构化表示。这些表示形式用作后续检索作的搜索索引。在图方面，图数据库经过更全面的处理，其中预训练的语言模型将图元素（实体、关系和三元组）转换为密集的向量表示，作为检索锚点 。此外，一些高级检索模型在图数据库上应用图神经网络 （GNN） 来提取高级结构特征，而一些方法甚至采用规则挖掘算法生成规则库，作为图知识的丰富、可搜索的索引</p><p><strong>Matching</strong> 匹配阶段在预处理的查询和索引图数据库之间建立连接。此过程将查询表示形式与图索引进行比较，以识别相关的知识片段。匹配算法同时考虑图中的<strong>语义相似性和结构关系</strong>。根据匹配分数，系统检索与查询高度相关的连通组件和子图，从而创建一组初始的候选知识。</p><p><strong>Knowledge Pruning</strong> 修剪阶段会优化最初检索的知识，以提高其质量和相关性。此优化过程解决了检索过多或不相关信息的常见挑战，尤其是在处理复杂查询或大型图数据库时。剪枝算法应用一系列细化作来整合和总结检索到的知识。具体来说，系统首先删除明显不相关或嘈杂的信息。然后，它整合了相关的知识片段，并生成了复杂图知识的简明摘要。通过提供精炼和重点突出的摘要，LLM 能够更好地理解信息的上下文和细微差别，从而做出更准确和有意义的回答。</p><h4 id="Sec-6-Knowledge-Retrieval-Techniques"><a href="#Sec-6-Knowledge-Retrieval-Techniques" class="headerlink" title="Sec.6 Knowledge Retrieval Techniques"></a>Sec.6 Knowledge Retrieval Techniques</h4><p><strong>基于语义相似性的检索器</strong> 通过测量离散语言空间或连续向量空间中的查询与知识库之间的相似性来进行适当的答案检索 （1）离散空间建模。离散空间建模方法主要利用语言离散统计知识直接对文本字符串进行建模。例如子字符串匹配、正则表达式和精确短语匹配等算法  （2）嵌入空间建模。利用预训练语言模型和词嵌入等方法，例如 TF-IDF、Word2Vec 和 GloVe</p><p><strong>基于逻辑推理的检索器</strong> 采用符号推理从图知识库中推断和提取相关信息。此方法包括创建逻辑规则和约束，以阐明知识库固有的关系和层次结构，例如 规则挖掘 、归纳逻辑编程 和 约束满足 等技术</p><p><strong>基于 GNN 的 Retriever</strong> 主要利用图神经网络对构建的图库中的节点进行编码。检索主要依赖于同时包含情感意义和结构关系理解的节点表征的编码相似性。基于 GNN 的检索器需要训练 GNN 编码器。此外，由于缺乏明确标注的数据，<strong>训练的重点是设计一个合适的损失函数，使 GNN 能够学习通过表示编码准确定位目标知识</strong></p><p><strong>基于 LLM 的检索器</strong> 关于构建的图库，基于 LLM 的知识检索器主要侧重于利用 LLM 来理解图并识别关键子图。</p><p><strong>基于强化学习的检索器</strong> 强化学习 为 GraphRAG 系统中的检索提供了一种自适应和动态策略。通过将检索过程构建为顺序决策挑战，基于 RL 的方法使代理能够在环境反馈的指导下学习和遍历图库，以寻找最相关的信息。</p><p>这种方法赋予系统通过主动交互和积累经验不断提高其检索性能的能力。这个过程可以描述如下：相关的推理背景在于一个特定于问题的子图 $G_{sub}$，其中包含所有源实体 $Q_s$、目标实体 $Q_t$ 及其邻居。理想的子图 $G_{sub}$ 应具有以下属性：（i） $G_{sub}$ 包含尽可能多的源实体和目标实体;（ii） $G_{sub}$ 中的实体和关系与问题上下文具有很强的相关性;（iii） $G_{sub}$ 简洁明了，几乎没有冗余信息，因此可以输入到长度有限的 LLM 中。相关方法：<a href="https://arxiv.org/pdf/2405.16420">Deep QNetworks</a>, <a href="https://arxiv.org/pdf/2401.06800">Policy Gradients</a>, and <a href="https://arxiv.org/pdf/2410.10584">Actor-Critic</a></p><h4 id="Sec-7-Knowledge-Integration"><a href="#Sec-7-Knowledge-Integration" class="headerlink" title="Sec.7 Knowledge Integration"></a>Sec.7 Knowledge Integration</h4><p><strong>微调技术：</strong>利用各种图形信息的微调过程可以根据输入目标的粒度分为三个不同的类别：（i） 节点级知识：关注图形中的各个节点。（ii） 路径级知识：专注于节点之间的连接和序列。（iii） 子图级知识：考虑由多个节点组成的较大结构及其互连。我们将详细探讨这些方面中的每一个。 </p><p>使用 Node 级知识进行微调。在许多基于图的 RAG 系统中，每个节点都链接到一个文档，例如引文网络中的摘要。由于特定领域的数据很少出现在预训练语料库中，因此一些研究在进行下游任务微调之前采用指令调优来加强对特定领域的知识理解。一种简单的微调方法包括将节点和相邻文本作为上下文信息馈送到 LLM 中，以帮助预测 <a href="https://openreview.net/forum?id=x5FfUvsLIE">1</a>、<a href="https://arxiv.org/pdf/2406.10393">2</a>、<a href="https://arxiv.org/pdf/2402.07483">3</a>。鉴于检索到的文档可能很广泛，研究人员可以利用 LLM 将这些文本提炼成单个嵌入 <a href="https://proceedings.neurips.cc/paper/2020/file/6b493230205f780e1bc26945df7481e5-Paper.pdf">4</a>。尽管缺乏词汇外标记的预训练数据，但 LLM 能够在实践中识别这些嵌入中的信息。</p><p>使用 Path-level Knowledge 进行微调。语言任务通常涉及复杂的推理，需要对事实关系有清晰的理解。利用知识图谱路径，LLM 通过关系和实体的引导增强自身推理能力。这些路径可以是从问题实体到答案实体的最直接路线，也可以使用 图检索模型 或 启发式方法 进行挖掘。它们可以用作输入和输出，但是当两个节点之间存在多条路径时，过滤掉嘈杂的路径同时保留知识图谱中的关系至关重要。为了保持实体表示的完整性及其沿路径的关系，一些方法侧重于将这些路径作为训练目标，预测两个节点之间路径上的节点和关系<a href="https://arxiv.org/pdf/2303.03922">5</a>，甚至跨多个路径<a href="https://arxiv.org/pdf/2310.01061">6</a>。这使 LLM 能够进行Path级推理并产生可靠的输出。</p><p>使用 Subgraph 级知识进行微调。与路径数据的线性拓扑不同，子图数据表现出更复杂、更不规则的拓扑。一种简单的方法是使用图编码器将子图级信息压缩到读出嵌入中。或者，将图数据转换为序列。然而，这些方法往往忽略了子图中丰富的文本内容，无法使 LLM 认识到底层的图结构。为了解决这个问题，一些工作专注于调整 transformer 架构以更好地处理结构化数据，例如<a href="https://arxiv.org/pdf/2212.01588">7</a>、<a href="https://arxiv.org/pdf/2402.11709">8</a>，而另一些则将节点和边的描述直接合并到提示中，例如<a href="https://aclanthology.org/2024.findings-eacl.132.pdf">9</a>、<a href="https://arxiv.org/pdf/2402.08170">10</a>。然而，现有方法仍然存在挑战。前者可能会因架构更改而丢失在预训练期间获得的知识，而后者可能难以处理具有大量节点和边的密集图。</p><h4 id="相关工作总结"><a href="#相关工作总结" class="headerlink" title="相关工作总结"></a>相关工作总结</h4><p>参考文献：<a href="https://arxiv.org/pdf/2501.13958">https://arxiv.org/pdf/2501.13958</a></p><p><img src="/../article_img/20250220/img3.png" alt="参考文献"></p>]]></content>
    
    
    
    <tags>
      
      <tag>LLM</tag>
      
      <tag>RAG</tag>
      
      <tag>Graph</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Path-Level GNN-Based Retrievers</title>
    <link href="/2025/02/20/GraphRAG/"/>
    <url>/2025/02/20/GraphRAG/</url>
    
    <content type="html"><![CDATA[<p>1.<a href="https://arxiv.org/abs/2405.20139">GNN-RAG: Graph Neural Retrieval for Large Language Model Reasoning</a></p><p>开源代码：<a href="https://github.com/cmavro/GNN-RAG">https://github.com/cmavro/GNN-RAG</a></p><p>Motivation: 将GNN的推理能力和LLM的语言理解能力相结合用于知识图谱问答</p><p>Methodology：首先，GNN 对密集的 KG 子图进行推理，以检索给定问题的答案候选者。其次，提取 KG 中连接问题实体和答案候选项的最短路径以表示 KG 推理路径。提取的路径被文本化并作为使用 RAG 进行 LLM 推理的输入。在GNN-RAG 框架中，GNN 充当密集的子图推理器来提取有用的图信息，而 LLM 则利用其自然语言处理能力进行最终的 KGQA（知识图谱问答）</p><p><img src="/../article_img/PathGNN/GNN-RAG.png" alt="GNN-RAG"></p><p><strong>GNN的训练过程：</strong>将知识图谱问答KGQA任务视为节点分类，使用question-answer pairs训练集，将KG entities被分为answers和non-answers，</p><p>$h_v^{(l)}&#x3D;\psi(h_v^{(l-1)}, \sum_{v’\in N_v}\omega(q,r)\cdot m_{vv’}^{(l)})$    Equ.1</p><p>其中函数$\omega(\cdot)$用来计算三元组(v,r,v’)中关系r和用户查询q之间的相关性。经过多跳传播后，所有节点根据最终表示被分为answer和non-answer两类。</p><p>此外，考虑到不同的GNN会产生不同的推理路径，如公式Equ.1所示，GNN的推理依赖于question-relation匹配函数$\omega(q,r)$，比较常用的设计方式为$\phi(q^{(k)}\odot r)$，其中$q^{(k)}$和$r$使用预训练语言模型来进行编码，$q^{(k)}&#x3D;\gamma_k(LM(q)), r&#x3D;\gamma_c(LM(r))$。</p><p><strong>GNN的推理过程：</strong>具有最高概率分数的节点（例如，高于概率阈值）将作为候选答案返回，以及将问题实体与候选答案连接起来的最短路径（推理路径）。检索到的推理路径用作基于 LLM 的 RAG 的输入。</p><p>为了确保推理路径的多样性，这里作者并没有采用不同的GNN架构，而是使用了不同的LM通过更改$\omega$函数来引导单一的GNN获取不同的节点表示</p><p><strong>LLM</strong>：没什么好说的，prompt-tuning </p><p>prompt: “Based on the reasoning paths, please answer the given question.\n Reasoning Paths: {Reasoning Paths} \n Question: {Question}”. </p><p>Reasoning paths 用语言描述为 “{question entity} → {relation} → {entity} → · · · → {relation} → {answer entity} \n”</p><p>2.<a href="https://arxiv.org/abs/2310.01061">Reasoning on Graphs: Faithful and Interpretable Large Language Model Reasoning</a></p><p>开源代码：<a href="https://github.com/RManLuo/reasoning-on-graphs">https://github.com/RManLuo/reasoning-on-graphs</a></p><p>Motivation: 现有的基于 KG 的 LLM 推理方法仅将 KG 视为事实知识库，而忽视了其结构信息对推理的重要性。作者提出了一种称为图推理（TOG）的方法，并设计了一种规划检索推理框架，以实现忠实和可解释的推理</p><p><img src="/../article_img/PathGNN/RoG.png" alt="RoG"></p><p>Methodology：RoG包含两个组件：（1）<strong>规划模块：</strong>基于用户查询生成关系路径作为KG的检索规划（2) <strong>检索推理模块：</strong>根据规划模块的关系路径从KG中检索有效的推理路径。RoG通过两项任务进行优化：（1）<strong>规划优化</strong>，由于LLM对KG中包含的关系一无所知，该模块的训练目标是让LLM生成的关系路径尽可能近似于KG的有效路径。作者使用关系路径Q（z）的后验分布最小化KL发散来实现，该后验分布可以通过KGs中的有效关系路径来近似（2）<strong>检索-推理优化</strong>，给定问题q和作为规划z的关系路径，检索模块旨在从KG图G检索这个推理路径$W_z$。检索过程包括在G中查找路径，其从问题实体$e_q$开始并遵循关系路径z。采用一个约束的广度优先搜索（BFS）来检索来自KGs的推理路径$W_z$。实验中，所有检索的路径都用于推理。</p><p><img src="/../article_img/PathGNN/Algorithm.png" alt="Optimization Algorithm"></p><p>总结：LLM用来生成关系路径，在训练的过程中尽可能的去拟合KG中的真实路径。推理时首先让LLM根据用户查询生成路径规划，然后再KG查询对应的有效路径，作为检索结果，与用户查询一起作为LLM输入用户生成回复。</p>]]></content>
    
    
    
    <tags>
      
      <tag>LLM</tag>
      
      <tag>RAG</tag>
      
      <tag>Graph</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Graph + RAG</title>
    <link href="/2025/02/18/20250218/"/>
    <url>/2025/02/18/20250218/</url>
    
    <content type="html"><![CDATA[<h1 id="Graph-RAG"><a href="#Graph-RAG" class="headerlink" title="Graph + RAG"></a>Graph + RAG</h1><p>图RAG相比于传统RAG的优势：</p><ol><li>多跳推理能力 2. 关系建模能力 3. 高效的知识更新与管理 4. 减少检索的噪声和生成的幻觉</li></ol><p>最近看了几篇图RAG的论文：</p><h3 id="G-Retriever-Retrieval-Augmented-Generation-for-Textual-Graph-Understanding-and-Question-Answering（https-arxiv-org-pdf-2402-07630）"><a href="#G-Retriever-Retrieval-Augmented-Generation-for-Textual-Graph-Understanding-and-Question-Answering（https-arxiv-org-pdf-2402-07630）" class="headerlink" title="G-Retriever: Retrieval-Augmented Generation for Textual Graph Understanding and Question Answering（https://arxiv.org/pdf/2402.07630）"></a>G-Retriever: Retrieval-Augmented Generation for Textual Graph Understanding and Question Answering（<a href="https://arxiv.org/pdf/2402.07630%EF%BC%89">https://arxiv.org/pdf/2402.07630）</a></h3><p>Motivation: 引入文本图进行检索增强</p><p>开源代码：<a href="https://github.com/XiaoxinHe/G-Retriever">https://github.com/XiaoxinHe/G-Retriever</a></p><p>首先根据已有的外部知识构建知识图谱，并对每个节点和关系，利用其固有的文本属性进行特征编码。针对用户的query，进行相似度检索，得到节点和边的topk子集。然后设计了一种强化学习策略基于检索得到的子集构建subgraph。在生成阶段，LLM的参数被冻结，除了用户的query以外，还有检索子图的文本属性所构成的hard prompt和基于可训练graph encoder得到的soft prompt <img src="/article_img/20250218/20250218_01.png" alt="G-Retriever"></p><h3 id="Knowledge-Graph-Retrieval-Augmented-Generation-For-LLM-Based-Recommendation-https-arxiv-org-pdf-2501-02226"><a href="#Knowledge-Graph-Retrieval-Augmented-Generation-For-LLM-Based-Recommendation-https-arxiv-org-pdf-2501-02226" class="headerlink" title="Knowledge Graph Retrieval-Augmented Generation For LLM-Based Recommendation(https://arxiv.org/pdf/2501.02226)"></a>Knowledge Graph Retrieval-Augmented Generation For LLM-Based Recommendation(<a href="https://arxiv.org/pdf/2501.02226">https://arxiv.org/pdf/2501.02226</a>)</h3><p>Motivation: 传统的RAG方法忽视了知识的结构关系，借助外部知识构建KG，对推荐进行检索增强；</p><p>检索阶段，首先训练一个GNN网络用来对item-entity知识图谱的每个节点和关系进行编码，并以每个节点作为中心节点生成的多跳特征表示收集起来，构建向量数据库。对于待预测的用户节点，根据其交互历史中的每个节点，利用其文本特征从KG VecDB中检索相关子图，并对检索得到的所有子图进行重排序用于构建soft prompt，从而增强LLM的推荐能力。<img src="/article_img/20250218/20250218_02.png" alt="K-RagRec"></p><h3 id="KG-Retriever-Efficient-Knowledge-Indexing-for-Retrieval-Augmented-Large-Language-Models（https-arxiv-org-pdf-2412-05547）"><a href="#KG-Retriever-Efficient-Knowledge-Indexing-for-Retrieval-Augmented-Large-Language-Models（https-arxiv-org-pdf-2412-05547）" class="headerlink" title="KG-Retriever: Efficient Knowledge Indexing for Retrieval-Augmented Large Language Models（https://arxiv.org/pdf/2412.05547）"></a>KG-Retriever: Efficient Knowledge Indexing for Retrieval-Augmented Large Language Models（<a href="https://arxiv.org/pdf/2412.05547%EF%BC%89">https://arxiv.org/pdf/2412.05547）</a></h3><p>Motivation: 对结构信息分层检索，从根本上缓解信息碎片化的问题</p><p>设计了Doc层(Document-level Graph)和KG层(Entity-level Graph)分别用于建立文档内和文档间的连接，Doc Graph的构建是对每个文档进行文本编码，并基于语义相似性获得每个节点的K近邻居；KG Graph的构建是对每个文档进行信息抽取，从而获得对应的知识图谱。检索策略上，作者考虑了两级检索：文档级检索和实体级检索，前者除了考虑用户查询和每个文档的语义相似性以外，还根据Doc Graph获取这些topN文档的one-hop邻居；后者则针对上一阶段检索的所有文档所对应的kg图检索语义相关的实体集，和query一起作为最终LLM的输入<br><img src="/article_img/20250218/20250218_03.png" alt="KG-Retriever"></p>]]></content>
    
    
    
    <tags>
      
      <tag>LLM</tag>
      
      <tag>RAG</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>关于模型padding左对齐和右对齐的问题</title>
    <link href="/2025/02/17/LLM%20padding/"/>
    <url>/2025/02/17/LLM%20padding/</url>
    
    <content type="html"><![CDATA[<p>最近微调模型进行原因预测任务训练的时候，在eval阶段进行推理生成时遇到了一个警告：“A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set <code>padding_side=&#39;left&#39;</code> when initializing the tokenizer.”，我将tokenizer初始化的定义设置为左填充依然出现该警告，问了下身边对大模型比较了解的同学，发现是token序列结尾加了eos符号导致出现的warning，下面是transformers库中该警告出现的条件：</p><p><img src="/article_img/LLM%20padding/warning.png" alt="warning"></p><p>为什么模型训练选择右填充，推理时选择左填充；为什么训练时需要在结尾添加<eos>标记符，而推理时则不需要，下面给出解释说明：</p><p>训练：Q+A[EOS]  </p><p>模型训练时，对于输入的token序列，我们知道其真实标签（Ques部分可直接用-100作为 <strong>mask</strong> 填充或无效标签，以确保这些位置不会影响损失计算），采用右填充是为了让每个batch内的样本长度对齐。在结尾添加eos标记符是为了告诉模型输入序列的结束位置，如果没有 EOS token，模型可能会将序列当作是没有结束的，进而可能会试图无限制地生成下一个 token，导致训练不稳定或生成行为不正确</p><p>推理：Q</p><p>自回归模型在推理时，从bos标记符开始从左到右依次预测下一个词来生成内容，如下面的图所示：</p><p><img src="/article_img/LLM%20padding/padding.png" alt="padding"></p><p>如果采用右填充，<pad>将被放置在In-Context的右端，从而改变了In-Context内容，这将导致生成时模型的上下文理解出现偏差，结果可能会影响到生成的质量或连贯性。此外，如果在In-Context的结尾处加入eos标记符，这将导致模型停止生成。</p><p>参考链接：<a href="https://zhuanlan.zhihu.com/p/4581421783">模型训练选择right填充，推理选择left填充，为什么？ - 知乎</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>Bug</tag>
      
      <tag>Code</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>大模型显存占用计算</title>
    <link href="/2025/02/17/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%98%BE%E5%AD%98%E5%8D%A0%E7%94%A8%E8%AE%A1%E7%AE%97/"/>
    <url>/2025/02/17/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%98%BE%E5%AD%98%E5%8D%A0%E7%94%A8%E8%AE%A1%E7%AE%97/</url>
    
    <content type="html"><![CDATA[<h2 id="1-显存单位换算"><a href="#1-显存单位换算" class="headerlink" title="1. 显存单位换算"></a>1. 显存单位换算</h2><p>在讨论显存占用时，首先要明白“B”和“G”的含义。通常，“B”指的是十亿（1B &#x3D; 10^9），而“G”则表示千兆字节（1G &#x3D; 10^9字节）。例如，1B参数意味着有10亿个参数。显存的单位通常以字节计算，而1个字节等于8位。<br>🎈如果使用全精度训练（fp32），每个参数需要占用32位（即4个字节），因此1B的参数需要占用4GB的显存。<br>🎈如果使用半精度（fp16或bf16），则每个参数占用2字节，1B的参数只需占用2GB的显存。</p><h2 id="2-显存开销的其他组成部分"><a href="#2-显存开销的其他组成部分" class="headerlink" title="2. 显存开销的其他组成部分"></a>2. 显存开销的其他组成部分</h2><p>除了模型参数本身外，训练过程中还会消耗一定的显存，主要包括以下几部分：<br>🎈梯度：每个参数对应一个梯度，因此梯度的显存占用与参数量相同。<br>🎈优化器状态：优化器，如Adam，通常会为每个参数保存一阶动量和二阶动量，因此优化器的显存开销为参数量的2倍（对于Adam）。对于其他优化器（如SGD），则取决于优化器的具体实现，若是带动量的SGD，则为参数量的1倍。</p><h2 id="3-显存总占用计算"><a href="#3-显存总占用计算" class="headerlink" title="3. 显存总占用计算"></a>3. 显存总占用计算</h2><p>假设我们训练一个参数量为1B的模型，采用全精度（fp32）并使用Adam优化器，显存的占用计算如下：<br>🎈参数：1B × 4GB &#x3D; 4GB<br>🎈梯度：1B × 4GB &#x3D; 4GB<br>🎈优化器状态：1B × 8GB &#x3D; 8GB<br>因此，总显存占用为16GB。如果使用半精度（bf16），则显存占用减半，为8GB。混合精度训练则会根据各部分精度调整计算结果。</p>]]></content>
    
    
    
    <tags>
      
      <tag>LLM</tag>
      
      <tag>Code</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>人工智能竞赛汇总</title>
    <link href="/2025/02/01/competition/"/>
    <url>/2025/02/01/competition/</url>
    
    <content type="html"><![CDATA[<h2 id="1-人工智能竞赛平台-Biendata：Data-Competition-Community-Biendata"><a href="#1-人工智能竞赛平台-Biendata：Data-Competition-Community-Biendata" class="headerlink" title="1.人工智能竞赛平台 Biendata：Data Competition Community - Biendata"></a><strong>1.人工智能竞赛平台 Biendata：</strong><a href="https://link.zhihu.com/?target=https://www.biendata.xyz/">Data Competition Community - Biendata</a></h2><p><strong>介绍：</strong></p><blockquote><p>2018 年 5 月，人工智能和大数据的竞赛平台 Biendata 完成天使轮融资，由DeepTech深科技投资，旨在打造中国人工智能赛事顶级 IP，赛事相关媒体运营。Biendata 的比赛客户既包括<strong>今日头条、知乎、摩拜、搜狐等企业，也包括了 IEEE、ACM、中国计算机学会、中国人工智能学会</strong>等国内外顶尖学术组织。</p></blockquote><p>总体上来说就是一个各自AI比赛汇总的平台（除了一些大厂有自己的大规模AI赛事比如阿里云天池、华为云、百度、腾讯），类似的办赛平台IP还有</p><ul><li><strong>datafountain：</strong><a href="https://www.datafountain.cn/competitions">数据科学竞赛&#x2F;大数据 竞赛 - DataFountain</a></li><li><strong>Kaggle(国外)：</strong><a href="https://www.kaggle.com/competitions">Kaggle Competitions</a></li></ul><p><strong>时间：</strong>基本上是什么时间都有，需要持续关注官网，同一个企业基本每年举办的timeline不变</p><h2 id="2-阿里云天池：算法大赛-天池大数据竞赛-天池大赛-阿里云天池"><a href="#2-阿里云天池：算法大赛-天池大数据竞赛-天池大赛-阿里云天池" class="headerlink" title="2.阿里云天池：算法大赛-天池大数据竞赛-天池大赛-阿里云天池"></a><strong>2.阿里云天池：</strong><a href="https://tianchi.aliyun.com/competition/gameList/algorithmList">算法大赛-天池大数据竞赛-天池大赛-阿里云天池</a></h2><p><strong>介绍：</strong>老牌，2014年启办，面向全世界科研人员和高校师生，业务场景丰富（2B2C都有cover），奖金池也丰富。</p><p><strong>时间：</strong>基本上是5月份开始报名——夏天初赛\复赛——10月决赛答辩</p><h2 id="3-华为云-：华为云大赛平台"><a href="#3-华为云-：华为云大赛平台" class="headerlink" title="3.华为云 ：华为云大赛平台"></a><strong>3.华为云 ：</strong><a href="https://competition.huaweicloud.com/competitions">华为云大赛平台</a></h2><p><strong>介绍：</strong>众所周知华为对研发投入比例很大，也十分重视创新大赛的举办，会联合各种高校、产品线、学术机构\组织办赛，每年任何时间节点都可能会有比赛。相关领域的同学需要持续关注。</p><p><strong>时间：</strong>一年中任意时刻</p><h2 id="4-百度飞桨AI-Studio大赛：飞桨AI-Studio-人工智能学习与实训社区"><a href="#4-百度飞桨AI-Studio大赛：飞桨AI-Studio-人工智能学习与实训社区" class="headerlink" title="4. 百度飞桨AI Studio大赛：飞桨AI Studio - 人工智能学习与实训社区"></a><strong>4. 百度飞桨AI Studio大赛：</strong><a href="https://aistudio.baidu.com/aistudio/competition/1/1">飞桨AI Studio - 人工智能学习与实训社区</a></h2><p><strong>介绍：</strong>AI Studio是基于百度深度学习平台飞桨的人工智能学习与实训社区，整体跟华为云的形式差异不大，但赛程周期会紧凑一些，内容也和百度的业务内容强相关（如<strong>NLP</strong>\图像检测\导航路径）。个人感觉场景会更专精于NLP，比如事件抽取\机器翻译\QA\阅读理解\情感分析。</p><p><strong>时间：</strong>每年3-5月</p><h2 id="5-腾讯：-2021腾讯广告算法大赛"><a href="#5-腾讯：-2021腾讯广告算法大赛" class="headerlink" title="5.腾讯： 2021腾讯广告算法大赛"></a><strong>5.腾讯：</strong> <a href="https://algo.qq.com/index.html%3Flang%3Dcn">2021腾讯广告算法大赛</a></h2><p><strong>介绍：</strong>腾讯AI赛事做的不是特别好，除了广告算法大赛还比较成规模（基本每年有延续），其他的都很分散，没有统一的平台。</p><blockquote><p>AI Lab偶尔有些算法挑战赛，AI温室种番茄什么的: <a href="https://ai.tencent.com/ailab/zh/index">腾讯 AI Lab - 腾讯人工智能实验室官网</a></p></blockquote><p><strong>时间：</strong>三月开始报名——夏季初赛\复赛——八月决赛答辩</p><h2 id="6-AIOps-Challenge智能运维挑战赛：竞赛列表"><a href="#6-AIOps-Challenge智能运维挑战赛：竞赛列表" class="headerlink" title="6.AIOps Challenge智能运维挑战赛：竞赛列表"></a><strong>6.AIOps Challenge智能运维挑战赛</strong>：<a href="https://iops.ai/competition_list/">竞赛列表</a></h2><p><strong>介绍：</strong>规模较小，每年承办方会有变化，但是主题围绕着AIOps不变</p><p><strong>时间：</strong>基本上是1月年初开始报名——春季初赛\复赛——五月决赛答辩</p><h2 id="7-KDD-CUP：KDD-2021-Singapore"><a href="#7-KDD-CUP：KDD-2021-Singapore" class="headerlink" title="7.KDD CUP：KDD 2021 | Singapore"></a><strong>7.KDD CUP：</strong><a href="https://www.kdd.org/kdd2021/%23">KDD 2021 | Singapore</a></h2><p><strong>介绍:</strong> 国外赛事，商业性质较弱所以奖金池较小，主题是多源数据时序异常检测\OGB-LSC\城市大脑挑战，延续性比较好。</p><p><strong>时间：</strong>每年五月-八月</p>]]></content>
    
    
    
    <tags>
      
      <tag>Study</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Recsys大牛实验室官网链接</title>
    <link href="/2025/02/01/lab/"/>
    <url>/2025/02/01/lab/</url>
    
    <content type="html"><![CDATA[<p>Xiangnan He：<a href="https://hexiangnan.github.io/">https://hexiangnan.github.io/</a><br>Yuan Fang@SMU: <a href="https://www.yfang.site/">https://www.yfang.site/</a><br>Huang Chao: <a href="https://sites.google.com/view/chaoh">https://sites.google.com/view/chaoh</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>Study</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>搭建代理服务器访问外网</title>
    <link href="/2025/02/01/buildvpn/"/>
    <url>/2025/02/01/buildvpn/</url>
    
    <content type="html"><![CDATA[<h1 id="buildVpn"><a href="#buildVpn" class="headerlink" title="buildVpn"></a>buildVpn</h1><h3 id="图文教程搭建一个vpn"><a href="#图文教程搭建一个vpn" class="headerlink" title="图文教程搭建一个vpn"></a>图文教程搭建一个vpn</h3><h3 id="准备工作：有支付宝账户，有一个可用邮箱，有10美元。（根据我下面的链接注册，会赠送300美元的体验金，可以试一下没成本，如果长期用可以再去充值）"><a href="#准备工作：有支付宝账户，有一个可用邮箱，有10美元。（根据我下面的链接注册，会赠送300美元的体验金，可以试一下没成本，如果长期用可以再去充值）" class="headerlink" title="准备工作：有支付宝账户，有一个可用邮箱，有10美元。（根据我下面的链接注册，会赠送300美元的体验金，可以试一下没成本，如果长期用可以再去充值）"></a>准备工作：有支付宝账户，有一个可用邮箱，有10美元。（根据我下面的链接注册，会赠送300美元的体验金，可以试一下没成本，如果长期用可以再去充值）</h3><h3 id="前言：首先我们选择Vultr供应商来购买海外VPS服务器，具有12个地区可以选择，当然也可以选择其他的供应商，但是Vultr的优点在于，所有服务器创建成功后开始计费，并且是按照小时来计费的，如果你删除掉服务器将不再计费。众所周知，目前国内的VPN打击特别严厉，很多VPN已经被封掉了，我们购买的海外服务器也有可能是被墙掉IP或者用一段时间被墙的。所以Vultr可以随时创建一个新的服务器（会分配一个新的ip），删除掉原有的。"><a href="#前言：首先我们选择Vultr供应商来购买海外VPS服务器，具有12个地区可以选择，当然也可以选择其他的供应商，但是Vultr的优点在于，所有服务器创建成功后开始计费，并且是按照小时来计费的，如果你删除掉服务器将不再计费。众所周知，目前国内的VPN打击特别严厉，很多VPN已经被封掉了，我们购买的海外服务器也有可能是被墙掉IP或者用一段时间被墙的。所以Vultr可以随时创建一个新的服务器（会分配一个新的ip），删除掉原有的。" class="headerlink" title="前言：首先我们选择Vultr供应商来购买海外VPS服务器，具有12个地区可以选择，当然也可以选择其他的供应商，但是Vultr的优点在于，所有服务器创建成功后开始计费，并且是按照小时来计费的，如果你删除掉服务器将不再计费。众所周知，目前国内的VPN打击特别严厉，很多VPN已经被封掉了，我们购买的海外服务器也有可能是被墙掉IP或者用一段时间被墙的。所以Vultr可以随时创建一个新的服务器（会分配一个新的ip），删除掉原有的。"></a>前言：首先我们选择Vultr供应商来购买海外VPS服务器，具有12个地区可以选择，当然也可以选择其他的供应商，但是Vultr的优点在于，所有服务器创建成功后开始计费，并且是按照小时来计费的，如果你删除掉服务器将不再计费。众所周知，目前国内的VPN打击特别严厉，很多VPN已经被封掉了，我们购买的海外服务器也有可能是被墙掉IP或者用一段时间被墙的。所以Vultr可以随时创建一个新的服务器（会分配一个新的ip），删除掉原有的。</h3><h3 id="创建账户及购买VPS服务器"><a href="#创建账户及购买VPS服务器" class="headerlink" title="创建账户及购买VPS服务器"></a>创建账户及购买VPS服务器</h3><p>第一步：登录vultr官网注册一个账户，只需要一个邮箱和密码即可。然后到你注册的邮箱中去验证你的账户。</p><h4 id="官网推广链接-https-www-vultr-com-ref-9695214-9J-这个链接是官网一个推荐链接，有300刀体验金"><a href="#官网推广链接-https-www-vultr-com-ref-9695214-9J-这个链接是官网一个推荐链接，有300刀体验金" class="headerlink" title="官网推广链接(https://www.vultr.com/?ref=9695214-9J) 这个链接是官网一个推荐链接，有300刀体验金"></a>官网推广链接(<a href="https://www.vultr.com/?ref=9695214-9J">https://www.vultr.com/?ref=9695214-9J</a>) 这个链接是官网一个推荐链接，有300刀体验金</h4><p>官网链接(<a href="https://www.vultr.com/?ref=7348872">https://www.vultr.com/?ref=7348872</a>)        这个链接就是普通的推荐链接</p><p><img src="/article_img/buildvpn/20180307101419422.jpg" alt="第一步"></p><p>第二步：登录你的账户，然后在如图所示地方进行充值。这里我们可以使用微信或支付宝扫码支付，充值成功后，可以再右上角看到你的账户余额。</p><p><img src="/article_img/buildvpn/20180307101506198.jpg" alt="第二步"></p><p>第三步：购买VPS服务器。在Servers标签中看到，我们目前还没有服务器，这时选择右上角的加号新添加一个服务器。</p><p><img src="/article_img/buildvpn/20180307101543991.jpg" alt="第三步"></p><p>我们首先选择服务器所在地区，这里我们选择NewYork纽约，一般来说选择日本、纽约、洛杉矶、硅谷都还可以（全看人品）。</p><p><img src="/article_img/buildvpn/20180307101558312.jpg" alt="第四步"></p><p>其次我们选择服务器的系统版本。这里注意选择CentOS6 ，默认是7由于防火墙等原因可能会影响接下来的操作。</p><p><img src="/article_img/buildvpn/20180307101612917.jpg" alt="第五步"></p><p>最后，我们选择每个月2.5刀，500G带宽的就可以了。</p><p><img src="/article_img/buildvpn/20180307101629749.jpg" alt="第六步"></p><p>其他的不需要选择，如果需要使用IPv6就在第四部选择。这里我们不选择，默认使用IPv4，最下面我们选择Depliy Now 新建服务器。</p><p><img src="/article_img/buildvpn/20180307101643841.jpg" alt="第七步"></p><p>到目前为止我们就已经成功的创建了一个海外服务器。但是这个服务器是否可用，有没有被墙掉呢？当服务器安装完成之后，我们来测试一下。</p><p><img src="/article_img/buildvpn/20180307101656506.jpg" alt="第八步"></p><p>我们可以看到已经在运行的服务器ip为  45.63.7.251 ，接下来就测试一下是否能够连接。<br>Win： win + R快捷键或者在开始菜单-附件-运行，调出运行窗口，输入cmd，然后输入ping  45.63.7.251 可以看到是否被墙。（多ping几次）<br>Mac + Linux：直接在命令行窗口输入ping  45.63.7.251 （按ctrl + c 退出）<br>或者通过网站ping检测，如果全是超时代表被墙了。<a href="http://ping.chinaz.com/">http://ping.chinaz.com/</a></p><p><img src="/article_img/buildvpn/20180307101706991.jpg" alt="第九步"></p><p>这里可以看到刚刚新建的服务器是被墙掉的。无法访问，这时就再新建一个服务器，然后在ping。如果同一个地区多次无法ping通，就换一个地区试试，这里推荐日本。（每次新建服务器，按小时首付0.01刀，删除后不计费，十次也才不到一块钱）</p><p>如图，我再次新建了一个服务器，这回可以ping通，说明没有被墙。就是延时高一点，延时与你的网络和当前的时间段，使用的人数有关。</p><p><img src="/article_img/buildvpn/20180307101717865.jpg" alt="第九步"></p><p><img src="/article_img/buildvpn/20180307101733164.jpg" alt="第十步"></p><p>这时我们把之前被墙掉的服务器删除就可以了。新建可用的服务器已经全部完成了。</p><p><img src="/article_img/buildvpn/20180307101748789.jpg" alt="第十一步"></p><p>点击详情可以看到服务器的用户名和密码</p><p><img src="/article_img/buildvpn/20180307101802763.jpg" alt="第十二步"></p><p><img src="/article_img/buildvpn/20180307101812373.jpg" alt="第十三步"></p><p>接下来我们就可以搭建VPN了。距离成功已经很近了。</p><p>首先如果我们是Windows系统需要下载一个软件（Mac 或 Linux不需要），Xshell或者SecureCRT。这里我用的是SecureCRT。<br>填写你的IP地址，用户名为root，点击链接，点击接受保存，输入你的密码，成功连接。</p><p><img src="/article_img/buildvpn/20180307101823784.jpg" alt="第十四步"><br><img src="/article_img/buildvpn/20180307101836775.jpg" alt="第十五步"></p><p>其次要设置编码格式，不然一会的中文会显示乱码。菜单栏 选项-会话选项-外观-字符编码-UTF8-确认。<br>关闭软件，重新连接（直接双击IP就可以了）。</p><p><img src="/article_img/buildvpn/20180307101848327.jpg" alt="第十六步"></p><p>然后复制下面的一键部署管理脚本，粘贴到窗口中（鼠标右键一下即可粘贴）</p><p>CentOS6&#x2F;Debian6&#x2F;Ubuntu14 ShadowsocksR一键部署管理脚本(可以把下面命令按行拆开分步执行)：</p><p>wget –no-check-certificate <a href="https://raw.githubusercontent.com/teddysun/shadowsocks_install/master/shadowsocksR.sh">https://raw.githubusercontent.com/teddysun/shadowsocks_install/master/shadowsocksR.sh</a></p><p>chmod +x shadowsocksR.sh</p><p>.&#x2F;shadowsocksR.sh 2&gt;&amp;1 | tee shadowsocksR.log</p><h3 id="主脚本安装SSR："><a href="#主脚本安装SSR：" class="headerlink" title="主脚本安装SSR："></a>主脚本安装SSR：</h3><p>第一步：设定密码，default 为默认密码</p><p><img src="/article_img/buildvpn/new2.png" alt="第十六-1步"></p><p>第二步：设定端口，default 为随机生成默认端口<br><img src="/article_img/buildvpn/new3.png" alt="第十六-2步"></p><p>第三步：设定加密方式，default 为默认加密方式<br><img src="/article_img/buildvpn/new4.png" alt="第十六-3步"></p><p>第四步：设协议，default 为默认协议<br><img src="/article_img/buildvpn/new1.png" alt="第十六-4步"></p><p>第五步：设定混淆方式，default 为默认混淆<br><img src="/article_img/buildvpn/new5.png" alt="第十六-5步"></p><p>第六步：安任意键，回车开始进行安装，安装完成后自动启动<br><img src="/article_img/buildvpn/new7.png" alt="第十六-6步"></p><p>当安装出现问题时，有可能是centOS中缺少相应c编译器，可以分别执行以下指令安装编译器后再安装SSR：</p><p>yum -y install gcc automake autoconf libtool make</p><p>yum -y install gcc-c++</p><p>最终：安装完成，展示你所设置的内容，可以按照链接信息进行连接(最近较严有可能被墙或者端口被封)<br><img src="/article_img/buildvpn/new6.png" alt="第十六-7步"></p><p>安装过后如果想要修改，运行如下相关命令</p><p>启动：&#x2F;etc&#x2F;init.d&#x2F;shadowsocks start</p><p>停止：&#x2F;etc&#x2F;init.d&#x2F;shadowsocks stop</p><p>重启：&#x2F;etc&#x2F;init.d&#x2F;shadowsocks restart</p><p>状态：&#x2F;etc&#x2F;init.d&#x2F;shadowsocks status</p><p>配置文件路径：&#x2F;etc&#x2F;shadowsocks.json  修改文件用vi 或者 vim命令，使用方法百度</p><p>日志文件路径：&#x2F;var&#x2F;log&#x2F;shadowsocks.log </p><p>安装路径：&#x2F;usr&#x2F;local&#x2F;shadowsocks&#x2F;shadowsoks</p><p>卸载.&#x2F;shadowsocksR.sh uninstall</p><h3 id="备用脚本安装SSR：（如果此时链接断了，重连后输入-ssr-sh-就可以进入下面安装操作，以后修改时也输入-ssr-sh）"><a href="#备用脚本安装SSR：（如果此时链接断了，重连后输入-ssr-sh-就可以进入下面安装操作，以后修改时也输入-ssr-sh）" class="headerlink" title="备用脚本安装SSR：（如果此时链接断了，重连后输入.&#x2F;ssr.sh 就可以进入下面安装操作，以后修改时也输入.&#x2F;ssr.sh）"></a>备用脚本安装SSR：（如果此时链接断了，重连后输入.&#x2F;ssr.sh 就可以进入下面安装操作，以后修改时也输入.&#x2F;ssr.sh）</h3><p>备用脚本（上面的脚步不可用再输入这个）：</p><p>wget -N –no-check-certificate <a href="https://raw.githubusercontent.com/ToyoDAdoubi/doubi/master/ssr.sh">https://raw.githubusercontent.com/ToyoDAdoubi/doubi/master/ssr.sh</a> &amp;&amp; chmod +x ssr.sh &amp;&amp; bash ssr.sh</p><p>如果在输入命令式提示wget:command not found，则表示没有wget工具，先输入下面指令进行安装，然后再部署管理脚本<br>yum -y install wget</p><p>第一步：选择1 </p><p><img src="/article_img/buildvpn/20180307101902154.jpg" alt="第十七步"></p><p>第二步：直接默认即可。（理论上说是可以随便的，1 - 65535）</p><p><img src="/article_img/buildvpn/20180307101913385.jpg" alt="第十八步"></p><p>第三步：设置密码</p><p><img src="/article_img/buildvpn/20180307101927773.jpg" alt="第十九步"></p><p>第四步：加密方式，选择aes-128-cfb就可以</p><p><img src="/article_img/buildvpn/20180307101937845.jpg" alt="第二十步"></p><p>第五步：协议插件，为了使SS也能够使用，这里选择origin</p><p><img src="/article_img/buildvpn/20180307101948387.jpg" alt="第二十一步"></p><p>第六步：选择混淆plain</p><p><img src="/article_img/buildvpn/20180307101957836.jpg" alt="第二十二步"></p><p>第七步：设置连接数量，默认回车即可。然后开始进行安装。</p><p><img src="/article_img/buildvpn/20180307102008416.jpg" alt="第二十三步"></p><p>如果遇到输入项，问y还是n时，输入y 回车确认。</p><p><img src="/article_img/buildvpn/20180307102018140.jpg" alt="第二十四步"></p><h3 id="到此安装就完成了。可以通过客户端进行链接翻墙上网了。"><a href="#到此安装就完成了。可以通过客户端进行链接翻墙上网了。" class="headerlink" title="到此安装就完成了。可以通过客户端进行链接翻墙上网了。"></a>到此安装就完成了。可以通过客户端进行链接翻墙上网了。</h3><p><img src="/article_img/buildvpn/2018030710205260.jpg" alt="第二十五步"></p><p>如果SSR成功安装，但是不能正常启动，在centOS中主要原因是缺少python环境，利用以下指令进行安装</p><p>yum -y install python36</p><p>cd &#x2F;usr&#x2F;bin</p><p>ln -s python3 python</p><h3 id="为了能够提高上网速度，YouTube从480-体验为1080。我们接下来进行安装加速软件（速锐、BBR两者选其一，不可共存）。"><a href="#为了能够提高上网速度，YouTube从480-体验为1080。我们接下来进行安装加速软件（速锐、BBR两者选其一，不可共存）。" class="headerlink" title="为了能够提高上网速度，YouTube从480 体验为1080。我们接下来进行安装加速软件（速锐、BBR两者选其一，不可共存）。"></a>为了能够提高上网速度，YouTube从480 体验为1080。我们接下来进行安装加速软件（速锐、BBR两者选其一，不可共存）。</h3><h3 id="BBR加速（如果需要速锐，跳过此段）"><a href="#BBR加速（如果需要速锐，跳过此段）" class="headerlink" title="BBR加速（如果需要速锐，跳过此段）"></a>BBR加速（如果需要速锐，跳过此段）</h3><p>BBR加速特别简单，复制下面脚本代码即可。<br>谷歌BBR加速脚本：</p><p>第一个指令：wget –no-check-certificate <a href="https://github.com/teddysun/across/raw/master/bbr.sh">https://github.com/teddysun/across/raw/master/bbr.sh</a></p><p>第二个指令：chmod +x bbr.sh</p><p>第三个指令：.&#x2F;bbr.sh</p><p>1、遇到停顿按回车即可。然后继续安装。（多等一会）</p><p><img src="/article_img/buildvpn/20180307102104748.jpg" alt="第二十六步"></p><p>2、安装完成后问你是否重启，这里输入y，回车。</p><p><img src="/article_img/buildvpn/20180307102114503.jpg" alt="第二十七步"></p><p>3、重新连接后，输入 lsmod | grep bbr 查看BBR是否启动，可以看到已经启动了。</p><p><img src="/article_img/buildvpn/20180307102126665.jpg" alt="第二十八步"></p><h3 id="备用脚本安装SS："><a href="#备用脚本安装SS：" class="headerlink" title="备用脚本安装SS："></a>备用脚本安装SS：</h3><p>wget –no-check-certificate -O shadowsocks-all.sh <a href="https://raw.githubusercontent.com/teddysun/shadowsocks_install/master/shadowsocks-all.sh">https://raw.githubusercontent.com/teddysun/shadowsocks_install/master/shadowsocks-all.sh</a></p><p>第一项选1 python安装，其他选择与主脚本描述一样</p><h3 id="速锐加速"><a href="#速锐加速" class="headerlink" title="速锐加速"></a>速锐加速</h3><p>1、首先输入<br>uname -a 查看内核为</p><p>2、然后输入<br>cat &#x2F;etc&#x2F;redhat-release  查看系统版本</p><p>3、下载CentOS 6.6版本的内核（速锐支持6.6版本的）<br>wget <a href="http://ftp.scientificlinux.org/linux/scientific/6.6/x86_64/updates/security/kernel-2.6.32-504.3.3.el6.x86_64.rpm">http://ftp.scientificlinux.org/linux/scientific/6.6/x86_64/updates/security/kernel-2.6.32-504.3.3.el6.x86_64.rpm</a></p><p>4、安装内核<br>rpm -ivh kernel-2.6.32-504.3.3.el6.x86_64.rpm –force</p><p>5、重启服务器<br>reboot</p><p>6、安装速锐<br>wget -N –no-check-certificate <a href="https://raw.githubusercontent.com/91yun/serverspeeder/master/serverspeeder-all.sh">https://raw.githubusercontent.com/91yun/serverspeeder/master/serverspeeder-all.sh</a> &amp;&amp; bash serverspeeder-all.sh</p><p>这时正常情况速锐就已经安装完成并且启动了。</p><p><img src="/article_img/buildvpn/20180307102140531.jpg" alt="第二十九步"></p><p>service serverSpeeder status     查看速锐的状态<br>service serverSpeeder start | stop | restart  停止暂停重启锐速</p><h3 id="到此为止，通过BBR或速锐加速的VPN服务器已经全部搭建完成了。"><a href="#到此为止，通过BBR或速锐加速的VPN服务器已经全部搭建完成了。" class="headerlink" title="到此为止，通过BBR或速锐加速的VPN服务器已经全部搭建完成了。"></a>到此为止，通过BBR或速锐加速的VPN服务器已经全部搭建完成了。</h3><h3 id="接下来使用SSR或者SS客户端连接即可"><a href="#接下来使用SSR或者SS客户端连接即可" class="headerlink" title="接下来使用SSR或者SS客户端连接即可"></a>接下来使用SSR或者SS客户端连接即可</h3><p>MAC：<a href="https://github.com/shadowsocksr-backup/ShadowsocksX-NG/releases">https://github.com/shadowsocksr-backup/ShadowsocksX-NG/releases</a><br>WIN：<a href="https://github.com/shadowsocksr-backup/shadowsocksr-csharp/releases">https://github.com/shadowsocksr-backup/shadowsocksr-csharp/releases</a><br>IPHONE：FirstWingy、potatso lite  （商店里有，实在找不到可以弄个美国APPLEID，什么都能下）</p><p>以iphone为例：首先右上角加号，添加服务器配置信息。</p><p><img src="/article_img/buildvpn/20180307102153178.jpg" alt="第三十步"></p><p>然后：填写一开始安装时的信息,保存。如果忘了记得 .&#x2F;ssr.sh  选择5 查看连接信息</p><p><img src="/article_img/buildvpn/20180307102204995.jpg" alt="第三十一步"></p><p>这时你可以愉快的翻墙上网了。</p><p><img src="/article_img/buildvpn/20180307102258394.jpg" alt="第三十二步"></p><h3 id="如果使用SSR无法连接网络，则可能是centOS未开放相关端口，接下来查询并开启端口"><a href="#如果使用SSR无法连接网络，则可能是centOS未开放相关端口，接下来查询并开启端口" class="headerlink" title="如果使用SSR无法连接网络，则可能是centOS未开放相关端口，接下来查询并开启端口"></a>如果使用SSR无法连接网络，则可能是centOS未开放相关端口，接下来查询并开启端口</h3><p>.&#x2F;ssr.sh 选择5 查看配置信息中的端口号</p><p>假如我们使用的是2333端口</p><p>用以下指令可以查看是否开启对应端口</p><p>firewall-cmd –list-ports</p><p><img src="/article_img/buildvpn/port.png" alt="第三十三步"></p><p>如果没有出现2333&#x2F;tcp的字样，那么该端口还没有开放。使用以下命令开放相应端口并重启：</p><p>firewall-cmd –zone&#x3D;public –add-port&#x3D;2333&#x2F;tcp –permanent</p><p>reboot</p><p>等待1分钟左右，端口已开放，SSR可以连接</p>]]></content>
    
    
    
    <tags>
      
      <tag>Web</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>日语词汇积累</title>
    <link href="/2025/01/01/%E6%97%A5%E8%AF%AD%E8%AF%8D%E6%B1%87%E7%A7%AF%E7%B4%AF/"/>
    <url>/2025/01/01/%E6%97%A5%E8%AF%AD%E8%AF%8D%E6%B1%87%E7%A7%AF%E7%B4%AF/</url>
    
    <content type="html"><![CDATA[<p>2025.03.17<br> 愛(あい) 爱  池(いけ) 鱼塘  おはよう 早上好  おねがい 拜托了  はい 是的  うるさい 烦人的  いいえ 不，不是  かわいい 可爱的  かっこいい 帅的  傘(かさ) 伞  おてあらい 厕所<br>2025.03.19<br> 海(うみ) 海  嘘(うそ) 假话  世界(せかい) 世界  寿司(すし) 寿司  おいしい 好吃  お菓子(おかし) 点心  菊(きく) 菊花  君(きみ) 你  声(こえ) 声音  柿(かき) 柿子<br>2025.03.21<br> 最高(さいこう) 最高，最好  顔(かお) 面孔，脸  猫(ねこ) 猫  ちち 父亲  たこやき 章鱼烧  机(つくえ) 桌子  何(なに) 什么  年(とし) 年龄　 夏(なつ) 夏天  梨(なし) 梨<br>2025.03.23<br> 招き猫(まねきねこ) 招财猫  あほ 白痴  兄(あに) 哥哥  西(にし) 西  下手(へた) 不擅长  地下鉄(ちかてつ) 地铁  うまい 好吃  耳(みみ) 耳朵  味增汁(みそしる)  味增汤  雨(あめ) 雨<br>2025.03.25<br> 山(やま) 山  彼(かれ) 他  無理(むり) 无理  娘(むすめ) 女儿  もしもし 喂（电话用语） 鳥(とり) 鸟  雲り(くもり) 阴天  奈良(なら) 奈良县  桜(さくら) 樱花  花嫁(はなよめ) 新娘<br>2025.03.27<br> ぜろ  0  いち  1  に  2  さん  3  よん  4　 ろく  6  なな  7  あか  红色  あお  蓝色  くろ  黑色  しろ  白色</p><p>日常用语整理：</p><ol><li>指示代词<br>第一人称：わたし　わたくし　あたし(女士专用)　ぼく&#x2F;おれ(男士专用)<br>第二人称：あなた　きみ　おまえ<br>第三人称：かれ 他　かのじょ 她；女朋友　かれし 他；男朋友<br>不定人称：だれ　どなた</li><li>自我介绍<br>はじめまして 初次见面 わたし 我  なまえ 名字 ことし 今年 だいがく 大学 いちねんせい 大一学生<br>がっこう 学校 せんもん 专业 しゅっしん 出身，家乡 かぞく 家族 これがら 今后<br>はは 母亲 ちち 父亲 いっしょに 一起 どうぞ 请 よろしく 多多关照</li><li>问候语<br>おはよう 早上好 こんにちは 你好，下午好 こんぱんは 晚上好 おやすみなさい 晚安 いってきます 我出门了 いってらっしやい 路上小心<br>なだいま 我回来了 おかえりなさい 欢迎回来 ありがとう 谢谢 ごめんなさい 对不起 すみません 对不起 いただきます 我开动了<br>ごちそうさまでした 多谢款待 はじめまして 初次见面 さようなら 再见 ぱいぱい 拜拜 じゃね 拜拜</li></ol>]]></content>
    
    
    
    <tags>
      
      <tag>Language</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
