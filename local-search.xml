<?xml version="1.0" encoding="utf-8"?>
<search>
  
  
  
  <entry>
    <title>GraphRAG综述</title>
    <link href="/2025/02/20/GraphRAG%E7%BB%BC%E8%BF%B0/"/>
    <url>/2025/02/20/GraphRAG%E7%BB%BC%E8%BF%B0/</url>
    
    <content type="html"><![CDATA[<h4 id="Sec-1-传统RAG系统面临的挑战"><a href="#Sec-1-传统RAG系统面临的挑战" class="headerlink" title="Sec.1 传统RAG系统面临的挑战"></a>Sec.1 传统RAG系统面临的挑战</h4><p><strong>复杂的查询理解：</strong>传统的 RAG 方法依赖于简单的关键字匹配和向量相似性技术，不足以捕捉准确和全面所需的深层语义细微差别和多步骤推理过程。例如，当询问概念 A 和概念 D 之间的联系时，这些系统通常只检索直接相关的信息，而错过了像 B 和 C 这样可以弥合关系的关键中间概念。这种狭窄的检索范围限制了 RAG 的能力，使其无法进行广泛的上下文理解和复杂的推理</p><p><strong>集成来自分布式源的领域知识：</strong>检索到的知识通常是扁平的、广泛的和错综复杂的，而领域概念通常分散在多个文档中，不同概念之间没有明确的层次结构关系。尽管 RAG 系统试图通过将文档划分为较小的块以实现有效和高效的索引来管理这种复杂性，但这种方法无意中牺牲了关键的上下文信息，严重损害了检索准确性和上下文理解。这种限制阻碍了在相关知识点之间建立强大联系的能力，导致理解碎片化，并降低利用特定领域专业知识的效率。</p><p><strong>LLM的固有约束：</strong>受到其固定上下文窗口的限制，LLM无法完全捕获复杂文档中的长距离依赖关系。在专业领域中，在广泛的知识背景下保持连贯性的挑战变得越来越棘手，因为关键信息可能会在上下文窗口截断期间丢失。</p><p><strong>系统效率和可拓展性：</strong>RAG 系统在计算上可能既昂贵又耗时 ，尤其是在处理大规模知识源时，因为模型需要搜索大量非结构化文本以查找相关信息。此外，实时检索和跨文档推理可能会带来相当大的延迟，从而对用户体验产生负面影响。此外，实时检索和跨文档推理可能会带来相当大的延迟，从而对用户体验产生负面影响，从而限制了它在广泛和动态的专业环境中的实际部署</p><h4 id="Sec-2-现有的GraphRAG分类"><a href="#Sec-2-现有的GraphRAG分类" class="headerlink" title="Sec.2 现有的GraphRAG分类"></a>Sec.2 现有的GraphRAG分类</h4><p><strong>基于知识的GraphRAG:</strong> 它使用图作为知识载体，专注于将非结构化文本文档转换为显式和结构化的 KG，其中node代表领域概念，edge捕获它们之间的语义关系，从而更好地表示分层关系和复杂的知识依赖关系</p><p><strong>基于索引的GraphRAG:</strong> 使用图作为索引工具从语料库中检索相关的原始文本，它保留了原始文本形式，同时主要将图形结构用作索引机制来有效地组织和检索相关文本块。通过将图形结构合并到文本索引中，基于索引的 GraphRAG 方法在文本块之间建立语义连接，以实现高效的查找作和检索</p><p><strong>基于知识的 GraphRAG 旨在通过基于图的推理能力创建结构化的知识表示，以便更好地理解复杂的关系；而基于索引的 GraphRAG 则侧重于通过基于图的索引策略优化相关文本信息的检索和可访问性</strong></p><h4 id="Sec-3-Overview-the-Framework-of-RAG"><a href="#Sec-3-Overview-the-Framework-of-RAG" class="headerlink" title="Sec.3 Overview the Framework of RAG"></a>Sec.3 Overview the Framework of RAG</h4><p>RAG框架主要由三个部分组成，Knowledge Organization,Knowledge Retrieval,Knowledge Integration。下图展示了传统RAG、基于知识的GraphRAG和基于索引的GraphRAG在这三部分的各自实现方式。</p><p><img src="/../article_img/20250220/img1.png" alt="传统 RAG 和两个典型 GraphRAG 工作流程的全面概述。传统 RAG 将语料库组织成块，按相似性对它们进行排名，并检索最相关的文本以生成响应。基于知识的 GraphRAG 使用实体识别和关系提取从语料库中提取详细的知识图谱，提供精细的、特定于领域的信息。基于索引的 GraphRAG 将语料库总结为高级主题节点，这些节点链接以形成索引图，而事实链接将主题映射到文本。这种两层结构将高效的主题检索与详细的文本知识相结合，与基于知识的 GraphRAG 相比，提供了可扩展性和性能"></p><p><strong>Knowledge Organization:  该部分关注外部知识库的构建</strong></p><p><strong>传统RAG</strong>：传统RAG主要采用将大规模文本语料库拆分为可管理块的策略，然后使用嵌入模型将这些块转换为嵌入，其中嵌入用作向量数据库中原始文本块的键。此设置通过在语义空间中基于距离的搜索实现高效的查找作和检索相关内容。常见的优化方法：<strong>粒度优化</strong>和<strong>索引优化</strong>。</p><p><strong>GraphRAG:</strong> 基于图的方法构建外部信息，通过显式知识表示（作为知识载体的图）或索引机制（用于知识索引的图）。这些方法可实现高效的上下文感知信息检索。</p><p><strong>Knowledge Retrieval: 该部分关注如何更精准高效的进行检索</strong></p><p><strong>传统RAG</strong>: 常涉及的检索方法：KNN、TF-IDF、BM25。为了提高检索的准确性和效率，一方面可以在检索前通过优化表示或重排序等技术来提高检索模型的准确性，例如<a href="https://arxiv.org/pdf/2009.08553">GAR</a>、<a href="https://arxiv.org/pdf/2305.17080">EAR</a>等；另一方面，对检索模型进行训练，例如<a href="https://arxiv.org/pdf/2301.12652">Replug</a>，<a href="https://www.jmlr.org/papers/volume24/23-0037/23-0037.pdf">Atlas</a>，<a href="https://arxiv.org/pdf/2305.06983">Flare</a>等。</p><p><strong>GraphRAG:</strong> GraphRAG 模型使用基于图的规划器（可学习的规划器或基于图算法的规划器）根据输入查询检索相关信息。这些检索技术不仅考虑了查询和每个文本块之间的语义相似性，还考虑了查询类型和检索到的子图之间的逻辑连贯性</p><p><strong>Knowledge Integration:  该部分关注如何将检索结果和用户查询高质量整合</strong></p><p><strong>传统RAG</strong>:提高检索到的内容的质量：强化学习方法<a href="https://www.sciencedirect.com/science/article/pii/S294971912400013X">LeanContext</a> LLM自评估方法<a href="https://arxiv.org/pdf/2310.11511">Self-RAG</a>  <a href="https://arxiv.org/pdf/2307.03027">评估检索内容重要性</a>。此外，考虑到对大量检索到的段落进行编码是资源密集型的，这会导致大量的计算和内存开销，相关优化方法：<a href="https://arxiv.org/pdf/2404.12457">RAGCache</a> <a href="https://arxiv.org/pdf/2310.15556">TCRA-LLM</a></p><p><strong>GraphRAG:</strong> 一旦检索到相关知识，GraphRAG 模型就会将其与用户查询整合起来作为LLM input。集成过程的目标是将检索到的知识无缝合并到生成的文本中，从而提高其质量和信息量。一个关键的设计考虑因素是，如何在最终的基于文本的提示中保留检索到的子图信息的丰富性，而不会引入冗余或错误地强调文本描述中不太关键的方面</p><h4 id="Sec-4-Knowledge-Organization-in-GraphRAG"><a href="#Sec-4-Knowledge-Organization-in-GraphRAG" class="headerlink" title="Sec.4 Knowledge Organization in GraphRAG"></a>Sec.4 Knowledge Organization in GraphRAG</h4><p>首先构建一个图结构来组织知识，然后检索和集成与查询相关的信息。下面对Sec.2 提到的范式展开具体介绍。</p><p><strong>Graphs for Knowledge Indexing</strong></p><p>​基于索引的 GraphRAG 方法利用图结构来索引和检索相关的原始文本块，然后将其馈送到 LLM 中以进行知识注入和上下文理解。这些索引图应用语义相似性或特定于域的关系等原则来有效地桥接单独文本段落之间的连接。与仅将图用作知识载体相比，这种技术通过直接总结与查询相关的原始文本块中的信息来提供更丰富的答案。</p><p>​面临的挑战：（1） <strong>简洁性和相关性</strong>：确保构建的图仅捕获相关关系，而不会因不必要的连接而过载，这是一项重大挑战，从而有助于有效调用相关文本块而不会产生冗余 （2）<strong>一致性和冲突解决：</strong>不同的数据块可能会引入冲突的信息。解决这些冲突并确保图保持一致、可靠和结构良好至关重要。</p><p><strong>Graphs for Knowledge Carriers</strong></p><p>​优势：1）高效检索与查询相关的知识 2）长跨度的连贯多步推理  </p><p>​局限性：（1） <strong>缺乏高质量的 KG：</strong>对于直接使用KG作为外部知识库，这一研究方向受到高质量KG可用性的限制。构建KG是资源密集型的，但大多数公开可用的KG仍然远非全面（2） <strong>效率和有效性之间的权衡：</strong>当从文本语料库构建 KG 时，提取知识的粒度在平衡效率和有效性方面起着至关重要的作用。保留细粒度信息会导致更大、更详细的 KG，这可能会阻碍计算效率。相反，紧凑的 KG 可能会牺牲重要的细节，从而导致潜在的信息丢失。</p><h4 id="Sec-5-Knowledge-Retrieval-Process"><a href="#Sec-5-Knowledge-Retrieval-Process" class="headerlink" title="Sec.5 Knowledge Retrieval Process"></a>Sec.5 Knowledge Retrieval Process</h4><p>​基于图的知识检索一般分为Preprocess&#x2F;Matching&#x2F;Pruning三个步骤，如下图所示：</p><p><img src="/../article_img/20250220/img2.png" alt="Retrieval Process"></p><p><strong>Query&#x2F;Graph Preprocess</strong> 预处理阶段同时对查询数据库和图形数据库运行，以便为高效检索做好准备。对于查询预处理，系统通过矢量化或关键术语提取将输入问题转换为结构化表示。这些表示形式用作后续检索作的搜索索引。在图方面，图数据库经过更全面的处理，其中预训练的语言模型将图元素（实体、关系和三元组）转换为密集的向量表示，作为检索锚点 。此外，一些高级检索模型在图数据库上应用图神经网络 （GNN） 来提取高级结构特征，而一些方法甚至采用规则挖掘算法生成规则库，作为图知识的丰富、可搜索的索引</p><p><strong>Matching</strong> 匹配阶段在预处理的查询和索引图数据库之间建立连接。此过程将查询表示形式与图索引进行比较，以识别相关的知识片段。匹配算法同时考虑图中的<strong>语义相似性和结构关系</strong>。根据匹配分数，系统检索与查询高度相关的连通组件和子图，从而创建一组初始的候选知识。</p><p><strong>Knowledge Pruning</strong> 修剪阶段会优化最初检索的知识，以提高其质量和相关性。此优化过程解决了检索过多或不相关信息的常见挑战，尤其是在处理复杂查询或大型图数据库时。剪枝算法应用一系列细化作来整合和总结检索到的知识。具体来说，系统首先删除明显不相关或嘈杂的信息。然后，它整合了相关的知识片段，并生成了复杂图知识的简明摘要。通过提供精炼和重点突出的摘要，LLM 能够更好地理解信息的上下文和细微差别，从而做出更准确和有意义的回答。</p><h4 id="Sec-6-Knowledge-Retrieval-Techniques"><a href="#Sec-6-Knowledge-Retrieval-Techniques" class="headerlink" title="Sec.6 Knowledge Retrieval Techniques"></a>Sec.6 Knowledge Retrieval Techniques</h4><p><strong>基于语义相似性的检索器</strong> 通过测量离散语言空间或连续向量空间中的查询与知识库之间的相似性来进行适当的答案检索 （1）离散空间建模。离散空间建模方法主要利用语言离散统计知识直接对文本字符串进行建模。例如子字符串匹配、正则表达式和精确短语匹配等算法  （2）嵌入空间建模。利用预训练语言模型和词嵌入等方法，例如 TF-IDF、Word2Vec 和 GloVe</p><p><strong>基于逻辑推理的检索器</strong> 采用符号推理从图知识库中推断和提取相关信息。此方法包括创建逻辑规则和约束，以阐明知识库固有的关系和层次结构，例如 规则挖掘 、归纳逻辑编程 和 约束满足 等技术</p><p><strong>基于 GNN 的 Retriever</strong> 主要利用图神经网络对构建的图库中的节点进行编码。检索主要依赖于同时包含情感意义和结构关系理解的节点表征的编码相似性。基于 GNN 的检索器需要训练 GNN 编码器。此外，由于缺乏明确标注的数据，<strong>训练的重点是设计一个合适的损失函数，使 GNN 能够学习通过表示编码准确定位目标知识</strong></p><p><strong>基于 LLM 的检索器</strong> 关于构建的图库，基于 LLM 的知识检索器主要侧重于利用 LLM 来理解图并识别关键子图。</p><p><strong>基于强化学习的检索器</strong> 强化学习 为 GraphRAG 系统中的检索提供了一种自适应和动态策略。通过将检索过程构建为顺序决策挑战，基于 RL 的方法使代理能够在环境反馈的指导下学习和遍历图库，以寻找最相关的信息。</p><p>这种方法赋予系统通过主动交互和积累经验不断提高其检索性能的能力。这个过程可以描述如下：相关的推理背景在于一个特定于问题的子图 $G_{sub}$，其中包含所有源实体 $Q_s$、目标实体 $Q_t$ 及其邻居。理想的子图 $G_{sub}$ 应具有以下属性：（i） $G_{sub}$ 包含尽可能多的源实体和目标实体;（ii） $G_{sub}$ 中的实体和关系与问题上下文具有很强的相关性;（iii） $G_{sub}$ 简洁明了，几乎没有冗余信息，因此可以输入到长度有限的 LLM 中。相关方法：<a href="https://arxiv.org/pdf/2405.16420">Deep QNetworks</a>, <a href="https://arxiv.org/pdf/2401.06800">Policy Gradients</a>, and <a href="https://arxiv.org/pdf/2410.10584">Actor-Critic</a></p><h4 id="Sec-7-Knowledge-Integration"><a href="#Sec-7-Knowledge-Integration" class="headerlink" title="Sec.7 Knowledge Integration"></a>Sec.7 Knowledge Integration</h4><p><strong>微调技术：</strong>利用各种图形信息的微调过程可以根据输入目标的粒度分为三个不同的类别：（i） 节点级知识：关注图形中的各个节点。（ii） 路径级知识：专注于节点之间的连接和序列。（iii） 子图级知识：考虑由多个节点组成的较大结构及其互连。我们将详细探讨这些方面中的每一个。 </p><p>使用 Node 级知识进行微调。在许多基于图的 RAG 系统中，每个节点都链接到一个文档，例如引文网络中的摘要。由于特定领域的数据很少出现在预训练语料库中，因此一些研究在进行下游任务微调之前采用指令调优来加强对特定领域的知识理解。一种简单的微调方法包括将节点和相邻文本作为上下文信息馈送到 LLM 中，以帮助预测 <a href="https://openreview.net/forum?id=x5FfUvsLIE">1</a>、<a href="https://arxiv.org/pdf/2406.10393">2</a>、<a href="https://arxiv.org/pdf/2402.07483">3</a>。鉴于检索到的文档可能很广泛，研究人员可以利用 LLM 将这些文本提炼成单个嵌入 <a href="https://proceedings.neurips.cc/paper/2020/file/6b493230205f780e1bc26945df7481e5-Paper.pdf">4</a>。尽管缺乏词汇外标记的预训练数据，但 LLM 能够在实践中识别这些嵌入中的信息。</p><p>使用 Path-level Knowledge 进行微调。语言任务通常涉及复杂的推理，需要对事实关系有清晰的理解。利用知识图谱路径，LLM 通过关系和实体的引导增强自身推理能力。这些路径可以是从问题实体到答案实体的最直接路线，也可以使用 图检索模型 或 启发式方法 进行挖掘。它们可以用作输入和输出，但是当两个节点之间存在多条路径时，过滤掉嘈杂的路径同时保留知识图谱中的关系至关重要。为了保持实体表示的完整性及其沿路径的关系，一些方法侧重于将这些路径作为训练目标，预测两个节点之间路径上的节点和关系<a href="https://arxiv.org/pdf/2303.03922">5</a>，甚至跨多个路径<a href="https://arxiv.org/pdf/2310.01061">6</a>。这使 LLM 能够进行Path级推理并产生可靠的输出。</p><p>使用 Subgraph 级知识进行微调。与路径数据的线性拓扑不同，子图数据表现出更复杂、更不规则的拓扑。一种简单的方法是使用图编码器将子图级信息压缩到读出嵌入中。或者，将图数据转换为序列。然而，这些方法往往忽略了子图中丰富的文本内容，无法使 LLM 认识到底层的图结构。为了解决这个问题，一些工作专注于调整 transformer 架构以更好地处理结构化数据，例如<a href="https://arxiv.org/pdf/2212.01588">7</a>、<a href="https://arxiv.org/pdf/2402.11709">8</a>，而另一些则将节点和边的描述直接合并到提示中，例如<a href="https://aclanthology.org/2024.findings-eacl.132.pdf">9</a>、<a href="https://arxiv.org/pdf/2402.08170">10</a>。然而，现有方法仍然存在挑战。前者可能会因架构更改而丢失在预训练期间获得的知识，而后者可能难以处理具有大量节点和边的密集图。</p><h4 id="相关工作总结"><a href="#相关工作总结" class="headerlink" title="相关工作总结"></a>相关工作总结</h4><p>参考文献：<a href="https://arxiv.org/pdf/2501.13958">https://arxiv.org/pdf/2501.13958</a></p><p><img src="/../article_img/20250220/img3.png" alt="参考文献"></p>]]></content>
    
    
    
    <tags>
      
      <tag>LLM</tag>
      
      <tag>RAG</tag>
      
      <tag>Graph</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Graph + RAG</title>
    <link href="/2025/02/18/20250218/"/>
    <url>/2025/02/18/20250218/</url>
    
    <content type="html"><![CDATA[<h1 id="Graph-RAG"><a href="#Graph-RAG" class="headerlink" title="Graph + RAG"></a>Graph + RAG</h1><p>图RAG相比于传统RAG的优势：</p><ol><li>多跳推理能力 2. 关系建模能力 3. 高效的知识更新与管理 4. 减少检索的噪声和生成的幻觉</li></ol><p>最近看了几篇图RAG的论文：</p><h3 id="G-Retriever-Retrieval-Augmented-Generation-for-Textual-Graph-Understanding-and-Question-Answering（https-arxiv-org-pdf-2402-07630）"><a href="#G-Retriever-Retrieval-Augmented-Generation-for-Textual-Graph-Understanding-and-Question-Answering（https-arxiv-org-pdf-2402-07630）" class="headerlink" title="G-Retriever: Retrieval-Augmented Generation for Textual Graph Understanding and Question Answering（https://arxiv.org/pdf/2402.07630）"></a>G-Retriever: Retrieval-Augmented Generation for Textual Graph Understanding and Question Answering（<a href="https://arxiv.org/pdf/2402.07630%EF%BC%89">https://arxiv.org/pdf/2402.07630）</a></h3><p>Motivation: 引入文本图进行检索增强</p><p>开源代码：<a href="https://github.com/XiaoxinHe/G-Retriever">https://github.com/XiaoxinHe/G-Retriever</a></p><p>首先根据已有的外部知识构建知识图谱，并对每个节点和关系，利用其固有的文本属性进行特征编码。针对用户的query，进行相似度检索，得到节点和边的topk子集。然后设计了一种强化学习策略基于检索得到的子集构建subgraph。在生成阶段，LLM的参数被冻结，除了用户的query以外，还有检索子图的文本属性所构成的hard prompt和基于可训练graph encoder得到的soft prompt <img src="/article_img/20250218/20250218_01.png" alt="G-Retriever"></p><h3 id="Knowledge-Graph-Retrieval-Augmented-Generation-For-LLM-Based-Recommendation-https-arxiv-org-pdf-2501-02226"><a href="#Knowledge-Graph-Retrieval-Augmented-Generation-For-LLM-Based-Recommendation-https-arxiv-org-pdf-2501-02226" class="headerlink" title="Knowledge Graph Retrieval-Augmented Generation For LLM-Based Recommendation(https://arxiv.org/pdf/2501.02226)"></a>Knowledge Graph Retrieval-Augmented Generation For LLM-Based Recommendation(<a href="https://arxiv.org/pdf/2501.02226">https://arxiv.org/pdf/2501.02226</a>)</h3><p>Motivation: 传统的RAG方法忽视了知识的结构关系，借助外部知识构建KG，对推荐进行检索增强；</p><p>检索阶段，首先训练一个GNN网络用来对item-entity知识图谱的每个节点和关系进行编码，并以每个节点作为中心节点生成的多跳特征表示收集起来，构建向量数据库。对于待预测的用户节点，根据其交互历史中的每个节点，利用其文本特征从KG VecDB中检索相关子图，并对检索得到的所有子图进行重排序用于构建soft prompt，从而增强LLM的推荐能力。<img src="/article_img/20250218/20250218_02.png" alt="K-RagRec"></p><h3 id="KG-Retriever-Efficient-Knowledge-Indexing-for-Retrieval-Augmented-Large-Language-Models（https-arxiv-org-pdf-2412-05547）"><a href="#KG-Retriever-Efficient-Knowledge-Indexing-for-Retrieval-Augmented-Large-Language-Models（https-arxiv-org-pdf-2412-05547）" class="headerlink" title="KG-Retriever: Efficient Knowledge Indexing for Retrieval-Augmented Large Language Models（https://arxiv.org/pdf/2412.05547）"></a>KG-Retriever: Efficient Knowledge Indexing for Retrieval-Augmented Large Language Models（<a href="https://arxiv.org/pdf/2412.05547%EF%BC%89">https://arxiv.org/pdf/2412.05547）</a></h3><p>Motivation: 对结构信息分层检索，从根本上缓解信息碎片化的问题</p><p>设计了Doc层(Document-level Graph)和KG层(Entity-level Graph)分别用于建立文档内和文档间的连接，Doc Graph的构建是对每个文档进行文本编码，并基于语义相似性获得每个节点的K近邻居；KG Graph的构建是对每个文档进行信息抽取，从而获得对应的知识图谱。检索策略上，作者考虑了两级检索：文档级检索和实体级检索，前者除了考虑用户查询和每个文档的语义相似性以外，还根据Doc Graph获取这些topN文档的one-hop邻居；后者则针对上一阶段检索的所有文档所对应的kg图检索语义相关的实体集，和query一起作为最终LLM的输入<br><img src="/article_img/20250218/20250218_03.png" alt="KG-Retriever"></p>]]></content>
    
    
    
    <tags>
      
      <tag>LLM</tag>
      
      <tag>RAG</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>关于模型padding左对齐和右对齐的问题</title>
    <link href="/2025/02/17/LLM%20padding/"/>
    <url>/2025/02/17/LLM%20padding/</url>
    
    <content type="html"><![CDATA[<p>最近微调模型进行原因预测任务训练的时候，在eval阶段进行推理生成时遇到了一个警告：“A decoder-only architecture is being used, but right-padding was detected! For correct generation results, please set <code>padding_side=&#39;left&#39;</code> when initializing the tokenizer.”，我将tokenizer初始化的定义设置为左填充依然出现该警告，问了下身边对大模型比较了解的同学，发现是token序列结尾加了eos符号导致出现的warning，下面是transformers库中该警告出现的条件：</p><p><img src="/article_img/LLM%20padding/warning.png" alt="warning"></p><p>为什么模型训练选择右填充，推理时选择左填充；为什么训练时需要在结尾添加<eos>标记符，而推理时则不需要，下面给出解释说明：</p><p>训练：Q+A[EOS]  </p><p>模型训练时，对于输入的token序列，我们知道其真实标签（Ques部分可直接用-100作为 <strong>mask</strong> 填充或无效标签，以确保这些位置不会影响损失计算），采用右填充是为了让每个batch内的样本长度对齐。在结尾添加eos标记符是为了告诉模型输入序列的结束位置，如果没有 EOS token，模型可能会将序列当作是没有结束的，进而可能会试图无限制地生成下一个 token，导致训练不稳定或生成行为不正确</p><p>推理：Q</p><p>自回归模型在推理时，从bos标记符开始从左到右依次预测下一个词来生成内容，如下面的图所示：</p><p><img src="/article_img/LLM%20padding/padding.png" alt="padding"></p><p>如果采用右填充，<pad>将被放置在In-Context的右端，从而改变了In-Context内容，这将导致生成时模型的上下文理解出现偏差，结果可能会影响到生成的质量或连贯性。此外，如果在In-Context的结尾处加入eos标记符，这将导致模型停止生成。</p><p>参考链接：<a href="https://zhuanlan.zhihu.com/p/4581421783">模型训练选择right填充，推理选择left填充，为什么？ - 知乎</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>Bug</tag>
      
      <tag>Code</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>大模型显存占用计算</title>
    <link href="/2025/02/17/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%98%BE%E5%AD%98%E5%8D%A0%E7%94%A8%E8%AE%A1%E7%AE%97/"/>
    <url>/2025/02/17/%E5%A4%A7%E6%A8%A1%E5%9E%8B%E6%98%BE%E5%AD%98%E5%8D%A0%E7%94%A8%E8%AE%A1%E7%AE%97/</url>
    
    <content type="html"><![CDATA[<h2 id="1-显存单位换算"><a href="#1-显存单位换算" class="headerlink" title="1. 显存单位换算"></a>1. 显存单位换算</h2><p>在讨论显存占用时，首先要明白“B”和“G”的含义。通常，“B”指的是十亿（1B &#x3D; 10^9），而“G”则表示千兆字节（1G &#x3D; 10^9字节）。例如，1B参数意味着有10亿个参数。显存的单位通常以字节计算，而1个字节等于8位。<br>🎈如果使用全精度训练（fp32），每个参数需要占用32位（即4个字节），因此1B的参数需要占用4GB的显存。<br>🎈如果使用半精度（fp16或bf16），则每个参数占用2字节，1B的参数只需占用2GB的显存。</p><h2 id="2-显存开销的其他组成部分"><a href="#2-显存开销的其他组成部分" class="headerlink" title="2. 显存开销的其他组成部分"></a>2. 显存开销的其他组成部分</h2><p>除了模型参数本身外，训练过程中还会消耗一定的显存，主要包括以下几部分：<br>🎈梯度：每个参数对应一个梯度，因此梯度的显存占用与参数量相同。<br>🎈优化器状态：优化器，如Adam，通常会为每个参数保存一阶动量和二阶动量，因此优化器的显存开销为参数量的2倍（对于Adam）。对于其他优化器（如SGD），则取决于优化器的具体实现，若是带动量的SGD，则为参数量的1倍。</p><h2 id="3-显存总占用计算"><a href="#3-显存总占用计算" class="headerlink" title="3. 显存总占用计算"></a>3. 显存总占用计算</h2><p>假设我们训练一个参数量为1B的模型，采用全精度（fp32）并使用Adam优化器，显存的占用计算如下：<br>🎈参数：1B × 4GB &#x3D; 4GB<br>🎈梯度：1B × 4GB &#x3D; 4GB<br>🎈优化器状态：1B × 8GB &#x3D; 8GB<br>因此，总显存占用为16GB。如果使用半精度（bf16），则显存占用减半，为8GB。混合精度训练则会根据各部分精度调整计算结果。</p>]]></content>
    
    
    
    <tags>
      
      <tag>LLM</tag>
      
      <tag>Code</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>Recsys大牛实验室官网链接</title>
    <link href="/2025/02/01/lab/"/>
    <url>/2025/02/01/lab/</url>
    
    <content type="html"><![CDATA[<p>Xiangnan He：<a href="https://hexiangnan.github.io/">https://hexiangnan.github.io/</a><br>Yuan Fang@SMU: <a href="https://www.yfang.site/">https://www.yfang.site/</a><br>Huang Chao: <a href="https://sites.google.com/view/chaoh">https://sites.google.com/view/chaoh</a></p>]]></content>
    
    
    
    <tags>
      
      <tag>Study</tag>
      
    </tags>
    
  </entry>
  
  
  
  <entry>
    <title>人工智能竞赛汇总</title>
    <link href="/2025/02/01/competition/"/>
    <url>/2025/02/01/competition/</url>
    
    <content type="html"><![CDATA[<h2 id="1-人工智能竞赛平台-Biendata：Data-Competition-Community-Biendata"><a href="#1-人工智能竞赛平台-Biendata：Data-Competition-Community-Biendata" class="headerlink" title="1.人工智能竞赛平台 Biendata：Data Competition Community - Biendata"></a><strong>1.人工智能竞赛平台 Biendata：</strong><a href="https://link.zhihu.com/?target=https://www.biendata.xyz/">Data Competition Community - Biendata</a></h2><p><strong>介绍：</strong></p><blockquote><p>2018 年 5 月，人工智能和大数据的竞赛平台 Biendata 完成天使轮融资，由DeepTech深科技投资，旨在打造中国人工智能赛事顶级 IP，赛事相关媒体运营。Biendata 的比赛客户既包括<strong>今日头条、知乎、摩拜、搜狐等企业，也包括了 IEEE、ACM、中国计算机学会、中国人工智能学会</strong>等国内外顶尖学术组织。</p></blockquote><p>总体上来说就是一个各自AI比赛汇总的平台（除了一些大厂有自己的大规模AI赛事比如阿里云天池、华为云、百度、腾讯），类似的办赛平台IP还有</p><ul><li><strong>datafountain：</strong><a href="https://www.datafountain.cn/competitions">数据科学竞赛&#x2F;大数据 竞赛 - DataFountain</a></li><li><strong>Kaggle(国外)：</strong><a href="https://www.kaggle.com/competitions">Kaggle Competitions</a></li></ul><p><strong>时间：</strong>基本上是什么时间都有，需要持续关注官网，同一个企业基本每年举办的timeline不变</p><h2 id="2-阿里云天池：算法大赛-天池大数据竞赛-天池大赛-阿里云天池"><a href="#2-阿里云天池：算法大赛-天池大数据竞赛-天池大赛-阿里云天池" class="headerlink" title="2.阿里云天池：算法大赛-天池大数据竞赛-天池大赛-阿里云天池"></a><strong>2.阿里云天池：</strong><a href="https://tianchi.aliyun.com/competition/gameList/algorithmList">算法大赛-天池大数据竞赛-天池大赛-阿里云天池</a></h2><p><strong>介绍：</strong>老牌，2014年启办，面向全世界科研人员和高校师生，业务场景丰富（2B2C都有cover），奖金池也丰富。</p><p><strong>时间：</strong>基本上是5月份开始报名——夏天初赛\复赛——10月决赛答辩</p><h2 id="3-华为云-：华为云大赛平台"><a href="#3-华为云-：华为云大赛平台" class="headerlink" title="3.华为云 ：华为云大赛平台"></a><strong>3.华为云 ：</strong><a href="https://competition.huaweicloud.com/competitions">华为云大赛平台</a></h2><p><strong>介绍：</strong>众所周知华为对研发投入比例很大，也十分重视创新大赛的举办，会联合各种高校、产品线、学术机构\组织办赛，每年任何时间节点都可能会有比赛。相关领域的同学需要持续关注。</p><p><strong>时间：</strong>一年中任意时刻</p><h2 id="4-百度飞桨AI-Studio大赛：飞桨AI-Studio-人工智能学习与实训社区"><a href="#4-百度飞桨AI-Studio大赛：飞桨AI-Studio-人工智能学习与实训社区" class="headerlink" title="4. 百度飞桨AI Studio大赛：飞桨AI Studio - 人工智能学习与实训社区"></a><strong>4. 百度飞桨AI Studio大赛：</strong><a href="https://aistudio.baidu.com/aistudio/competition/1/1">飞桨AI Studio - 人工智能学习与实训社区</a></h2><p><strong>介绍：</strong>AI Studio是基于百度深度学习平台飞桨的人工智能学习与实训社区，整体跟华为云的形式差异不大，但赛程周期会紧凑一些，内容也和百度的业务内容强相关（如<strong>NLP</strong>\图像检测\导航路径）。个人感觉场景会更专精于NLP，比如事件抽取\机器翻译\QA\阅读理解\情感分析。</p><p><strong>时间：</strong>每年3-5月</p><h2 id="5-腾讯：-2021腾讯广告算法大赛"><a href="#5-腾讯：-2021腾讯广告算法大赛" class="headerlink" title="5.腾讯： 2021腾讯广告算法大赛"></a><strong>5.腾讯：</strong> <a href="https://algo.qq.com/index.html%3Flang%3Dcn">2021腾讯广告算法大赛</a></h2><p><strong>介绍：</strong>腾讯AI赛事做的不是特别好，除了广告算法大赛还比较成规模（基本每年有延续），其他的都很分散，没有统一的平台。</p><blockquote><p>AI Lab偶尔有些算法挑战赛，AI温室种番茄什么的: <a href="https://ai.tencent.com/ailab/zh/index">腾讯 AI Lab - 腾讯人工智能实验室官网</a></p></blockquote><p><strong>时间：</strong>三月开始报名——夏季初赛\复赛——八月决赛答辩</p><h2 id="6-AIOps-Challenge智能运维挑战赛：竞赛列表"><a href="#6-AIOps-Challenge智能运维挑战赛：竞赛列表" class="headerlink" title="6.AIOps Challenge智能运维挑战赛：竞赛列表"></a><strong>6.AIOps Challenge智能运维挑战赛</strong>：<a href="https://iops.ai/competition_list/">竞赛列表</a></h2><p><strong>介绍：</strong>规模较小，每年承办方会有变化，但是主题围绕着AIOps不变</p><p><strong>时间：</strong>基本上是1月年初开始报名——春季初赛\复赛——五月决赛答辩</p><h2 id="7-KDD-CUP：KDD-2021-Singapore"><a href="#7-KDD-CUP：KDD-2021-Singapore" class="headerlink" title="7.KDD CUP：KDD 2021 | Singapore"></a><strong>7.KDD CUP：</strong><a href="https://www.kdd.org/kdd2021/%23">KDD 2021 | Singapore</a></h2><p><strong>介绍:</strong> 国外赛事，商业性质较弱所以奖金池较小，主题是多源数据时序异常检测\OGB-LSC\城市大脑挑战，延续性比较好。</p><p><strong>时间：</strong>每年五月-八月</p>]]></content>
    
    
    
    <tags>
      
      <tag>Study</tag>
      
    </tags>
    
  </entry>
  
  
  
  
</search>
